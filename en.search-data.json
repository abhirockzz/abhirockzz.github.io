{"/blog/a-simple-convenience-package-for-the-azure-cosmos-db-go-sdk/":{"data":{"":"","authentication-auth#Authentication (\u003ccode\u003eauth\u003c/code\u003e)":"","azure-functions-triggers-functionstrigger#Azure Functions Triggers (\u003ccode\u003efunctions/trigger\u003c/code\u003e)":"","conclusion#Conclusion":"Like I mentioned, its still early days and the cosmosdb-go-sdk-helper package provides simple convenience functions for common tasks to help reduce boilerplate. I expect to keep adding to it gradually, so if you have any suggestions or features you’d like to see, please open an issue.","database-and-container-operations-common#Database and Container Operations (\u003ccode\u003ecommon\u003c/code\u003e)":"","error-handling-cosmosdb_errors#Error Handling (\u003ccode\u003ecosmosdb_errors\u003c/code\u003e)":"\nWhen using the Go SDK for the Azure Cosmos DB NoSQL API, I often find myself writing boilerplate code for various operations. This includes database/container operations, querying, and more. cosmosdb-go-sdk-helper (I know, not a great name!) is a package with convenience functions for some of these tasks.\nIn this blog post, I will go over the packages in the repository with examples on how (and when) you can use them. It’s early days for this project, but I hope to keep adding to it gradually.\nOverview auth: Simplifies authentication for both production and local development. common: Helps with common database and container operations. query: Offers type-safe, generic query helpers and optional metrics, and helps with Cosmos DB query metrics. functions: Eases the parsing of Azure Functions Cosmos DB trigger payloads. cosmosdb_errors: Extracts structured error information for simple error handling. Quick Start To get started, install the package:\ngo get github.com/abhirockzz/cosmosdb-go-sdk-helper Try it out using the example below:\npackage main import ( \"fmt\" \"log\" \"github.com/abhirockzz/cosmosdb-go-sdk-helper/auth\" \"github.com/abhirockzz/cosmosdb-go-sdk-helper/common\" \"github.com/abhirockzz/cosmosdb-go-sdk-helper/query\" ) func main() { endpoint := \"https://ACCOUNT_NAME.documents.azure.com:443\" type Task struct { ID string `json:\"id\"` Info string `json:\"info\"` } client, err := auth.GetCosmosDBClient(endpoint, false, nil) if err != nil { log.Fatalf(\"Azure AD auth failed: %v\", err) } container, err := client.NewContainer(databaseName, containerName) if err != nil { log.Fatalf(\"NewContainer failed: %v\", err) } task := Task{ ID: \"45\", Info: \"Sample task\", } insertedTask, err := common.InsertItemWithResponse(container, task, azcosmos.NewPartitionKeyString(task.ID), nil) if err != nil { log.Fatalf(\"InsertItem failed: %v\", err) } fmt.Printf(\"Inserted task: %s (%s)\\n\", insertedTask.ID, insertedTask.Info) tasks, err := query.QueryItems[Task](container, sqlQuery, azcosmos.NewPartitionKey(), nil) if err != nil { log.Fatalf(\"QueryItems failed: %v\", err) } for _, task := range tasks { fmt.Printf(\"Task: %s (%s)\\n\", task.ID, task.Info) } } Let’s quickly go over the packages.\nAuthentication (auth) The auth package gets authenticated Cosmos DB client handle for both Azure AD and local Cosmos DB emulator. It simplifies the process, making it easier to switch between production and local development environments.\nExample:\nWhen connecting to actual Cosmos DB endpoint, function uses DefaultAzureCredential. DefaultAzureCredential uses an ordered sequence of mechanisms for authentication (including environment variables, managed identity, Azure CLI credential etc.).\nclient, err := auth.GetCosmosDBClient(\"https://your-account.documents.azure.com:443\", false, nil) if err != nil { log.Fatalf(\"Azure AD auth failed: %v\", err) } When using the emulator, simply set useEmulator flag to true and pass the emulator URL (e.g. http://localhost:8081) without changing anything else.\nclient, err := auth.GetCosmosDBClient(\"http://localhost:8081\", true, nil) if err != nil { log.Fatalf(\"Emulator auth failed: %v\", err) } Database and Container Operations (common) The common package lets you to create databases and containers only if they don’t already exist. This is useful for idempotent resource management, especially in CI/CD pipelines. It also provides other utility functions, such as listing all databases and containers, etc.\nExample:\nprops := azcosmos.DatabaseProperties{ID: \"tododb\"} db, err := common.CreateDatabaseIfNotExists(client, props, nil) containerProps := azcosmos.ContainerProperties{ ID: \"tasks\", PartitionKeyDefinition: azcosmos.PartitionKeyDefinition{ Paths: []string{\"/id\"}, Kind: azcosmos.PartitionKeyKindHash, }, } container, err := common.CreateContainerIfNotExists(db, containerProps, nil) This is also goroutine-safe (can be used with concurrent programs), so you can run it in multiple instances without worrying about race conditions since its idempotent.\nQuery Operations (query) The query package provides generic helpers for querying multiple or single items, returning strongly-typed results. This eliminates the need for manual unmarshalling and reduces boilerplate code.\nExample:\ntype Task struct { ID string `json:\"id\"` Info string `json:\"info\"` } tasks, err := query.QueryItems[Task](container, \"SELECT * FROM c\", azcosmos.NewPartitionKey(), nil) // Query a single item task, err := query.QueryItem[Task](container, \"item-id\", azcosmos.NewPartitionKeyString(\"item-id\"), nil) Query Metrics (metrics) You can use the metrics package to conveniently execute queries and the get results as a Go struct (that includes the metrics).\nExample:\n// Query with metrics result, err := query.QueryItemsWithMetrics[Task](container, \"SELECT * FROM c WHERE c.status = 'complete'\", azcosmos.NewPartitionKey(), nil) for i, metrics := range result.Metrics { fmt.Printf(\"Page %d: TotalExecutionTimeInMs=%f\\n\", i, metrics.TotalExecutionTimeInMs) } You can also parse the metrics string manually using ParseQueryMetrics:\nqm, err := metrics.ParseQueryMetrics(\"totalExecutionTimeInMs=12.5;queryCompileTimeInMs=1.2;...\") fmt.Println(qm.TotalExecutionTimeInMs, qm.QueryCompileTimeInMs) The QueryItemsWithMetrics uses ParseQueryMetrics behind the scenes to parse the metrics string.\nIt also provides a ParseIndexMetrics function that parses the index metrics string returned by Cosmos DB (decodes base64-encoded index metrics from query responses):\nindexMetrics, err := metrics.ParseIndexMetrics(\"base64-encoded-index-metrics\") fmt.Println(indexMetrics) Azure Functions Triggers (functions/trigger) When using Azure Cosmos DB triggers with Azure Functions written in Go (using Custom handlers), you will need to make sense of the raw payload sent by Azure Functions. The payload contains the changed documents in a nested JSON format. The functions/trigger package simplifies this by providing helpers to parse the payload into a format you can use directly in your function.\nYou can use ParseToCosmosDBDataMap to directly get the Cosmos DB documents data as a []map[string]any, which is flexible and easy to work with.\nExample:\n// from the Azure Function trigger payload := `{\"Data\":{\"documents\":\"\\\"[{\\\\\\\"id\\\\\\\":\\\\\\\"dfa26d32-f876-44a3-b107-369f1f48c689\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Setup monitoring\\\\\\\",\\\\\\\"_rid\\\\\\\":\\\\\\\"lV8dAK7u9cCUAAAAAAAAAA==\\\\\\\",\\\\\\\"_self\\\\\\\":\\\\\\\"dbs/lV8dAA==/colls/lV8dAK7u9cA=/docs/lV8dAK7u9cCUAAAAAAAAAA==/\\\\\\\",\\\\\\\"_etag\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"0f007efc-0000-0800-0000-67f5fb920000\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"_attachments\\\\\\\":\\\\\\\"attachments/\\\\\\\",\\\\\\\"_ts\\\\\\\":1744173970,\\\\\\\"_lsn\\\\\\\":160}]\\\"\"},\"Metadata\":{\"sys\":{\"MethodName\":\"cosmosdbprocessor\",\"UtcNow\":\"2025-04-09T04:46:10.723203Z\",\"RandGuid\":\"0d00378b-6426-4af1-9fc0-0793f4ce3745\"}}}` docs, err := trigger.ParseToCosmosDBDataMap(payload) Alternatively, you can use ParseToRawString to get the raw JSON string and then unmarshal it into your own struct. This is useful if you can define the structure of the data you expect and want to work with it in a more type-safe manner.\nExample:\n// from the Azure Function trigger payload := `{\"Data\":{\"documents\":\"\\\"[{\\\\\\\"id\\\\\\\":\\\\\\\"dfa26d32-f876-44a3-b107-369f1f48c689\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Setup monitoring\\\\\\\",\\\\\\\"_rid\\\\\\\":\\\\\\\"lV8dAK7u9cCUAAAAAAAAAA==\\\\\\\",\\\\\\\"_self\\\\\\\":\\\\\\\"dbs/lV8dAA==/colls/lV8dAK7u9cA=/docs/lV8dAK7u9cCUAAAAAAAAAA==/\\\\\\\",\\\\\\\"_etag\\\\\\\":\\\\\\\"\\\\\\\\\\\\\\\"0f007efc-0000-0800-0000-67f5fb920000\\\\\\\\\\\\\\\"\\\\\\\",\\\\\\\"_attachments\\\\\\\":\\\\\\\"attachments/\\\\\\\",\\\\\\\"_ts\\\\\\\":1744173970,\\\\\\\"_lsn\\\\\\\":160}]\\\"\"},\"Metadata\":{\"sys\":{\"MethodName\":\"cosmosdbprocessor\",\"UtcNow\":\"2025-04-09T04:46:10.723203Z\",\"RandGuid\":\"0d00378b-6426-4af1-9fc0-0793f4ce3745\"}}}` rawStringData, err := trigger.ParseToRawString(payload) type Task struct { ID string `json:\"id\"` Description string `json:\"description\"` } var documents []Task err := json.Unmarshal([]byte(rawStringData), \u0026documents) Error Handling (cosmosdb_errors) The cosmosdb_errors package extracts status code and message from Cosmos DB SDK errors and returns a struct for easier downstream handling.\nI expect to improve/add to this.\nExample:\nif err != nil { cosmosErr := cosmosdb_errors.GetError(err) if cosmosErr.Status == http.StatusNotFound { // Handle not found } else { // Handle other errors } } ","overview#Overview":"","query-operations-query#Query Operations (\u003ccode\u003equery\u003c/code\u003e)":""},"title":"A simple, convenience package for the Azure Cosmos DB Go SDK"},"/blog/access-sqldb-keyvault/":{"data":{"":"The Apache Spark connector for Azure SQL Database (and SQL Server) enables these databases to be used as input data sources and output data sinks for Apache Spark jobs. You can use the connector in Azure Synapse Analytics for big data analytics on real-time transactional data and to persist results for ad-hoc queries or reporting.\nAt the time of writing, there is no linked service or AAD pass-through support with the Azure SQL connector via Azure Synapse Analytics. But you can use other options such as Azure Active Directory authentication or via direct SQL authentication (username and password based). A secure way of doing this is to store the Azure SQL Database credentials in Azure Key Vault (as Secret) — this is what’s covered in this short blog post.\nAssuming you have an Azure Synapse Workspace and Azure SQL Database already created, all you need to is:\nCreate an Azure Key Vault and add a Secret to store the Azure SQL Database connectivity info. Create a Linked Service for your Azure Key Vault in Azure Synapse Workspace. Provide appropriate permissions to Azure Synapse workspace managed service identity to Azure Key Vault ","here-is-a-walk-through-of-the-process#Here is a walk through of the process":"Create an Azure Key Vault and add a Secret.\nI have stored the entire JDBC connection string in this case but you can choose to just store the password as well.\nTo retrieve secrets from Azure Key Vault, the recommended way is to create a Linked Service to your Azure Key Vault. Also, make sure that the Synapse workspace managed service identity (MSI) has Secret Get privileges on your Azure Key Vault. This will let Synapse authenticate to Azure Key Vault using the Synapse workspace managed service identity.\nYou can also authenticate using your user Azure Active Directory credential.\nCreate a Linked Service in Azure Synapse Workspace:\nGrant appropriate access for Azure Synapse workspace service managed identity to your Azure Key Vault:\nChoose Get permission on Secret:\nSearch for the Synapse Workspace Managed Service Identity — it’s the same name as that of the workspace\nAdd the policy:\nClick Save to confirm:","lets-see-how-to-use-this#Let’s see how to use this…":" I will be using pyspark in Synapse Spark pools as an example.\nSynapse uses Azure Active Directory (AAD) passthrough by default for authentication between resources. If you need to connect to a resource using other credentials, use the TokenLibrary directly — this simplifies the process of retrieving SAS tokens, AAD tokens, connection strings, and secrets stored in a linked service or from an Azure Key Vault.\nTo retrieve a secret stored from Azure Key Vault, use the TokenLibrary.getSecret() function. Here is a python example but same applies to C# or Scala.\nFor example, to access data from SalesLT.Customer table (part of AdventureWorks sample database), you can use the following:\nurl = TokenLibrary.getSecret(\"\u003cAzure Key Vault name\u003e\", \"\u003cSecret name\u003e\", \"\u003cLinked Service name\u003e\") dbtable = \"SalesLT.Customer\" customers = spark.read \\ .format(\"com.microsoft.sqlserver.jdbc.spark\") \\ .option(\"url\", url) \\ .option(\"dbtable\", dbtable) \\ .load() print(customers.count()) customers.show(5) That’s all there is to it!"},"title":"Securely access Azure SQL Database from Azure Synapse"},"/blog/ack-sqs-lambda-dynamodb/":{"data":{"":"In this blog post, you will be using AWS Controllers for Kubernetes on an Amazon EKS cluster to put together a solution wherein data from an Amazon SQS queue is processed by an AWS Lambda function and persisted to a DynamoDB table.\nAWS Controllers for Kubernetes (also known as ACK) leverage Kubernetes Custom Resource and Custom Resource Definitions and give you the ability to manage and use AWS services directly from Kubernetes without needing to define resources outside of the cluster. The idea behind ACK is to enable Kubernetes users to describe the desired state of AWS resources using the Kubernetes API and configuration language. ACK will then take care of provisioning and managing the AWS resources to match the desired state. This is achieved by using Service controllers that are responsible for managing the lifecycle of a particular AWS service. Each ACK service controller is packaged into a separate container image that is published in a public repository corresponding to an individual ACK service controller.\nThere is no single ACK container image. Instead, there are container images for each individual ACK service controller that manages resources for a particular AWS API.\nThis blog post will walk you through how to use the SQS, DynamoDB and Lambda service controllers for ACK.","clean-up#Clean up":"After you have explored the solution, you can clean up the resources by running the following commands:\nDelete SQS queue, DynamoDB table and the Lambda function:\nkubectl delete -f sqs-queue.yaml kubectl delete -f function.yaml kubectl delete -f dynamodb-table.yaml To uninstall the ACK service controllers, run the following commands:\nexport ACK_SYSTEM_NAMESPACE=ack-system helm ls -n $ACK_SYSTEM_NAMESPACE helm uninstall -n $ACK_SYSTEM_NAMESPACE \u003center name of the sqs chart\u003e helm uninstall -n $ACK_SYSTEM_NAMESPACE \u003center name of the lambda chart\u003e helm uninstall -n $ACK_SYSTEM_NAMESPACE \u003center name of the dynamodb chart\u003e ","conclusion-and-next-steps#Conclusion and next steps":"In this post, we have seen how to use AWS Controllers for Kubernetes to create a Lambda function, SQS, DynamoDB table and wire them together to deploy a solution. All of this (almost) was done using Kubernetes! I encourage you to try out other AWS services supported by ACK - here is a complete list.\nHappy Building!","create-sqs-queue-dynamodb-table-and-deploy-the-lambda-function#Create SQS queue, DynamoDB table and deploy the Lambda function":"Create SQS queue\nIn the file sqs-queue.yaml, replace the us-east-1 region with your preferred region as well as the AWS account ID. This is what the ACK manifest for SQS queue looks like:\napiVersion: sqs.services.k8s.aws/v1alpha1 kind: Queue metadata: name: sqs-queue-demo-ack annotations: services.k8s.aws/region: us-east-1 spec: queueName: sqs-queue-demo-ack policy: | { \"Statement\": [{ \"Sid\": \"__owner_statement\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"AWS_ACCOUNT_ID\" }, \"Action\": \"sqs:SendMessage\", \"Resource\": \"arn:aws:sqs:us-east-1:AWS_ACCOUNT_ID:sqs-queue-demo-ack\" }] } Create the queue using the following command:\nkubectl apply -f sqs-queue.yaml # list the queue kubectl get queue Create DynamoDB table\nThis is what the ACK manifest for DynamoDB table looks like:\napiVersion: dynamodb.services.k8s.aws/v1alpha1 kind: Table metadata: name: customer annotations: services.k8s.aws/region: us-east-1 spec: attributeDefinitions: - attributeName: email attributeType: S billingMode: PAY_PER_REQUEST keySchema: - attributeName: email keyType: HASH tableName: customer You can replace the us-east-1 region with your preferred region.\nCreate a table (named customer) using the following command:\nkubectl apply -f dynamodb-table.yaml # list the tables kubectl get tables Build function binary and create Docker image\nGOARCH=amd64 GOOS=linux go build -o main main.go aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws docker build -t demo-sqs-dynamodb-func-ack . Create a private ECR repository, tag and push the Docker image to ECR:\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com aws ecr create-repository --repository-name demo-sqs-dynamodb-func-ack --region us-east-1 docker tag demo-sqs-dynamodb-func-ack:latest $AWS_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/demo-sqs-dynamodb-func-ack:latest docker push $AWS_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/demo-sqs-dynamodb-func-ack:latest Create an IAM execution Role for the Lambda function and attach the required policies:\nexport ROLE_NAME=demo-sqs-dynamodb-func-ack-role ROLE_ARN=$(aws iam create-role \\ --role-name $ROLE_NAME \\ --assume-role-policy-document '{\"Version\": \"2012-10-17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}' \\ --query 'Role.[Arn]' --output text) aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole Since the Lambda function needs to write data to DynamoDB and invoke SQS, let’s add the following policies to the IAM role:\naws iam put-role-policy \\ --role-name \"${ROLE_NAME}\" \\ --policy-name \"dynamodb-put\" \\ --policy-document file://dynamodb-put.json aws iam put-role-policy \\ --role-name \"${ROLE_NAME}\" \\ --policy-name \"sqs-permissions\" \\ --policy-document file://sqs-permissions.json Create the Lambda function\nUpdate function.yaml file with the following info:\nimageURI - the URI of the Docker image that you pushed to ECR e.g. \u003cAWS_ACCOUNT_ID\u003e.dkr.ecr.us-east-1.amazonaws.com/demo-sqs-dynamodb-func-ack:latest role - the ARN of the IAM role that you created for the Lambda function e.g. arn:aws:iam::\u003cAWS_ACCOUNT_ID\u003e:role/demo-sqs-dynamodb-func-ack-role This is what the ACK manifest for the Lambda function looks like:\napiVersion: lambda.services.k8s.aws/v1alpha1 kind: Function metadata: name: demo-sqs-dynamodb-func-ack annotations: services.k8s.aws/region: us-east-1 spec: architectures: - x86_64 name: demo-sqs-dynamodb-func-ack packageType: Image code: imageURI: AWS_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/demo-sqs-dynamodb-func-ack:latest environment: variables: TABLE_NAME: customer role: arn:aws:iam::AWS_ACCOUNT_ID:role/demo-sqs-dynamodb-func-ack-role description: A function created by ACK lambda-controller To create the Lambda function, run the following command:\nkubectl create -f function.yaml # list the function kubectl get functions Add SQS trigger configuration Add SQS trigger which will invoke the Lambda function when event is sent to SQS queue.\nHere is an example using AWS Console - Open the Lambda function in the AWS Console and click on the Add trigger button. Select SQS as the trigger source, select the SQS queue and click on the Add button.\nNow you are ready to try out the end to end solution!","prerequisites#Prerequisites":"To follow along step-by-step, in addition to an AWS account, you will need to have AWS CLI, kubectl and helm installed.\nThere are a variety of ways in which you can create an Amazon EKS cluster. I prefer using eksctl CLI because of the convenience it offers. Creating an an EKS cluster using eksctl, can be as easy as this:\neksctl create cluster --name my-cluster --region region-code For details, refer to the Getting started with Amazon EKS – eksctl.\nClone this GitHub repository and change to the right directory:\ngit clone https://github.com/abhirockzz/k8s-ack-sqs-lambda cd k8s-ack-sqs-lambda Ok let’s get started!","setup-the-ack-service-controllers-for-aws-lambda-sqs-and-dynamodb#Setup the ACK service controllers for AWS Lambda, SQS and DynamoDB":"Install ACK controllers Log into the Helm registry that stores the ACK charts:\naws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws Deploy the ACK service controller for Amazon Lambda using the lambda-chart Helm chart:\nRELEASE_VERSION_LAMBDA_ACK=$(curl -sL \"https://api.github.com/repos/aws-controllers-k8s/lambda-controller/releases/latest\" | grep '\"tag_name\":' | cut -d'\"' -f4) helm install --create-namespace -n ack-system oci://public.ecr.aws/aws-controllers-k8s/lambda-chart \"--version=${RELEASE_VERSION_LAMBDA_ACK}\" --generate-name --set=aws.region=us-east-1 Deploy the ACK service controller for SQS using the sqs-chart Helm chart:\nRELEASE_VERSION_SQS_ACK=$(curl -sL \"https://api.github.com/repos/aws-controllers-k8s/sqs-controller/releases/latest\" | grep '\"tag_name\":' | cut -d'\"' -f4) helm install --create-namespace -n ack-system oci://public.ecr.aws/aws-controllers-k8s/sqs-chart \"--version=${RELEASE_VERSION_SQS_ACK}\" --generate-name --set=aws.region=us-east-1 Deploy the ACK service controller for DynamoDB using the dynamodb-chart Helm chart:\nRELEASE_VERSION_DYNAMODB_ACK=$(curl -sL \"https://api.github.com/repos/aws-controllers-k8s/dynamodb-controller/releases/latest\" | grep '\"tag_name\":' | cut -d'\"' -f4) helm install --create-namespace -n ack-system oci://public.ecr.aws/aws-controllers-k8s/dynamodb-chart \"--version=${RELEASE_VERSION_DYNAMODB_ACK}\" --generate-name --set=aws.region=us-east-1 Now, it’s time to configure the IAM permissions for the controller to invoke Lambda, DynamoDB and SQS.\nConfigure IAM permissions Create an OIDC identity provider for your cluster\nFor the below steps, replace the EKS_CLUSTER_NAME and AWS_REGION variables with your cluster name and region.\nexport EKS_CLUSTER_NAME=demo-eks-cluster\texport AWS_REGION=us-east-1 eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --region $AWS_REGION --approve OIDC_PROVIDER=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f2- | cut -d '/' -f2-) Create IAM roles for Lambda, SQS and DynamoDB ACK service controllers ACK Lambda controller\nSet the following environment variables:\nACK_K8S_SERVICE_ACCOUNT_NAME=ack-lambda-controller ACK_K8S_NAMESPACE=ack-system AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) Create the trust policy for the IAM role:\nread -r -d '' TRUST_RELATIONSHIP \u003c\u003cEOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": \"system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_K8S_SERVICE_ACCOUNT_NAME}\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" \u003e trust_lambda.json Create the IAM role:\nACK_CONTROLLER_IAM_ROLE=\"ack-lambda-controller\" ACK_CONTROLLER_IAM_ROLE_DESCRIPTION=\"IRSA role for ACK lambda controller deployment on EKS cluster using Helm charts\" aws iam create-role --role-name \"${ACK_CONTROLLER_IAM_ROLE}\" --assume-role-policy-document file://trust_lambda.json --description \"${ACK_CONTROLLER_IAM_ROLE_DESCRIPTION}\" Attach IAM policy to the IAM role:\n# we are getting the policy directly from the ACK repo INLINE_POLICY=\"$(curl https://raw.githubusercontent.com/aws-controllers-k8s/lambda-controller/main/config/iam/recommended-inline-policy)\" aws iam put-role-policy \\ --role-name \"${ACK_CONTROLLER_IAM_ROLE}\" \\ --policy-name \"ack-recommended-policy\" \\ --policy-document \"${INLINE_POLICY}\" Attach ECR permissions to the controller IAM role - these are required since Lambda functions will be pulling images from ECR.\naws iam put-role-policy \\ --role-name \"${ACK_CONTROLLER_IAM_ROLE}\" \\ --policy-name \"ecr-permissions\" \\ --policy-document file://ecr-permissions.json Associate the IAM role to a Kubernetes service account:\nACK_CONTROLLER_IAM_ROLE_ARN=$(aws iam get-role --role-name=$ACK_CONTROLLER_IAM_ROLE --query Role.Arn --output text) export IRSA_ROLE_ARN=eks.amazonaws.com/role-arn=$ACK_CONTROLLER_IAM_ROLE_ARN kubectl annotate serviceaccount -n $ACK_K8S_NAMESPACE $ACK_K8S_SERVICE_ACCOUNT_NAME $IRSA_ROLE_ARN Repeat the steps for the SQS controller.\nACK SQS controller\nSet the following environment variables:\nACK_K8S_SERVICE_ACCOUNT_NAME=ack-sqs-controller ACK_K8S_NAMESPACE=ack-system AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) Create the trust policy for the IAM role:\nread -r -d '' TRUST_RELATIONSHIP \u003c\u003cEOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": \"system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_K8S_SERVICE_ACCOUNT_NAME}\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" \u003e trust_sqs.json Create the IAM role:\nACK_CONTROLLER_IAM_ROLE=\"ack-sqs-controller\" ACK_CONTROLLER_IAM_ROLE_DESCRIPTION=\"IRSA role for ACK sqs controller deployment on EKS cluster using Helm charts\" aws iam create-role --role-name \"${ACK_CONTROLLER_IAM_ROLE}\" --assume-role-policy-document file://trust_sqs.json --description \"${ACK_CONTROLLER_IAM_ROLE_DESCRIPTION}\" Attach IAM policy to the IAM role:\n# for sqs controller, we use the managed policy ARN instead of the inline policy (unlike the Lambda controller) POLICY_ARN=\"$(curl https://raw.githubusercontent.com/aws-controllers-k8s/sqs-controller/main/config/iam/recommended-policy-arn)\" aws iam attach-role-policy --role-name \"${ACK_CONTROLLER_IAM_ROLE}\" --policy-arn \"${POLICY_ARN}\" Associate the IAM role to a Kubernetes service account:\nACK_CONTROLLER_IAM_ROLE_ARN=$(aws iam get-role --role-name=$ACK_CONTROLLER_IAM_ROLE --query Role.Arn --output text) export IRSA_ROLE_ARN=eks.amazonaws.com/role-arn=$ACK_CONTROLLER_IAM_ROLE_ARN kubectl annotate serviceaccount -n $ACK_K8S_NAMESPACE $ACK_K8S_SERVICE_ACCOUNT_NAME $IRSA_ROLE_ARN Repeat the steps for the DynamoDB controller.\nACK DynamoDB controller\nSet the following environment variables:\nACK_K8S_SERVICE_ACCOUNT_NAME=ack-dynamodb-controller ACK_K8S_NAMESPACE=ack-system AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) Create the trust policy for the IAM role:\nread -r -d '' TRUST_RELATIONSHIP \u003c\u003cEOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": \"system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_K8S_SERVICE_ACCOUNT_NAME}\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" \u003e trust_dynamodb.json Create the IAM role:\nACK_CONTROLLER_IAM_ROLE=\"ack-dynamodb-controller\" ACK_CONTROLLER_IAM_ROLE_DESCRIPTION=\"IRSA role for ACK dynamodb controller deployment on EKS cluster using Helm charts\" aws iam create-role --role-name \"${ACK_CONTROLLER_IAM_ROLE}\" --assume-role-policy-document file://trust_dynamodb.json --description \"${ACK_CONTROLLER_IAM_ROLE_DESCRIPTION}\" Attach IAM policy to the IAM role:\n# for dynamodb controller, we use the managed policy ARN instead of the inline policy (like we did for Lambda controller) POLICY_ARN=\"$(curl https://raw.githubusercontent.com/aws-controllers-k8s/dynamodb-controller/main/config/iam/recommended-policy-arn)\" aws iam attach-role-policy --role-name \"${ACK_CONTROLLER_IAM_ROLE}\" --policy-arn \"${POLICY_ARN}\" Associate the IAM role to a Kubernetes service account:\nACK_CONTROLLER_IAM_ROLE_ARN=$(aws iam get-role --role-name=$ACK_CONTROLLER_IAM_ROLE --query Role.Arn --output text) export IRSA_ROLE_ARN=eks.amazonaws.com/role-arn=$ACK_CONTROLLER_IAM_ROLE_ARN kubectl annotate serviceaccount -n $ACK_K8S_NAMESPACE $ACK_K8S_SERVICE_ACCOUNT_NAME $IRSA_ROLE_ARN Restart ACK controller Deployments and verify the setup Restart ACK service controller Deployment using the following commands - it will update service controller Pods with IRSA environment variables. Get list of ACK service controller deployments:\nexport ACK_K8S_NAMESPACE=ack-system kubectl get deployments -n $ACK_K8S_NAMESPACE Restart Lambda, SQS and DynamoDB controller Deployments:\nDEPLOYMENT_NAME_LAMBDA=\u003center deployment name for lambda controller\u003e kubectl -n $ACK_K8S_NAMESPACE rollout restart deployment $DEPLOYMENT_NAME_LAMBDA DEPLOYMENT_NAME_SQS=\u003center deployment name for sqs controller\u003e kubectl -n $ACK_K8S_NAMESPACE rollout restart deployment $DEPLOYMENT_NAME_SQS DEPLOYMENT_NAME_DYNAMODB=\u003center deployment name for dynamodb controller\u003e kubectl -n $ACK_K8S_NAMESPACE rollout restart deployment $DEPLOYMENT_NAME_DYNAMODB List Pods for these Deployments. Verify that the AWS_WEB_IDENTITY_TOKEN_FILE and AWS_ROLE_ARN environment variables exist for your Kubernetes Pod using the following commands:\nkubectl get pods -n $ACK_K8S_NAMESPACE LAMBDA_POD_NAME=\u003center Pod name for lambda controller\u003e kubectl describe pod -n $ACK_K8S_NAMESPACE $LAMBDA_POD_NAME | grep \"^\\s*AWS_\" SQS_POD_NAME=\u003center Pod name for sqs controller\u003e kubectl describe pod -n $ACK_K8S_NAMESPACE $SQS_POD_NAME | grep \"^\\s*AWS_\" DYNAMODB_POD_NAME=\u003center Pod name for dynamodb controller\u003e kubectl describe pod -n $ACK_K8S_NAMESPACE $DYNAMODB_POD_NAME | grep \"^\\s*AWS_\" Now that the ACK service controller have been setup and configured, you can create AWS resources!","test-the-application#Test the application":"Send few messages to the SQS queue. For the purposes of this demo, you can use the AWS CLI:\nexport SQS_QUEUE_URL=$(kubectl get queues/sqs-queue-demo-ack -o jsonpath='{.status.queueURL}') aws sqs send-message --queue-url $SQS_QUEUE_URL --message-body user1@foo.com --message-attributes 'name={DataType=String, StringValue=\"user1\"}, city={DataType=String,StringValue=\"seattle\"}' aws sqs send-message --queue-url $SQS_QUEUE_URL --message-body user2@foo.com --message-attributes 'name={DataType=String, StringValue=\"user2\"}, city={DataType=String,StringValue=\"tel aviv\"}' aws sqs send-message --queue-url $SQS_QUEUE_URL --message-body user3@foo.com --message-attributes 'name={DataType=String, StringValue=\"user3\"}, city={DataType=String,StringValue=\"new delhi\"}' aws sqs send-message --queue-url $SQS_QUEUE_URL --message-body user4@foo.com --message-attributes 'name={DataType=String, StringValue=\"user4\"}, city={DataType=String,StringValue=\"new york\"}' The Lambda function should be invoked and the data should be written to the DynamoDB table. Check the DynamoDB table using the CLI (or AWS console):\naws dynamodb scan --table-name customer "},"title":"Use AWS Controllers for Kubernetes to deploy a Serverless data processing solution with SQS, Lambda and DynamoDB"},"/blog/adx-synapse-integration/":{"data":{"":"Azure Data Explorer is a fully managed data analytics service that can handle large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. Azure Data Explorer makes it simple to ingest this data and enables you to do complex ad hoc queries on the data in seconds. It scales quickly to terabytes of data, in minutes, allowing rapid iterations of data exploration to discover relevant insights. It is already integrated with Apache Spark work via the Data Source and Data Sink Connector and is used to power solutions for near real-time data processing, data archiving, machine learning etc.\nThanks to an extension to this solution, Azure Data Explorer is available as a Linked Service in Azure Synapse Analytics, allowing seamless integration between Azure Data Explorer and Apache Spark pools in Azure Synapse.\nAzure Synapse brings together the best of SQL technologies used in enterprise data warehousing, Spark technologies used for big data, Pipelines for data integration and ETL/ELT, and deep integration with other Azure services such as Power BI, CosmosDB, and AzureML.\nThis blog post is a getting started guide to demonstrate the integration between Azure Data Explorer and Azure Synapse. It covers:\nHow to process existing data in Azure Data Explorer using Spark and Azure Synapse.\nProcess streaming and batch data using Spark and write it back to Azure data explore.\nNotebooks are available in this GitHub repo — https://github.com/abhirockzz/synapse-azure-data-explorer-101\nTo learn along, all you need is an Azure account (you can get one for free). Move on to the next section once you’re ready!","initial-setup-and-configuration#Initial setup and configuration":"Start by creating an Azure Synapse workspace along with an Apache Spark pool. Then, Create an Azure Data Explorer cluster and database\nAdjust the Ingestion Policy During the ingestion process, Azure Data Explorer attempts to optimise for throughput by batching small ingress data chunks together as they await ingestion — this is governed by the IngestionBatching policy. The default policy values are: 5 minutes as the maximum delay time, 1000 items and total size of 1G for batching. What this means that there is a certain amount of delay between when the data ingestion is triggered, until it is ready for querying. The good thing is that, the policy can be fine tuned as per requirements.\nFor the purposes of this demo, we focus on getting our data available for query as soon as possible. Hence, you should update the policy by using MaximumBatchingTimeSpan value of 30 seconds\n.alter database adxdb policy ingestionbatching @'{\"MaximumBatchingTimeSpan\": \"00:00:30\"}' The impact of setting this policy to a very small value is an increased cost and reduced performance — this is just for demo purposes\nConnect to Azure Data Explorer from Synapse In Azure Synapse Analytics, a Linked Service is where you define your connection information to other services. You can create a linked service for Azure Data Explorer using the Azure Synapse Analytics workspace.\nManaged Identity is being used as the Authentication Method as opposed to Service Principals\nAfter you create the Linked Service, it will show up in the list:\nOk you are all set!\nIf you’re already using Azure Data Explorer, it’s likely that you have a lot of data sitting there, ready to be processed! So let’s start off by exploring this aspect.","process-and-write-data-to-azure-data-explorer#Process and write data to Azure Data Explorer":"This section will cover how to process data using Spark (Synapse Spark Pools to be precise) and write it to Azure Data Explorer for further analysis.\nStart by creating another table StormEvents_2\n.create table StormEvents_2 (StartTime: datetime, EndTime: datetime, EpisodeId: int, EventId: int, State: string, EventType: string, InjuriesDirect: int, InjuriesIndirect: int, DeathsDirect: int, DeathsIndirect: int, DamageProperty: int, DamageCrops: int, Source: string, BeginLocation: string, EndLocation: string, BeginLat: real, BeginLon: real, EndLat: real, EndLon: real, EpisodeNarrative: string, EventNarrative: string, StormSummary: dynamic) We will use existing CSV data. This is the same data that we had earlier ingested into Azure Data Explorer. But, this time, we will download it to our local machine and upload it to the ADLS Gen2 account associated with the Azure Synapse workspace.\nStart by downloading this file:\ncurl -o StormEvents.csv \"https://kustosamplefiles.blob.core.windows.net/samplefiles/StormEvents.csv?sv=2019-12-12\u0026ss=b\u0026srt=o\u0026sp=r\u0026se=2022-09-05T02:23:52Z\u0026st=2020-09-04T18:23:52Z\u0026spr=https\u0026sig=VrOfQMT1gUrHltJ8uhjYcCequEcfhjyyMX%2FSc3xsCy4%3D\" Upload it to the ADLS file system using the workspace:\nFor the subsequent steps, you can either paste the code directly into a Synapse Studio notebook in Azure Synapse Analytics or import this notebook into the workspace.\nLoad the dataset from ADLS Gen2 to a DataFrame:\nevents = (spark.read .csv(\"/StormEvents.csv\", header=True, inferSchema='true') ) Apply some basic filtering using Apache Spark — omit rows with null data, drop columns we don’t need for processing and filter rows where there has not been any property damage.\nevents_filtered = events.dropna() \\ .drop('StormSummary', 'EndLat','EndLon','BeginLat','BeginLon') \\ .filter((events.DamageProperty \u003e 0)) print(events_filtered.count()) display(events_filtered.take(10)) Finally, write the DataFrame to Azure Data Explorer:\nevents_filtered.write \\ .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\ .option(\"spark.synapse.linkedService\", \"adx\") \\ .option(\"kustoDatabase\", \"adxdb\") \\ .option(\"kustoTable\", \"StormEvents_2\") \\ .option(\"tableCreateOptions\",\"FailIfNotExist\") \\ .mode(\"Append\") \\ .save() Notice that we’ve used FailIfNotExist which implies that the operation will fail if the table is not found in the requested cluster and database. The other option is CreateIfNotExist — if the table is not found in the requested cluster and database, it will be created, with a schema matching the DataFrame that is being written. For more refer to https://github.com/Azure/azure-kusto-spark/blob/master/docs/KustoSink.md#supported-options\nGive it a minute for the data to be written. Then you can execute Azure Data Explorer queries to your heart’s content! Try out the below:\n.show ingestion failures StormEvents_2| take 10 StormEvents_2 | summarize event_count=count() by bin(StartTime, 1d) | render timechart What you just executed was just a glimpse of a typical batch based data processing setup. But that’s not always going to be the case!","process-existing-data-in-azure-data-explorer#Process existing data in Azure Data Explorer":"Data Ingestion is key component for a Big Data Analytics services such as Azure Data Explorer. No wonder, it supports a plethora of ways using which you can pull in data from a variety of sources. Although a detailed discussion of ingestion techniques and options, you are welcome to read about it in the documentation.\nIn the interest of time, let’s ingest data manually. Don’t let the word “manually” mislead you. It’s quite simple and fast!\nStart by creating a table (let’s call it StormEvents_1) in the database:\n.create table StormEvents_1 (StartTime: datetime, EndTime: datetime, EpisodeId: int, EventId: int, State: string, EventType: string, InjuriesDirect: int, InjuriesIndirect: int, DeathsDirect: int, DeathsIndirect: int, DamageProperty: int, DamageCrops: int, Source: string, BeginLocation: string, EndLocation: string, BeginLat: real, BeginLon: real, EndLat: real, EndLon: real, EpisodeNarrative: string, EventNarrative: string, StormSummary: dynamic) … and ingest CSV data into the table (directly from Blob storage):\n.ingest into table StormEvents_1 'https://kustosamplefiles.blob.core.windows.net/samplefiles/StormEvents.csv?sv=2019-12-12\u0026ss=b\u0026srt=o\u0026sp=r\u0026se=2022-09-05T02:23:52Z\u0026st=2020-09-04T18:23:52Z\u0026spr=https\u0026sig=VrOfQMT1gUrHltJ8uhjYcCequEcfhjyyMX%2FSc3xsCy4%3D' with (ignoreFirstRecord=true) If you found this technique useful, I encourage you to try out one-click ingestion as well!\nIt might take a minute or so for ingestion to complete. Confirm if data is available and execute simple queries:\n.show ingestion failures StormEvents_1| count StormEvents_1| take 5 StormEvents_1| take 5 | project StartTime, EndTime, State, EventType, DamageProperty, Source The StormEvents_1 table provides some information about storms that happened in the United States. It looks like this:\nFor the subsequent steps, you can either paste the code directly into a Synapse Studio notebook in Azure Synapse Analytics or import this notebook into the workspace.\nStart off with something simple:\nkustoDf = spark.read \\ .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\ .option(\"spark.synapse.linkedService\", \"adx\") \\ .option(\"kustoDatabase\", \"adxdb\") \\ .option(\"kustoQuery\", \"StormEvents_1 | take 10\") \\ .load() display(kustoDf) To read data from Azure Data Explorer, we need to specify thequery using the kustoQuery option. In this case, we are simply executing StormEvents_1 | take 10 to validate the data.\nLet’s try another Kusto query this time:\nfiltered_df = spark.read \\ .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\ .option(\"spark.synapse.linkedService\", \"AzureDataExplorer1\") \\ .option(\"kustoDatabase\", \"mydb\") \\ .option(\"kustoQuery\", \"StormEvents_1 | where DamageProperty \u003e 0 and DeathsDirect \u003e 0 | project EventId, State, StartTime, EndTime, EventType, DamageProperty, DeathsDirect, Source\") \\ .load() filtered_df.createOrReplaceTempView(\"storm_dataset\") This will read all the records into a DataFrame, select the relevant columns and filter the data. For example, we are excluding events where there has not been any property damage or deaths. Finally, we create a temporary view (storm_dataset) in order to perform further data exploration using Apache Spark SQL.\nBefore that, lets use Seaborn (a Python data visualisation library) to draw a simple bar plot:\nimport seaborn as sns import matplotlib.pyplot as plt filtered_df = filtered_df.toPandas() ax = sns.barplot(x=\"DeathsDirect\", y=\"EventType\",data=filtered_df) ax.set_title('deaths per event type') ax.set_xlabel('Deaths#') ax.set_ylabel('Event Type') plt.show() Here is an example for Spark SQL on top of the temporary view.\n%%sql SELECT EventType, AVG(DamageProperty) AS avg_property_damage FROM storm_dataset GROUP BY EventType ORDER BY avg_property_damage DESC We calculated the average damage inflicted by each event type (avalanche, ice storm etc.). The below output is in the form of a column chart (but there are other options as well):\nHere is a slight variation of the above, where we find out the maximum no. of deaths per State.\n%%sql SELECT State , MAX(DeathsDirect) AS deaths FROM storm_dataset GROUP BY State ORDER BY deaths DESC And a Pie chart output this time:\nNow you know how to extract insights from existing data sets in Azure Data Explorer by processing using the Apache Spark pools in Azure Synapse.","quick-recap#Quick recap":"In this blog post, you learned:\nHow to setup and configure Azure Synapse and Azure Data Explorer (including secure access).\nHow to make the most of existing data in Azure Data Explorer and process it using Apache Spark pools in Azure Synapse.\nHow to process data from external sources and write the results back Azure Data Explorer for further analysis.","wrap-up#Wrap up!":"These were simple examples to help you get started. But, the full power of Apache Spark SQL, Python and Scala/Java libraries are available to you. I’d be remiss if I don’t mention Synapse SQL Pools (available in Serverless and Dedicated modes) that allows data access through T-SQL and open possibilities to a wide range of business intelligence, ad-hoc querying tools, and popular drivers.\n🙏🏻 Thanks to Manoj Raheja and Adi Polak for their review and feedback! 🙏🏻"},"title":"Getting started with Azure Data Explorer and Azure Synapse Analytics for Big Data processing"},"/blog/aiml-lambda-polly-texttospeech-go/":{"data":{"":"The field of machine learning has advanced considerably in recent years, enabling us to tackle complex problems with greater ease and accuracy. However, the process of building and training machine learning models can be a daunting task, requiring significant investments of time, resources, and expertise. This can pose a challenge for many individuals and organisations looking to leverage machine learning to drive innovation and growth.\nThat’s where pre-trained AI Services come in and allow users to leverage the power of machine learning without having extensive machine learning expertise and needing to build models from scratch - thereby making it more accessible for a wider audience. AWS machine learning services provide ready-made intelligence for your applications and workflows and easily integrate with your applications. And, if you add Serverless to the mix, well, that’s just icing on the cake, because now you can build scalable and cost-effective solutions without having to worry about the backend infrastructure.\nIn this blog post, you will learn how to build a Serverless text to speech conversion solution using Amazon Polly, AWS Lambda and the Go programming language. Text files uploaded to Amazon Simple Storage Service (S3) will trigger a Lambda function which will convert it into an MP3 audio format (using the AWS Go SDK) and store it in another S3 bucket.\nThe Lambda function is written using the aws-lambda-go library and you will use the Infrastructure-Is-Code paradigm to deploy the solution with AWS CDK (thanks to the Go bindings for AWS CDK)\nAs always, the code is available on GitHub","code-walk-through#Code walk through":"We will only focus on the important parts - some of the code has been omitted for brevity.\nCDK\nYou can refer to the complete CDK code here\nWe start by creating the source and target S3 buckets.\nsourceBucket := awss3.NewBucket(stack, jsii.String(\"source-bucket\"), \u0026awss3.BucketProps{ BlockPublicAccess: awss3.BlockPublicAccess_BLOCK_ALL(), RemovalPolicy: awscdk.RemovalPolicy_DESTROY, AutoDeleteObjects: jsii.Bool(true), }) targetBucket := awss3.NewBucket(stack, jsii.String(\"target-bucket\"), \u0026awss3.BucketProps{ BlockPublicAccess: awss3.BlockPublicAccess_BLOCK_ALL(), RemovalPolicy: awscdk.RemovalPolicy_DESTROY, AutoDeleteObjects: jsii.Bool(true), }) Then, we create the Lambda function and grant it the required permissions to read from the source bucket and write to the target bucket. A managed policy is also attached to the Lambda function’s IAM role to allow it to access Amazon Polly.\nfunction := awscdklambdagoalpha.NewGoFunction(stack, jsii.String(\"text-to-speech-function\"), \u0026awscdklambdagoalpha.GoFunctionProps{ Runtime: awslambda.Runtime_GO_1_X(), Environment: \u0026map[string]*string{\"TARGET_BUCKET_NAME\": targetBucket.BucketName()}, Entry: jsii.String(functionDir), }) sourceBucket.GrantRead(function, \"*\") targetBucket.GrantWrite(function, \"*\") function.Role().AddManagedPolicy(awsiam.ManagedPolicy_FromAwsManagedPolicyName(jsii.String(\"AmazonPollyReadOnlyAccess\"))) We add an event source to the Lambda function to trigger it when a new file is uploaded to the source bucket.\nfunction.AddEventSource(awslambdaeventsources.NewS3EventSource(sourceBucket, \u0026awslambdaeventsources.S3EventSourceProps{ Events: \u0026[]awss3.EventType{awss3.EventType_OBJECT_CREATED}, })) Finally, we export the bucket names as CloudFormation output.\nawscdk.NewCfnOutput(stack, jsii.String(\"source-bucket-name\"), \u0026awscdk.CfnOutputProps{ ExportName: jsii.String(\"source-bucket-name\"), Value: sourceBucket.BucketName()}) awscdk.NewCfnOutput(stack, jsii.String(\"target-bucket-name\"), \u0026awscdk.CfnOutputProps{ ExportName: jsii.String(\"target-bucket-name\"), Value: targetBucket.BucketName()}) Lambda function\nYou can refer to the complete Lambda Function code here\nfunc handler(ctx context.Context, s3Event events.S3Event) { for _, record := range s3Event.Records { sourceBucketName := record.S3.Bucket.Name fileName := record.S3.Object.Key err := textToSpeech(sourceBucketName, fileName) } } The Lambda function is triggered when a new file is uploaded to the source bucket. The handler function iterates over the S3 event records and calls the textToSpeech function to convert the text to speech.\nLet’s go through the textToSpeech function.\nfunc textToSpeech(sourceBucketName, textFileName string) error { voiceID := types.VoiceIdAmy outputFormat := types.OutputFormatMp3 result, err := s3Client.GetObject(context.Background(), \u0026s3.GetObjectInput{ Bucket: aws.String(sourceBucketName), Key: aws.String(textFileName), }) buffer := new(bytes.Buffer) buffer.ReadFrom(result.Body) text := buffer.String() output, err := pollyClient.SynthesizeSpeech(context.Background(), \u0026polly.SynthesizeSpeechInput{ Text: aws.String(text), OutputFormat: outputFormat, VoiceId: voiceID, }) var buf bytes.Buffer _, err = io.Copy(\u0026buf, output.AudioStream) outputFileName := strings.Split(textFileName, \".\")[0] + \".mp3\" _, err = s3Client.PutObject(context.TODO(), \u0026s3.PutObjectInput{ Body: bytes.NewReader(buf.Bytes()), Bucket: aws.String(targetBucket), Key: aws.String(outputFileName), ContentType: output.ContentType, }) return nil } The textToSpeech function first reads the text file from the source bucket.\nIt then calls the Amazon Polly SynthesizeSpeech API to convert the text to speech.\nThe output is an audio stream in MP3 format which is written to the target S3 bucket.","conclusion-and-next-steps#Conclusion and next steps":"In this post, you saw how to create a serverless solution that converts text to speech using Amazon Polly. The entire infrastructure life-cycle was automated using AWS CDK. All this was done using the Go programming language, which is well supported in AWS Lambda and AWS CDK.\nHere are a few things you can try out to improve/extend this solution:\nEnsure that the Lambda function is only provided fine-grained IAM permissions - for S3 and Polly.\nTry using different voices and/or output formats for the text-to-speech conversion.\nTry using different languages for the text-to-speech conversion.\nHappy building!","convert-text-to-speech#Convert text to speech":"Upload a text file to the source S3 bucket - you can use the sample text files provided in the GitHub repository. I will be used the S3 CLI to upload the file, but you can use the AWS console as well.\nexport SOURCE_BUCKET=\u003center source S3 bucket name - check the CDK output\u003e aws s3 cp ./file_1.txt s3://$SOURCE_BUCKET # verify that the file was uploaded aws s3 ls s3://$SOURCE_BUCKET Wait for a while and check the target S3 bucket. You should see a new file with the same name as the text file you uploaded, but with a .mp3 extension - this is the audio file generated by Amazon Polly.\nDownload it using the S3 CLI and play it to verify that the text was converted to speech.\nexport TARGET_BUCKET=\u003center target S3 bucket name - check the CDK output\u003e # list contents of the target bucket aws s3 ls s3://$TARGET_BUCKET # download the audio file aws s3 cp s3://$TARGET_BUCKET/file_1.mp3 . Don’t forget to clean up Once you’re done, to delete all the services, simply use:\ncdk destroy #output prompt (choose 'y' to continue) Are you sure you want to delete: LambdaPollyTextToSpeechGolangStack (y/n)? You were able to setup and try the complete solution. Before we wrap up, let’s quickly walk through some of important parts of the code to get a better understanding of what’s going on behind the scenes.","introduction#Introduction":"Amazon Polly is a cloud-based service that transforms written text into natural-sounding speech. By leveraging Amazon Polly, you can create interactive applications that enhance user engagement and make your content more accessible. With support for various languages and a diverse range of lifelike voices, Amazon Polly empowers you to build speech-enabled applications that cater to the needs of customers across different regions while selecting the perfect voice for your audience.\nCommon use cases for Amazon Polly include, but are not limited to, mobile applications such as newsreaders, games, eLearning platforms, accessibility applications for visually impaired people, and the rapidly growing segment of Internet of Things (IoT).\nE-learning and training: Create engaging audio content for e-learning courses and training materials, providing a more immersive learning experience for students.\nAccessibility: Convert written content into speech, making it accessible to people with visual or reading impairments.\nDigital media: Generate natural-sounding voices for podcasts, audiobooks, and other digital media content, enhancing the overall listening experience.\nVirtual assistants and chatbots: Use lifelike voices to create more natural-sounding responses for virtual assistants and chatbots, making them more user-friendly and engaging.\nCall centers: Create automated voice responses for call centers, reducing the need for live agents and improving customer service efficiency.\nIoT devices: Integrate it into internet of things (IoT) devices, providing a voice interface for controlling smart home devices and other connected devices.\nOverall, Amazon Polly’s versatility and scalability make it a valuable tool for a wide range of applications.\nLet’s learn Amazon Polly with a hands-on tutorial.","pre-requisites#Pre-requisites":"Before you proceed, make sure you have the following installed:\nGo programming language (v1.18 or higher)\nAWS CDK\nAWS CLI\nClone the project and change to the right directory:\ngit clone https://github.com/abhirockzz/ai-ml-golang-polly-text-to-speech cd ai-ml-golang-polly-text-to-speech ","use-aws-cdk-to-deploy-the-solution#Use AWS CDK to deploy the solution":"The AWS Cloud Development Kit (AWS CDK) is a framework that lets you define your cloud infrastructure as code in one of its supported programming and provision it through AWS CloudFormation.\nTo start the deployment, simply invoke cdk deploy and wait for a bit. You will see a list of resources that will be created and will need to provide your confirmation to proceed.\ncd cdk cdk deploy # output Bundling asset LambdaPollyTextToSpeechGolangStack/text-to-speech-function/Code/Stage... ✨ Synthesis time: 5.94s //.... omitted Do you wish to deploy these changes (y/n)? y Enter y to start creating the AWS resources required for the application.\nIf you want to see the AWS CloudFormation template which will be used behind the scenes, run cdk synth and check the cdk.out folder\nYou can keep track of the stack creation progress in the terminal or navigate to AWS console: CloudFormation \u003e Stacks \u003e LambdaPollyTextToSpeechGolangStack.\nOnce the stack creation is complete, you should have:\nTwo S3 buckets - Source bucket to upload text files and the target bucket to store the converted audio files.\nA Lambda function to convert text to audio using Amazon Polly.\n…. along with a few other components (like IAM roles etc.)\nYou will also see the following output in the terminal (resource names will differ in your case). In this case, these are the names of the S3 buckets created by CDK:\n✅ LambdaPollyTextToSpeechGolangStack ✨ Deployment time: 95.95s Outputs: LambdaPollyTextToSpeechGolangStack.sourcebucketname = lambdapollytexttospeechgolan-sourcebuckete323aae3-sh3neka9i4nx LambdaPollyTextToSpeechGolangStack.targetbucketname = lambdapollytexttospeechgolan-targetbucket75a012ad-1tveajlcr1uoo ..... You can now try out the end to end solution!"},"title":"Step-by-Step Guide to Building a Serverless Text to Speech Solution using Golang on AWS"},"/blog/autoscale-redis-apps-kubernetes/":{"data":{"":"","#":"This blog post demonstrates how to auto-scale your Redis based applications on Kubernetes. Redis is a widely used (and loved!) database which supports a rich set of data structures (String, Hash, Streams, Geospatial), as well as other features such as pub/sub messaging, clustering (HA) etc. One such data structure is a List which supports operations such as inserts (LPUSH, RPUSH, LINSERT etc.), reads (LRANGE), deletes (LREM, LPOP etc.) etc. But that’s not all!\nRedis Lists are quite versatile and used as the backbone for implementing scalable architectural patterns such as consumer-producer (based on queues), where producer applications push items into a List, and consumers (also called workers) process those items. Popular projects such as resque, sidekiq, celery etc. use Redis behind the scenes to implement background jobs.\nIn this blog, you will learn how to automatically scale your Celery workers that use Redis as the broker. There are multiple ways to achieve this - this blog uses a Kubernetes-based Event Driven Autoscaler (KEDA) to do the heavy lifting, including scaling up the worker fleet based on workload and also scaling it back to zero if there are no tasks in the queue!\nPlease note that this blog post uses a Golang application (thanks to gocelery!) as an example, but the same applies to Python or any other application that uses the Celery protocol.\nIt covers the following topics:\nStart off with the basics, overview of the application Setup the infra (AKS, Redis) and deploy the worker application along with kEDA Test the end to end auto-scaling in action The sample code is available in this GitHub repository\nTo start off, here is a quick round of introductions!\nCelery In a nutshell, Celery is a distributed message processing system. It uses brokers to orchestrate communication between clients and workers. Client applications add messages (tasks) to the broker, which is then delivered to one or more workers - this setup is horizontal scalable (and highly available) since you can have multiple workers to share the processing load.\nAlthough Celery is written in Python, the good thing is that the protocol can be implemented in any language. This means that you can have client and worker applications written in completely different programming languages (a Node.js based client and a Python based worker app), but they will be able to inter-operate, given they speak the Celery protocol!\nKEDA KEDA can drive the scaling of any container in Kubernetes based on the number of events needing to be processed. It adopts a plug-and-play architecture and builds on top of (extends) existing Kubernetes primitives such as Horizontal Pod Autoscaler.\nA KEDA scaler is responsible for integrating with an external service to fetch the metrics that drives auto scaling. We will be using the KEDA scaler for Redis, that auto scales applications based on the length (number of items) of a Redis List.\nKEDA deep-dive is out of scope of this blog post. To learn more, please refer to the following resources:\nConcepts Architecture Introductory blog post Hands on tutorial using Kafka Before we dive into the nitty-gritty, here is a high level overview of the application.","auto-scaling-in-action#Auto-scaling in action":"We are all set to test the end-to-end setup!\nScale to zero 💥💥 Check the Celery worker Pod:\nkubectl get pods -l=app=celery-worker //output: No resources found No resources found ??? Wait, we had a consumer application Pod ready, what just happened? Don’t worry, this is KEDA in action! Because there are no items in the Redis List right now (hence no work for the worker), KEDA made sure that there are no idle Pods running.\nThis behavior can be controlled by the minReplicaCount attribute in the ScaledObject (refer to the KEDA documentation\nKEDA uses the information in the ScaledObject to create a Horizontal Pod Autoscaler object:\nkubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE keda-hpa-redis-scaledobject Deployment/celery-worker \u003cunknown\u003e/10 (avg) 1 10 0 2m51s Scale UP ⬆️ Let’s run the Celery producer application and simulate some work by pushing items into the Redis List. Before you do that, switch to another terminal and start watching the consumer Deployment to track auto-scaling:\nkubectl get pods -l=app=celery-worker -w Go back to the previous terminal and run the application:\nexport REDIS_HOST=[replace with redis host and post info e.g. foobar.redis.cache.windows.net:6380] export REDIS_PASSWORD=[replace with redis password] docker run --rm -e REDIS_HOST=$REDIS_HOST -e REDIS_PASSWORD=$REDIS_PASSWORD abhirockzz/celery-go-producer //output: celery producer started... Wait for a few seconds. In the other terminal, you will notice that Celery worker Pods are being gradually created:\ncelery-worker-5b644c6688-2zk5c 0/1 ContainerCreating 0 0s celery-worker-5b644c6688-2zk5c 1/1 Running 0 4s celery-worker-5b644c6688-h22hp 0/1 Pending 0 0s celery-worker-5b644c6688-h22hp 0/1 Pending 0 0s celery-worker-5b644c6688-h22hp 0/1 ContainerCreating 0 0s celery-worker-5b644c6688-h22hp 1/1 Running 0 4s celery-worker-5b644c6688-r2m48 0/1 Pending 0 0s celery-worker-5b644c6688-r2m48 0/1 Pending 0 0s celery-worker-5b644c6688-r2m48 0/1 ContainerCreating 0 0s celery-worker-5b644c6688-r2m48 1/1 Running 0 3s If you check the Deployment (kubectl get deployment/celery-worker), you will see something similar to this (depending upon how many Pods have been created):\nNAME READY UP-TO-DATE AVAILABLE AGE celery-worker 3/3 3 3 9m51s You can check the Horizontal Pod Autoscaler as well. It should reflect the same stats:\nkubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE keda-hpa-redis-scaledobject Deployment/celery-worker 9/10 (avg) 1 10 3 8m15s If you happen to check the logs from one of the worker application Pods, you will log output such as this:\n... 2021/03/01 10:05:36 got info - Benjamin Moore liammiller233@test.com 9 Franklin Circle, Burrton, WY, 37213 2021/03/01 10:05:36 worker b2928e0f-2c79-a227-7547-7bd2acdaacba sleeping for 3 2021/03/01 10:05:39 saved hash info users:674 2021/03/01 10:05:39 got info - Lily Smith jacobwilliams126@example.net 84 Jackson Ter, New Deal, FM, 53234 2021/03/01 10:05:39 worker b2928e0f-2c79-a227-7547-7bd2acdaacba sleeping for 7 2021/03/01 10:05:46 saved hash info users:473 2021/03/01 10:05:46 got info - William Williams joshuadavis821@example.com 32 Washington Rdg, Baldock, MN, 60018 2021/03/01 10:05:46 worker b2928e0f-2c79-a227-7547-7bd2acdaacba sleeping for 9 2021/03/01 10:05:55 saved hash info users:275 ... While our workers are happily churning along, let’s check Redis as well. Use redis-cli:\nredis-cli -h [redis host e.g. foobar.redis.cache.windows.net] -p 6380 -a [redis password] --tls First, check the length of the Redis List (named celery in this case). The output will reflect the number jobs that have been pushed in by the producer application and have not been processed yet.\nllen celery (integer) 10 The worker application creates HASH with user info (based on the random data it receives from the producer application). To check, use SCAN:\nscan 0 match users* 1) \"960\" 2) 1) \"users:169\" 2) \"users:272\" 3) \"users:855\" 4) \"users:429\" Check a few entries (using hgetall). For e.g.\nhgetall users:169 1) \"name\" 2) \"Natalie White\" 3) \"email\" 4) \"ethanjackson245@test.net\" 5) \"address\" 6) \"20 Jefferson Ter,\\nDerby Center, ME, 18270\" 7) \"worker\" 8) \"6769253c-9dc3-9232-1860-4bc01ce760a3\" 9) \"processed_at\" 10) \"2021-03-01 10:13:11.230070643 +0000 UTC\" In addition to the user details, notice that the ID of the worker that processed this record is also available. This is to confirm that different worker instances are sharing the workload.\nWe had set 10 as the listLength in the ScaledObject manifest and specified maxReplicaCount as 10 (so the no. of Pods will be capped to this number).\nScale DOWN ⬇️ Stop the producer application.\nOnce all the items in the list are consumed and it’s empty, the Deployment will be scaled down after the cooldown period is reached (200 seconds in this example). Eventually, the number of Pods will go back to zero. You can “rinse and repeat” this again and experiment with different values for the number of messages you want send (simulate load), the no. of replicas you want to scale to, a different thresholdCount etc.","base-setup#Base setup":"To start off, please make sure to:\nSetup a Kubernetes cluster on Azure Create an Azure Cache for Redis instance Install KEDA KEDA allows multiple installation options. I will be using the YAML directly\nKEDA components will be installed into the keda namespace.\nkubectl apply -f https://github.com/kedacore/keda/releases/download/v2.1.0/keda-2.1.0.yaml This will start KEDA Operator and the Metrics API server as separate Deployments\nkubectl get deployment -n keda NAME READY UP-TO-DATE AVAILABLE AGE keda-operator 1/1 1 1 1h keda-operator-metrics-apiserver 1/1 1 1 1h Before you proceed further, wait for the Deployments to be READY\nWe can now deploy the components required to auto scale our application. Start by cloning this repository and change to the correct folder:\ngit clone https://github.com/abhirockzz/redis-celery-kubernetes-keda cd redis-celery-kubernetes-keda Deploy the Celery worker and KEDA ScaledObject We need to deploy the Secret first since its used by the Celery worker Deployment. First, encode (base64) the password for your Redis instance (check Access Keys in Azure Portal) in order to store it as a Secret.\necho 'enter_redis_password' | base64 Replace this in the credentials attribute in secret.yaml. For example, if the password is foobared:\necho -n 'foobared' | base64 //output: Zm9vYmFyZWQ= The final version of secret.yaml will look like this (notice the encoded password in the credentials attribute):\napiVersion: v1 kind: Secret metadata: name: redis-password type: Opaque data: credentials: Zm9vYmFyZWQ= Create the Secret:\nkubectl apply -f deploy/secret.yaml We are almost ready to deploy the Celery worker application. Before that, please update the consumer.yaml file with the Redis host and port. Here is the snippet:\n... env: - name: REDIS_PASSWORD valueFrom: secretKeyRef: name: redis-password key: credentials - name: REDIS_HOST value: [replace with redis host and port e.g. foobar.redis.cache.windows.net:6380] - name: REDIS_LIST value: celery ... celery is the default name of the Redis LIST created by the Celery worker - please leave it unchanged.\nDeploy the worker app, check the Pod and wait for status to transition to Running:\nkubectl apply -f deploy/consumer.yaml kubectl get pods -l=app=celery-worker -w NAME READY STATUS RESTARTS AGE celery-worker-5b644c6688-m8nf4 1/1 Running 0 20s You can check the logs using kubectl logs \u003cpod_name\u003e\nTo deploy the KEDA ScaledObject:\nkubectl apply -f deploy/redis-scaledobject.yaml ","clean-up#Clean up":"Once you’re done, don’t forget to delete the resources you created:\nDelete the Celery worker app, ScaledObject and Secret: kubectl delete -f deploy To uninstall KEDA: kubectl delete -f https://github.com/kedacore/keda/releases/download/v2.1.0/keda-2.1.0.yaml Delete the AKS cluster if you don’t need it anymore: az aks delete --name \u003ccluster name\u003e --resource-group \u003cgroup name\u003e Delete the Azure Cache for Redis instance: az redis delete --name \u003ccache name\u003e --resource-group \u003cgroup name\u003e ","conclusion#Conclusion":"We covered Redis Scaler in this blog, but KEDA offers many such scalers. KEDA deals with auto scaling your applications, but, what if you could run all of these application instances on an infrastructure other than the Kubernetes cluster nodes for e.g. a Serverless platform?\nIf this sounds interesting, do check out Virtual Nodes in Azure Kubernetes Service to see how you can use them to seamlessly scale your applications to Azure Container Instances and benefit from quick provisioning of pods, and only pay per second for their execution time. The virtual nodes add-on for AKS, is based on the open source project Virtual Kubelet which is an open source Kubernetes kubelet implementation.","high-level-architecture#High-level architecture":"The application includes the following components:\nRedis (used as the Celery broker) A producer which simulates a client application that submits tasks The worker application (running in Kubernetes) which processes the tasks Producer application The producer is a Go application that submits tasks to Redis (using gocelery library). You can check the code on GitHub, but here is a snippet:\ngo func() { for !closed { _, err := celeryClient.Delay(taskName, rdata.FullName(rdata.RandomGender), rdata.Email(), rdata.Address()) if err != nil { panic(err) } time.Sleep(1 * time.Second) } }() It runs a loop (as a goroutine) and sends randomly generate data (full name, email and address).\nThe producer application is available as a pre-built Docker image abhirockzz/celery-go-producer, however, you could choose to build another image using the Dockerfile provided in the repo.\nCelery worker The Celery worker application processes this information (via the Redis job queue). In this case, the processing logic involves storing data in a Redis HASH (but it could be anything). You can check the code on GitHub, but here is a snippet:\nsave := func(name, email, address string) string { sleepFor := rand.Intn(9) + 1 time.Sleep(time.Duration(sleepFor) * time.Second) info := map[string]string{\"name\": name, \"email\": email, \"address\": address, \"worker\": workerID, \"processed_at\": time.Now().UTC().String()} hashName := hashNamePrefix + strconv.Itoa(rand.Intn(1000)+1) _, err := redisPool.Get().Do(\"HSET\", redis.Args{}.Add(hashName).AddFlat(info)...) if err != nil { log.Fatal(err) } return hashName } The sleep has been added on purpose to allow the worker application can pause anywhere between 0 to 10 seconds (this is random). This will help simulate a “high load” scenario and will help demonstrate the auto-scaling (details in the upcoming sections).\nThe worker application is available as a pre-built Docker image abhirockzz/celery-go-worker, however, you could choose to build another image using the Dockerfile provided in the repo.\nKEDA ScaledObject A ScaledObject associates the Deployment we want to auto scale (in this case, its the Celery worker application) with the source of the metrics (length of a Redis List):\napiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: redis-scaledobject namespace: default spec: scaleTargetRef: kind: Deployment name: celery-worker pollingInterval: 15 cooldownPeriod: 200 maxReplicaCount: 10 triggers: - type: redis metadata: addressFromEnv: REDIS_HOST passwordFromEnv: REDIS_PASSWORD enableTLS: \"true\" listName: celery listLength: \"10\" Here is a summary of the attributes used in the manifest:\n(spec.scaleTargetRef.deploymentName) specifies which Deployment to target for auto scale. The trigger type is redis and the triggers.metadata section is used to provide additional details: the values for address in this example is REDIS_HOST, which is the name of the environment variable which is expected to be present in the Deployment (at runtime) listName is the name of the Redis List whose pending items are used to drive auto scale process listLength is the threshold (number of List items) over which a new Pod (for the specified Deployment) is created. In this example, a new Pod will be created for every 10 pending items in the Redis List (the number has been kept low for ease of testing) maxReplicaCount defines the upper limit to which the application can scale i.e. it is maximum number of Pods that can be created, irrespective of the scale criteria It’s time to move on to the practical stuff. But, before you go there, make sure you have the following ready:","pre-requisites#Pre-requisites":"To work through the application in this blog, you will need:\nAn Azure account. You can create a free account to get 12 months of free services. Docker Kubernetes cluster along with kubectl: I have used Azure Kubernetes Service, although minikube should work just as well. Install Azure CLI Redis: I have used Azure Cache for Redis, but feel free to explore other options e.g. you can install one in your Kubernetes cluster using a Helm chart). In the upcoming sections, we will:\nInstall KEDA Deploy individual components - Celery worker, ScaledObject etc. Generate load and test auto scaling "},"title":"Autoscaling Redis applications on Kubernetes 🚀🚀"},"/blog/azure-cosmosdb-use-cases-tradeoffs/":{"data":{"":"Azure Cosmos DB is a fully managed, elastically scalable and globally distributed database with a multi-model approach, and provides you with the ability to use document, key-value, wide-column, or graph-based data.\nWe will drill further into the multi-model capabilities and explore the options that are available to store and access data. Hopefully, it can help you make an informed decision on the right API are the right choice.\nCore (SQL) API: Flexibility of a NoSQL document store combined with the power of SQL for querying. MongoDB API: Supports the MongoDB wire protocol so that existing MongoDB client continue to work with Azure Cosmos DB as if they are running against an actual MongoDB database. Cassandra API: Supports the Cassandra wire protocol so that existing Apache drivers compliant with CQLv4 continue to work with Azure Cosmos DB as if they are running against an actual Cassandra database. Gremlin API: Supports graph data with Apache TinkerPop (a graph computing framework) and the Gremlin query language. Table API: Provides premium capabilities for applications written for Azure Table storage. You can read further on the some of the Key Benefits\nBefore diving in, let’s look at some of the common scenarios which Azure Cosmos DB is used for. This is by no means, an exhaustive list.\nModern gaming services need to deliver customized and personalized content like in-game stats, social media integration, and high-score leaderboards. As a fully managed offering, Azure Cosmos DB requires minimal setup and management to allow for rapid iteration, and reduced time to market.\nE-commerce platforms and retail applications store catalog data and for event sourcing in order processing pipelines.\nApplications that provide personalized experiences can be quiet complex. They need to be able to retrieve user specific settings effectively to render UI elements and experiences quickly.","conclusion#Conclusion":"Azure Cosmos DB supports offers a lot of options and flexibility in terms of APIs, and has it’s pros and cons depending on your use-case and requirements. Although the Core (SQL) is quite versatile and applicable to a wide range of scenarios, if your data is better represented in terms of relationships, then the Gremlin (graph) API is a suitable choice. In case you have existing applications using Cassandra or MongoDB, then migrating them to the respective Azure Cosmos DB APIs provides a path of least resistance with added benefits. In case you want to migrate from Azure Table Storage and do not want to refactor your application to use the Core (SQL) API, remember that you have the option of choosing the Azure Cosmos DB Table API, that can provide API compatibility with Table Storage as well as capabilities such as guaranteed high availability, automatic secondary indexing and much more!","core-sql-api-best-of-both-worlds#Core (SQL) API: best of both worlds":"Core (SQL) API is the default Azure Cosmos DB API. You can store data using JSON documents to represent your data, but there are a couple ways you can retrieve it:\nSQL queries: Write queries using the Structured Query Language (SQL) as a JSON query language Point reads: Key/value style lookup on a single item ID and partition key (these are cheaper and faster than SQL queries) For many applications, semi-structured data model can provide the flexibility they need. For example, an e-commerce setup with large number of product categories. You will need to add new product categories and support operations (search, sort etc.) across many product attributes. You can model this with a relational database, but, the constantly evolving product categories may require downtime to update the table schemas, queries, and databases.\nWith the Core (SQL), introducing a new product category is as simple as adding a document for the new product without schema changes or downtimes. The Azure Cosmos DB MongoDB API is also suitable for these requirements, but Core (SQL) API has an advantage since it’s supports SQL-like queries on top of a flexible data model.\nConsider the example of a blogging platform where users can create posts, like and add comments to those posts.\nYou can represent a post as such:\n{ \"id\": \"\u003cpost-id\u003e\", \"type\": \"post\", \"postId\": \"\u003cpost-id\u003e\", \"userId\": \"\u003cpost-author-id\u003e\", \"userUsername\": \"\u003cpost-author-username\u003e\", \"title\": \"\u003cpost-title\u003e\", \"content\": \"\u003cpost-content\u003e\", \"commentCount\": \u003ccount-of-comments\u003e, \"likeCount\": \u003ccount-of-likes\u003e, \"creationDate\": \"\u003cpost-creation-date\u003e\" } As well as comments and likes:\n{ \"id\": \"\u003ccomment-id\u003e\", \"type\": \"comment\", \"postId\": \"\u003cpost-id\u003e\", \"userId\": \"\u003ccomment-author-id\u003e\", \"userUsername\": \"\u003ccomment-author-username\u003e\", \"content\": \"\u003ccomment-content\u003e\", \"creationDate\": \"\u003ccomment-creation-date\u003e\" } { \"id\": \"\u003clike-id\u003e\", \"type\": \"like\", \"postId\": \"\u003cpost-id\u003e\", \"userId\": \"\u003cliker-id\u003e\", \"userUsername\": \"\u003cliker-username\u003e\", \"creationDate\": \"\u003clike-creation-date\u003e\" } To handle likes and comments, you can use a stored procedure:\nfunction createComment(postId, comment) { var collection = getContext().getCollection(); collection.readDocument( `${collection.getAltLink()}/docs/${postId}`, function (err, post) { if (err) throw err; post.commentCount++; collection.replaceDocument( post._self, post, function (err) { if (err) throw err; comment.postId = postId; collection.createDocument( collection.getSelfLink(), comment ); } ); }) } If you want to learn more, refer to Working with JSON in Azure Cosmos DB","migrate-from-an-existing-mongodb-instance#Migrate from an existing MongoDB instance":"Azure Cosmos DB implements the wire protocol for MongoDB which allows transparent compatibility with native MongoDB client SDKs, drivers, and tools. This means that any MongoDB client driver (as well as existing tools such as Studio 3T) that understands this protocol version should be able to natively connect to Cosmos DB.\nFor example, if you have an existing MongoDB database as the data store for purchase orders processing application to capture failed and partial orders, fulfillment data, shipping status with different formats. Due to increasing data volumes, you want to migrate to a scalable cloud based solution and continue using MongoDB - It makes perfect sense to use Azure Cosmos DB’s API for MongoDB. But you will want to do so with minimal code changes to the existing application and migrate the current data with as little downtime as possible.\nWith Azure Database Migration Service, you can perform an online (minimal downtime) migration and elastically scale the provisioned throughput and storage for your Cosmos databases based on your need and pay only for the throughput and storage you need. This leads to significant cost savings.\nRefer to the pre and post-migration guides.\nOnce you’ve migrated your data, you can continue to use your existing application. Here is an example using MongoDB .NET driver:\nInitialize the client:\nMongoClientSettings settings = new MongoClientSettings(); settings.Server = new MongoServerAddress(host, 10255); settings.UseSsl = true; settings.SslSettings = new SslSettings(); settings.SslSettings.EnabledSslProtocols = SslProtocols.Tls12; MongoIdentity identity = new MongoInternalIdentity(dbName, userName); MongoIdentityEvidence evidence = new PasswordEvidence(password); settings.Credential = new MongoCredential(\"SCRAM-SHA-1\", identity, evidence); MongoClient client = new MongoClient(settings); Retrieve the database and the collection:\nprivate string dbName = \"Tasks\"; private string collectionName = \"TasksList\"; var database = client.GetDatabase(dbName); var todoTaskCollection = database.GetCollection\u003cMyTask\u003e(collectionName); Insert a task into the collection:\npublic void CreateTask(MyTask task) { var collection = GetTasksCollectionForEdit(); try { collection.InsertOne(task); } catch (MongoCommandException ex) { string msg = ex.Message; } } Retrieve all tasks:\ncollection.Find(new BsonDocument()).ToList(); You should also carefully consider how MongoDB write/read concerns are mapped to the Azure Cosmos consistency levels, manage indexing and leverage Change feed support.\nPlease note that the Core (SQL) API would have been an appropriate choice if there wasn’t a requirement to reuse existing code and import data from existing MongoDB database.\nHandle real-time data with Cassandra If your application needs to handle high volume, real-time data, Apache Cassandra is an ideal choice. With Azure Cosmos DB Cassandra API, you can use existing Apache drivers compliant with CQLv4, and in most cases, you should be able to switch from using Apache Cassandra to using Azure Cosmos DB’s Cassandra API, by just changing a connection string. You can also continue to use Cassandra-based tools such as cqlsh.\nFor advanced analytics use cases, you should combine Azure Cosmos DB Cassandra API with Apache Spark. You can use the familiar Spark connector for Cassandra to connect to Azure Cosmos DB Cassandra API. Additionally, you will also need the azure-cosmos-cassandra-spark-helper helper library from Azure Cosmos DB for features such as custom connection factories and retry policies.\nIt’s possible to access Azure Cosmos DB Cassandra API from Azure Databricks as well as Spark on YARN with HDInsight\nTo connect to Cosmos DB, you can use the Scala API as such:\nimport org.apache.spark.sql.cassandra._ import com.datastax.spark.connector._ import com.datastax.spark.connector.cql.CassandraConnector import com.microsoft.azure.cosmosdb.cassandra spark.conf.set(\"spark.cassandra.connection.host\",\"YOUR_ACCOUNT_NAME.cassandra.cosmosdb.azure.com\") spark.conf.set(\"spark.cassandra.connection.port\",\"10350\") spark.conf.set(\"spark.cassandra.connection.ssl.enabled\",\"true\") spark.conf.set(\"spark.cassandra.auth.username\",\"YOUR_ACCOUNT_NAME\") spark.conf.set(\"spark.cassandra.auth.password\",\"YOUR_ACCOUNT_KEY\") spark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\") spark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\") spark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\") spark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"1000\") spark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\") spark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\") spark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"600000000\") You can perform aggregations, such as average, min/max, sum etc.:\nspark .read .cassandraFormat(\"books\", \"books_ks\", \"\") .load() .select(\"book_price\") .agg(avg(\"book_price\")) .show spark .read .cassandraFormat(\"books\", \"books_ks\", \"\") .load() .select(\"book_id\",\"book_price\") .agg(min(\"book_price\")) .show spark .read .cassandraFormat(\"books\", \"books_ks\", \"\") .load() .select(\"book_price\") .agg(sum(\"book_price\")) .show Connect everything using the Gremlin (Graph) API As part of a personalized customer experience, your application may need to provide recommendations for products on the website. For example, “people who bought product X also bought product Y”. Your use cases might also need to predict customer behavior, or connect people with others with similar interests.\nAzure Cosmos DB supports Apache Tinkerpop’s graph traversal language (known as Gremlin). There are many use cases with give rise to common problems associated with lack of flexibility and relational approaches. For example, managing connected social networks, Geospatial use cases such as finding a location of interest within an area or locate the shortest/optimal route between two location, or modelling network and connections between IoT devices as a graph in order to build a better understanding of the state of your devices and assets. On top of this, you can continue to enjoy features common to all the Azure Cosmos DB APIs such as global distribution, elastic scaling of storage and throughput, automatic indexing and query and tunable consistency levels.\nFor example, you could use the following commands to add three vertices for product and two edges for related-purchases to a graph:\ng.addV('product').property('productName', 'Industrial Saw').property('description', 'Cuts through anything').property('quantity', 261) g.addV('product').property('productName', 'Belt Sander').property('description', 'Smoothes rough edges').property('quantity', 312) g.addV('product').property('productName', 'Cordless Drill').property('description', 'Bores holes').property('quantity', 647) g.V().hasLabel('product').has('productName', 'Industrial Saw').addE('boughtWith').to(g.V().hasLabel('product').has('productName', 'Belt Sander')) g.V().hasLabel('product').has('productName', 'Industrial Saw').addE('boughtWith').to(g.V().hasLabel('product').has('productName', 'Cordless Drill')) Then, you can query the additional products that were purchased along with the ‘Industrial Saw’:\ng.V().hasLabel('product').has('productName', 'Industrial Saw').outE('boughtWith') Here is what the results will look like:\n[ { \"id\": \"6c69fba7-2f76-421f-a24e-92d4b8295d67\", \"label\": \"boughtWith\", \"type\": \"edge\", \"inVLabel\": \"product\", \"outVLabel\": \"product\", \"inV\": \"faaf0997-f5d8-4d01-a527-ae29534ac234\", \"outV\": \"a9b13b8f-258f-4148-99c0-f71b30918146\" }, { \"id\": \"946e81a9-8cfa-4303-a999-9be3d67176d5\", \"label\": \"boughtWith\", \"type\": \"edge\", \"inVLabel\": \"product\", \"outVLabel\": \"product\", \"inV\": \"82e1556e-f038-4d7a-a02a-f780a2b7215c\", \"outV\": \"a9b13b8f-258f-4148-99c0-f71b30918146\" } ] In addition to traditional clients, you can use the graph bulk executor .NET library to perform bulk operations in Azure Cosmos DB Gremlin API. Using this will improve the data migration efficiency as compared to using a Gremlin client. Traditionally, inserting data with Gremlin will require the application send a query at a time that will need to be validated, evaluated, and then executed to create the data. The bulk executor library will handle the validation in the application and send multiple graph objects at a time for each network request. Here is an example of how to create Vertices and Edges:\nIBulkExecutor graphbulkExecutor = new GraphBulkExecutor(documentClient, targetCollection); BulkImportResponse vResponse = null; BulkImportResponse eResponse = null; try { vResponse = await graphbulkExecutor.BulkImportAsync( Utils.GenerateVertices(numberOfDocumentsToGenerate), enableUpsert: true, disableAutomaticIdGeneration: true, maxConcurrencyPerPartitionKeyRange: null, maxInMemorySortingBatchSize: null, cancellationToken: token); eResponse = await graphbulkExecutor.BulkImportAsync( Utils.GenerateEdges(numberOfDocumentsToGenerate), enableUpsert: true, disableAutomaticIdGeneration: true, maxConcurrencyPerPartitionKeyRange: null, maxInMemorySortingBatchSize: null, cancellationToken: token); } catch (DocumentClientException de) { Trace.TraceError(\"Document client exception: {0}\", de); } catch (Exception e) { Trace.TraceError(\"Exception: {0}\", e); } Although it is possible to model and store this data using the Core (SQL) API as JSON documents, it’s not suitable for queries which need to determine relationships between entities (e.g. products).\nYou may want to explore Graph data modeling as well as data partitioning for the Azure Cosmos DB Gremlin API.\nAzure Table Storage on steroids! If you have existing applications written for Azure Table storage, they can be migrated to Azure Cosmos DB by using the Table API with no code changes and take advantage of premium capabilities.\nMoving your database from Azure Table Storage into Azure Cosmos DB with a low throughput could bring lots of benefits in terms of latency (single-digit millisecond for reads and writes), throughput, global distribution, comprehensive SLAs and as well as cost (you can use consumption-based or provisioned capacity modes. Storing table data in Cosmos DB automatically indexes all the properties (without an index management overhead) as opposed to Table Storage that only allows for indexing on the Partition and Row keys.\nThe Table API has client SDKs available for .NET, Java, Python, and Node.js. For example, with the Java client, use a connection string to store the table endpoint and credentials:\npublic static final String storageConnectionString = \"DefaultEndpointsProtocol=https;\" + \"AccountName=your_cosmosdb_account;\" + \"AccountKey=your_account_key;\" + \"TableEndpoint=https://your_endpoint;\" ; .. and perform CRUD (create, read, update, delete) operations as such:\ntry { CloudStorageAccount storageAccount = CloudStorageAccount.parse(storageConnectionString); CloudTableClient tableClient = storageAccount.createCloudTableClient(); CloudTable cloudTable = tableClient.getTableReference(\"customers\"); cloudTable.createIfNotExists(); for (String table : tableClient.listTables()) { System.out.println(table); } CustomerEntity customer = ....; TableOperation insert = TableOperation.insertOrReplace(customer); cloudTable.execute(insert); TableOperation retrieve = TableOperation.retrieve(\"Smith\", \"Jeff\", CustomerEntity.class); CustomerEntity result = cloudTable.execute(retrieve).getResultAsType(); TableOperation del = TableOperation.delete(jeff); cloudTable.execute(del); } Refer to Where is Table API not identical with Azure Table storage behavior? for more details.","which-api-to-use-and-when#Which API to use, and when?":"There is no perfect formula! At times, the choice will be clear, but other scenarios may require some analysis.\nA rather obvious point to consider is, whether there are existing applications that use any of the supported APIs via the wire protocol (i.e. Cassandra and MongoDB)? If the answer is yes, you should consider using the specific Azure Cosmos DB API, as that will reduce your migration tasks, and make the best use of previous experience in your team.\nIf you expect the schema to change a lot, you may want to leverage a document database, making Core (SQL) a good choice (although the MongoDB API should also be considered).\nIf your data model consists of relationships between entities with associated metadata, you’re better off using the graph support in Azure Cosmos DB Gremlin API.\nIf you are currently using Azure Table Storage, the Core (SQL) API would be a better choice, as it offers a richer query experience, with improved indexing over the Table API. If you don’t want to rewrite your application, consider migrating to the Azure Cosmos DB Table API.\nLet’s look each of the APIs and apply some of this."},"title":"Azure Cosmos DB: Use Cases and Trade-Offs"},"/blog/azure-redis-tls-tip/":{"data":{"":"","#":"Azure Cache for Redis provides an in-memory data store based on the open-source software Redis.\nAs a part of the industry-wide push toward the exclusive use of Transport Layer Security (TLS) version 1.2 or later, Azure Cache for Redis will not support TLS versions 1.0 and 1.1 i.e. your application will be required to use TLS 1.2 or later to communicate with your cache\nTo read the details, please refer to this page from the product documentation\nIt might be helpful to know how will this might manifest in your Go apps (I am using go-redis client as an example)\nIf you don’t specify TLS at all e.g.\nc := redis.NewClient(\u0026redis.Options{Addr: endpoint, Password: password}) err := c.Ping().Err() if err != nil { log.Fatal(err) } defer c.Close() .. you will encounter this error i/o timeout (probably not that helpful)\nIf the specified TLS version is less than 1.2 e.g.\ntlsConfig := \u0026tls.Config{MaxVersion: tls.VersionTLS11, MinVersion: tls.VersionTLS10} c := redis.NewClient(\u0026redis.Options{Addr: endpoint, Password: password, TLSConfig: tlsConfig}) err := c.Ping().Err() if err != nil { log.Fatal(err) } defer c.Close() ..you will end up an tls: DialWithDialer timed out error (again, not that obvious)\nThe solution is obvious though If you don’t set MaxVersion or MinVersion i.e. use tlsConfig := \u0026tls.Config{} it will work since MaxVersion defaulta to TLS1.3 (see https://golang.org/pkg/crypto/tls/#Config)\nFor sake of clarity, it’s better to be explicit i.e.\ntlsConfig := \u0026tls.Config{MinVersion: tls.VersionTLS12} I hope this proves helpful if you stumble across any issues while connecting to Azure Cache for Redis with Go\nCheers!"},"title":"Tip: Using the latest TLS version with Azure Cache for Redis"},"/blog/azure-stream-analytics-tutorial/":{"data":{"":"","#":"With traditional architectures, it’s quite hard to counter challenges imposed by real-time streaming data – one such use case is joining streams of data from disparate sources. For example, think about a system that accepts processed orders from customers (real time, high velocity data source) and the requirement is to enrich these “raw” orders with additional customer info such as name, email, location etc. A possible solution is to build a service that fetches customer data for each customer ID from an external system (for example, a database), perform a join (in-memory) and stores the enriched data in another database perhaps (materialized view). This has several problems though and one of them is not being able to keep up (process with low latency) with a high volume data.\nStream Processing solutions are custom built for these kind of problems. One of them is Azure Stream Analytics, that is a real-time analytics and complex event-processing engine, designed to analyse and process high volumes of fast streaming data from multiple sources simultaneously.\nIt supports the notion of a Job, each of which consists of an Input, Query, and an Output. Azure Stream Analytics can ingest data from Azure Event Hubs (including Azure Event Hubs from Apache Kafka), Azure IoT Hub, or Azure Blob Storage. The query, which is based on SQL query language, can be used to easily filter, sort, aggregate, and join streaming data over a period of time.\nHands-on tutorial This GitHub repository contains a sample application to demonstrate the related concepts and provides step-by-step guide to setup and run the end to end demo. It showcases how to build a data enrichment pipeline with streaming joins using a combination of Azure Event Hubs for data ingestion, Azure SQL Database for storing reference data, Azure Stream Analytics for data processing and Azure Cosmos DB for storing “enriched” data.\nThese are powerful, off-the-shelf services which you will be able to configure and use without setting up any infrastructure. You should be able to go through this tutorial using the Azure Portal (or Azure CLI), without writing any code!\nhttps://github.com/abhirockzz/streaming-data-pipeline-azure\nTL;DR\nHere are the individual components:\nAzure Event Hubs (Input Data source) - ingests raw orders data Azure SQL Database (Reference Data source) - stores reference customer data Azure Stream Analytics (Stream Processing) - joins the stream of orders data from Azure Event Hubs with the static reference customers data Azure Cosmos DB (Output data source) - acts as a “sink” to store enriched orders info I hope this helps you get started with Azure Stream Analytics and test the waters before moving on to more involved use cases. In addition to this, there is plenty of material for you to dig in!\nExplore Architecture patterns Reference solutions such as Twitter sentiment analysis, fraud detection, IoT data processing etc. Common query patterns in Azure Stream Analytics "},"title":"Build a pipeline to join streams of real time data"},"/blog/build-a-mcp-server-using-go-to-interact-with-database/":{"data":{"":"\nWhen it comes to building AI agents, the Model Context Protocol (MCP) is a game-changer. It allows you to create agents that can interact with various data sources, including databases, using a standardized protocol. In this blog post, I’ll show you how to build a simple MCP server using Go that interacts with Azure Cosmos DB. This server will expose tools for performing CRUD operations on Cosmos DB, making it easy for AI agents to access and manipulate data.\nLike many of you, I have been playing around with Model Context Protocol (MCP). To dive in, I built a sample MCP server implementation for Azure Cosmos DB with Go. It uses the Go SDK, and mcp-go as the MCP Go implementation.\nThis MCP server exposes the following tools for interacting with Azure Cosmos DB:\nList Databases: Retrieve a list of all databases in a Cosmos DB account. List Containers: Retrieve a list of all containers in a specific database. Read Container Metadata: Fetch metadata or configuration details of a specific container. Create Container: Create a new container in a specified database with a defined partition key. Add Item to Container: Add a new item to a specified container in a database. Read Item: Read a specific item from a container using its ID and partition key. Execute Query: Execute a SQL query on a Cosmos DB container with optional partition key scoping. Here is a demo (recommend watching at 2x speed 😉) using VS Code Insiders in Agent mode:\n{% embed https://www.youtube.com/watch?v=CsM-mspWJeM %}","azure-cosmos-db-rbac-permissions-and-authentication#Azure Cosmos DB RBAC permissions and authentication":" The user principal you will be using should have permissions (control and data plane) to execute CRUD operations on database, container, and items.\nAuthentication\nLocal credentials - Just login locally using Azure CLI (az login) and the MCP server will use the DefaultAzureCredential implementation automatically. Or, you can set the COSMOSDB_ACCOUNT_KEY environment variable in the MCP server configuration: { \"servers\": { \"CosmosDB Golang MCP\": { \"type\": \"stdio\", \"command\": \"/Users/demo/mcp_azure_cosmosdb\", \"env\": { \"COSMOSDB_ACCOUNT_KEY\": \"enter the key\" } } } } You are good to go! Now spin up VS Code Insiders in Agent Mode, or any other MCP tool (like Claude Desktop) and try this out!","how-to-run#How to run":" git clone https://github.com/abhirockzz/mcp_cosmosdb_go cd mcp_cosmosdb_go go build -o mcp_azure_cosmosdb main.go Configure the MCP server:\nmkdir -p .vscode # Define the content for mcp.json MCP_JSON_CONTENT=$(cat \u003c\u003cEOF { \"servers\": { \"CosmosDB Golang MCP\": { \"type\": \"stdio\", \"command\": \"$(pwd)/mcp_azure_cosmosdb\" } } } EOF ) # Write the content to mcp.json echo \"$MCP_JSON_CONTENT\" \u003e .vscode/mcp.json ","local-devtesting#Local dev/testing":"\nStart with MCP inspector - npx @modelcontextprotocol/inspector ./mcp_azure_cosmosdb","mcp-mcp-everywhere#MCP, MCP everywhere!":"\nWhile this is not an “official” Cosmos DB MCP server, I really wanted to try MCP with Go and also demonstrate the Go SDK for Azure Cosmos DB. Win win 🙌 After all, why should Pythonistas have all the fun 🤷‍♂️ Huge props to the creator of the mcp-go project, Ed Zynda! MCP (at the moment) does not have an “official” Go SDK (yeah I know, shocking right!). But there are discussions going on and I encourage all Gophers to chime in!\nMCP is touted as the next big thing in agentic app landscape. Let’s see how that goes. In the meanwhile, keep building stuff and trying out new things. Try out the MCP server and let me know how it goes!"},"title":"Build a MCP server using Go to connect AI agents with databases"},"/blog/building-event-driven-go-applications-with-azure-cosmos-db-and-azure-functions/":{"data":{"":"\nThe Go programming language is a great fit for building serverless applications. Go applications can be easily compiled to a single, statically linked binary, making deployment simple and reducing external dependencies. They start up quickly, which is ideal for serverless environments where functions are frequently invoked from a cold start. Go applications also tend to use less memory compared to other languages, helping optimize resource usage and reduce costs in serverless scenarios.\nAzure Functions supports Go using custom handlers, and you can use triggers and input and output bindings via extension bundles. Azure Functions is tightly integrated with Azure Cosmos DB using bindings (input, output), and triggers.\nThis blog post will walk you through how to build Azure Functions with Go that make use of these Azure Cosmos DB integrations. Bindings allow you to easily read and write data to Cosmos DB, while triggers are useful for building event-driven applications that respond to changes in your data in Cosmos DB.\nPart 1 of this blog starts off with a function that gets triggered by changes in a Cosmos DB container and simply logs the raw Azure Functions event payload and the Cosmos DB document. You will learn how to run the function and also test it with Cosmos DB locally, thanks to the Cosmos DB emulator and Azure Functions Core Tools. If this is your first time working with Go and Azure Functions, you should find it helpful to get up and running quickly. Although you can deploy it to Azure, we will save that for the next part of this blog.\nPart 2 dives into another function that generates embeddings for the documents in the Cosmos DB container. This example will use an Azure OpenAI embedding model to generate embeddings for the documents in the container and then store the embeddings back in the container. This is useful for building applications that require semantic search or other generative AI applications.\nCheck out the GitHub repository for the complete code.","conclusion#Conclusion":"In this blog post, you learned how to build Azure Functions with Go that use Cosmos DB triggers and bindings. You started with a simple function that logs the raw event payload and the Cosmos DB document, and then moved on to a more complex function that generates embeddings for the documents in the Cosmos DB container using Azure OpenAI. You also learned how to run the functions locally using the Cosmos DB emulator and Azure Functions Core Tools, and how to deploy them to Azure.\nYou can use these examples as a starting point for building your own serverless applications with Go and Azure Functions. The combination of Go’s performance and simplicity, along with Azure Functions’ scalability and integration with Cosmos DB, makes it a powerful platform for building modern applications.","part-1-build-a-simple-cosmos-db-trigger-based-function-and-run-it-locally#Part 1: Build a simple Cosmos DB trigger-based function and run it locally":"Just as the Cosmos DB emulator lets you run Cosmos DB locally, Azure Functions Core Tools lets you develop and test your functions locally.\nStart by installing the Azure Functions Core Tools – refer to the documentation for instructions for your OS. For example, on Linux, you can:\nsudo apt-get update sudo apt-get install azure-functions-core-tools-4 Next, start the Cosmos DB emulator. The commands below are for Linux and use the Docker container-based approach - refer to the documentation for other options.\nYou need to have Docker installed and running on your machine. If you don’t have it installed, please refer to the Docker installation guide.\ndocker pull mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest docker run \\ --publish 8081:8081 \\ --name linux-emulator \\ -e AZURE_COSMOS_EMULATOR_PARTITION_COUNT=1 \\ mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest Make sure to configure the emulator SSL certificate as well. For example, for the Linux system I was using, I ran the following command to download the certificate and regenerate the certificate bundle:\ncurl --insecure https://localhost:8081/_explorer/emulator.pem \u003e ~/emulatorcert.crt sudo update-ca-certificates Use the following URL to navigate to the Cosmos DB Data Explorer using your browser: http://localhost:8081/_explorer/index.html. Create the following resources:\nA database A container with partition key /id – this is the source container A lease container with the name leases and partition key /id – it is used by the trigger to keep track of the changes in the source container. Clone the GitHub repository with the code for the function:\ngit clone https://github.com/abhirockzz/golang_cosmosdb_azure_functions.git cd golang_cosmosdb_azure_functions/getting_started_guide Create a local.settings.json file with the Cosmos DB related info. Use the same database and container names as you created in the previous step. The local.settings.json file is used to store the configuration settings for your function app when running locally:\n{ \"IsEncrypted\": false, \"Values\": { \"AzureWebJobsStorage\": \"\", \"FUNCTIONS_WORKER_RUNTIME\": \"custom\", \"COSMOS_CONNECTION\": \"AccountEndpoint=https://localhost:8081/;AccountKey=C2y6yDjf5/R+ob0N8A7Cgv30VRDJIWEHLM+4QDU5DE2nQ9nDuVTqobD4b8mGGyPMbIZnqyMsEcaGQy67XIw/Jw==;\", \"COSMOS_DATABASE_NAME\": \"test\", \"COSMOS_CONTAINER_NAME\": \"tasks\" } } COSMOS_CONNECTION has a static value for the connection string for the Cosmos DB emulator – do not change it.\nBuild the Go function binary using the following command. This will create a binary file named main in the current directory:\ngo build -o main main.go Start the function locally:\nfunc start This will start the function app and listen for incoming requests. You should see output similar to this:\n[2025-04-25T07:44:53.921Z] Worker process started and initialized. Functions: processor: cosmosDBTrigger For detailed output, run func with --verbose flag. [2025-04-25T07:44:58.809Z] Host lock lease acquired by instance ID '0000000000000000000000006ADD8D3E'. //... Add data to the source container in Cosmos DB. You can do this by navigating to Data Explorer in the emulator. For example, add a document with the following JSON:\n{ \"id\": \"42\", \"description\": \"test\" } The function should be triggered automatically when the document is added to the container. You can check the logs of the function app to see if it was triggered successfully:\n[2025-04-25T07:48:10.559Z] Executing 'Functions.processor' (Reason='New changes on container tasks at 2025-04-25T07:48:10.5593689Z', Id=7b62f8cf-683b-4a5b-9db0-83d049bc4c86) [2025-04-25T07:48:10.565Z] processor function invoked... [2025-04-25T07:48:10.565Z] Raw event payload: {{\"[{\\\"id\\\":\\\"42\\\",\\\"description\\\":\\\"test\\\",\\\"_rid\\\":\\\"AxI2AL1rrFoDAAAAAAAAAA==\\\",\\\"_self\\\":\\\"dbs/AxI2AA==/colls/AxI2AL1rrFo=/docs/AxI2AL1rrFoDAAAAAAAAAA==/\\\",\\\"_etag\\\":\\\"\\\\\\\"00000000-0000-0000-b5b6-6123f4d401db\\\\\\\"\\\",\\\"_attachments\\\":\\\"attachments/\\\",\\\"_ts\\\":1745567285,\\\"_lsn\\\":4}]\"}} {{processor 2025-04-25T07:48:10.560243Z 4f29b3f3-ba95-4043-9b67-2856a43b4734}}} [2025-04-25T07:48:10.566Z] Cosmos DB document: {42 AxI2AL1rrFoDAAAAAAAAAA== dbs/AxI2AA==/colls/AxI2AL1rrFo=/docs/AxI2AL1rrFoDAAAAAAAAAA==/ \"00000000-0000-0000-b5b6-6123f4d401db\" attachments/ 1745567285 4} [2025-04-25T07:48:10.566Z] Executed 'Functions.processor' (Succeeded, Id=7b62f8cf-683b-4a5b-9db0-83d049bc4c86, Duration=6ms) //..... How it works Here is a very high-level overview of the code:\nmain.go – Implements an HTTP server with a processor endpoint. When triggered, it reads a Cosmos DB trigger payload from the request, parses the nested documents, logs information, and returns a structured JSON response. It uses types and helpers from the common package.\ncommon package: Contains shared types and utilities for Cosmos DB trigger processing:\npayload.go: Defines data structures for the trigger payload, documents, and response. parse.go: Provides a Parse function to extract and unmarshal documents from the trigger payload’s nested JSON structure. ","part-2-use-azure-openai-to-generate-embeddings-for-the-documents-in-the-cosmos-db-container#Part 2: Use Azure OpenAI to generate embeddings for the documents in the Cosmos DB container":"In addition to its low-latency, high-performance, and scalability characteristics, its support for Vector (semantic/similarity), Full-text, and Hybrid search makes Azure Cosmos DB a great fit for generative AI applications.\nConsider a use case for managing a product catalog for an e-commerce platform. Each time a new product is added to the system (with a short description like “Bluetooth headphones with noise cancellation”), we want to immediately make that item searchable semantically. As soon as the product document is written to Cosmos DB, an Azure Function is triggered. It extracts the product description, generates a vector embedding using Azure OpenAI, and writes the embedding back to the same document using an output binding. With the embedding in place, the product is now indexed and ready for semantic and hybrid search queries, without any additional effort.\nPrerequisites You will run this example in Azure, so you need to have an Azure account. If you don’t have one, you can create a free account.\nCreate an Azure Cosmos DB for NoSQL account. Enable the vector indexing and search feature – this is a one-time operation.\nJust like before, you will need to create the following resources:\nA database A container with partition key /id – this is the source container A lease container with the name leases and partition key /id – it is used by the trigger to keep track of the changes in the source container. The lease container needs to be created in advance since we have configured Azure Functions to use managed identity to access the Cosmos DB account – you don’t need to use keys or connection strings.\nCreate an Azure OpenAI Service resource. Azure OpenAI Service provides access to OpenAI’s models including GPT-4o, GPT-4o mini (and more), as well as embedding models. Deploy an embedding model of your choice using the Azure AI Foundry portal (for example, I used the text-embedding-3-small model). Just like the Cosmos DB account, the Azure Function app uses a managed identity to access the Azure OpenAI Service resource.\nDeploy resources Move into the right directory:\ncd ../embeddings_generator To simplify the deployment of the function app along with the required resources and configuration, you can use the deploy.sh script. At a high level, it:\nSets up environment variables for Azure resources. Creates an Azure resource group, storage account, and function app plan. Deploys a custom Go-based Azure Function App. Builds the Go binary for Windows. Publishes the function app to Azure. Enables the function app system identity and provides it the required roles for Cosmos DB and Azure OpenAI resource access. Before you deploy the solution, update the local.settings.json. Use the same database and container names as you created in the previous step:\n{ \"IsEncrypted\": false, \"Values\": { \"AzureWebJobsStorage\": \"\", \"FUNCTIONS_WORKER_RUNTIME\": \"custom\", \"COSMOS_CONNECTION__accountEndpoint\": \"https://ENTER_COSMOSDB_ACCOUNT_NAME.documents.azure.com:443/\", \"COSMOS_DATABASE_NAME\": \"name of the database\", \"COSMOS_CONTAINER_NAME\": \"name of the container\", \"COSMOS_HASH_PROPERTY\": \"hash\", \"COSMOS_VECTOR_PROPERTY\": \"embedding\", \"COSMOS_PROPERTY_TO_EMBED\": \"description\", \"OPENAI_DEPLOYMENT_NAME\": \"enter the embedding model deployment name e.g. text-embedding-3-small\", \"OPENAI_DIMENSIONS\": \"enter the dimensions e.g. 1536\", \"OPENAI_ENDPOINT\": \"https://ENTER_OPENAI_RESOURCE_NAME.openai.azure.com/\" } } COSMOS_CONNECTION_accountEndpoint: Endpoint URL for the Azure Cosmos DB account. COSMOS_DATABASE_NAME: Name of the Cosmos DB database to use. COSMOS_CONTAINER_NAME: Name of the Cosmos DB container to use. COSMOS_HASH_PROPERTY: Name of the property used as a hash in Cosmos DB documents (no need to modify this). COSMOS_VECTOR_PROPERTY: Name of the property storing vector embeddings in Cosmos DB. COSMOS_PROPERTY_TO_EMBED: Name of the property whose value will be embedded. Change this based on your document structure. OPENAI_DEPLOYMENT_NAME: Name of the Azure OpenAI model deployment to use for embeddings. OPENAI_DIMENSIONS: Number of dimensions for the embedding vectors. OPENAI_ENDPOINT: Endpoint URL for the Azure OpenAI resource. Run the deploy.sh script:\nchmod +x deploy.sh ./deploy.sh As part of the azure functionapp publish command that’s used in the script, you will be prompted to overwrite the value of the existing AzureWebJobsStorage setting in the local.settings.json file to Azure – choose “no”.\nRun the end-to-end example Add data to the source container in Cosmos DB. For example, add a document with the following JSON:\n{ \"id\": \"de001c6d-4efe-4a65-a59a-39a0580bfa2a\", \"description\": \"Research new technology\" } The function should be triggered automatically when the document is added to the container. You can check the logs of the function app to see if it was triggered successfully:\nfunc azure functionapp logstream \u003cFUNCTION_APP_NAME\u003e You should see logs similar to this (the payload will be different depending on the data you add):\n2025-04-23T05:34:41Z [Information] function invoked 2025-04-23T05:34:41Z [Information] cosmosVectorPropertyName: embedding 2025-04-23T05:34:41Z [Information] cosmosVectorPropertyToEmbedName: description 2025-04-23T05:34:41Z [Information] cosmosHashPropertyName: hash 2025-04-23T05:34:41Z [Information] Processing 1 documents 2025-04-23T05:34:41Z [Information] Processing document ID: de001c6d-4efe-4a65-a59a-39a0580bfa2a 2025-04-23T05:34:41Z [Information] Document data: Research new technology 2025-04-23T05:34:41Z [Information] New document detected, generated hash: 5bb57053273563e2fbd4202c666373ccd48f86eaf9198d7927a93a555aa200aa 2025-04-23T05:34:41Z [Information] Document modification status: true, hash: 5bb57053273563e2fbd4202c666373ccd48f86eaf9198d7927a93a555aa200aa 2025-04-23T05:34:41Z [Information] Created embedding for document: map[description:Research new technology id:de001c6d-4efe-4a65-a59a-39a0580bfa2a] 2025-04-23T05:34:41Z [Information] Adding 1 document with embeddings 2025-04-23T05:34:41Z [Information] Added enriched documents to binding output 2025-04-23T05:34:41Z [Information] Executed 'Functions.cosmosdbprocessor' (Succeeded, Id=91f4760f-047a-4867-9030-46a6602ab179, Duration=128ms) //.... Verify the data in Cosmos DB. You should see an embedding for the description property of the document stored in the embedding property. It should look something like this:\n{ \"id\": \"de001c6d-4efe-4a65-a59a-39a0580bfa2a\", \"description\": \"Research new technology\", \"embedding\": [ 0.028226057, -0.00958694 //.... ], \"hash\": \"5bb57053273563e2fbd4202c666373ccd48f86eaf9198d7927a93a555aa200aa\" } Once the embeddings are generated, you can integrate this with generative AI applications. For example, you can use the Vector Search feature of Azure Cosmos DB to perform similarity searches based on the embeddings.\nHow it works Here is a very high-level overview of the code:\nmain.go: Implements an HTTP server with a cosmosdbprocessor endpoint. When triggered, it reads a Cosmos DB trigger payload from the request, parses the nested documents, generates embeddings using Azure OpenAI, and writes the enriched documents back to the Cosmos DB container. Exposes the cosmosdbprocessor endpoint, which processes incoming Cosmos DB documents. For each document, checks if it is new or modified (using a hash), generates an embedding (vector) using Azure OpenAI, and prepares enriched documents for output. Handles logging and error reporting for the function execution. common package: Contains shared utilities and types for processing Cosmos DB documents embedding.go: Handles creation of embeddings using Azure OpenAI. parse.go: Parses and extracts documents from the Cosmos DB trigger payload. payload.go: Defines data structures for payloads and responses used across the project. The function uses a hash property to check if the document has already been processed. If the hash value is different from the one stored in Cosmos DB, it means that the document has been modified and needs to be re-processed. In this case, the function will generate a new embedding and update the document with the new hash value. This ensures that the function does not get stuck in an infinite loop. If the hash value is the same, it means that the document has not been modified and does not need to be re-processed. In this case, the function will log that the document is unchanged and will not generate a new embedding.\nYou should see logs similar to this:\n2025-04-23T05:34:42Z [Information] function invoked 2025-04-23T05:34:42Z [Information] cosmosVectorPropertyName: embedding 2025-04-23T05:34:42Z [Information] cosmosVectorPropertyToEmbedName: description 2025-04-23T05:34:42Z [Information] cosmosHashPropertyName: hash 2025-04-23T05:34:42Z [Information] Processing 1 document 2025-04-23T05:34:42Z [Information] Processing document ID: de001c6d-4efe-4a65-a59a-39a0580bfa2a 2025-04-23T05:34:42Z [Information] Document data: Research new technology 2025-04-23T05:34:42Z [Information] Document unchanged, hash: 5bb57053273563e2fbd4202c666373ccd48f86eaf9198d7927a93a555aa200aa 2025-04-23T05:34:42Z [Information] Document modification status: false, hash: 2025-04-23T05:34:42Z [Information] Executed 'Functions.cosmosdbprocessor' (Succeeded, Id=f0cf039a-5de5-4cc1-b29d-928ce32b294e, Duration=6ms) //.... Delete resources Be sure to clean up the resources you created in Azure. You can do this using the Azure portal or the Azure CLI. For example, to delete the resource group and all its resources, run:\naz group delete --name \u003cresource-group-name\u003e This will delete the resource group and all its resources, including the Cosmos DB account, function app, and storage account."},"title":"Building Event-Driven Go applications with Azure Cosmos DB and Azure Functions"},"/blog/building-resilient-go-applications-simple-tip-for-mocking-and-testing-database-error-responses/":{"data":{"":"\nWhen building applications that rely on databases (which is almost every application, right?), one of the biggest challenges developers face is testing how their code handles various error scenarios. What happens when the database returns a HTTP 400 error? How does your application respond to throttling? Will your retry logic work as expected?\nThese questions are crucial because in production, errors are inevitable. This holds true for Azure Cosmos DB as well. The database’s distributed nature means that errors can arise from various sources, including network issues (503 Service Unavailable), request timeouts (408 Request timeout), rate limits (429 Too many requests), and more. Therefore, robust error handling and testing are essential to maintain a reliable application that handles these gracefully rather than crashing or losing data.\nTesting edge cases and different permutations of error scenarios traditionally requires a lot of effort and can be quite complex. Common approaches include:\nTriggering real errors in your development environment (unreliable and hard to reproduce) Mocking entire SDK responses (complex and may not reflect real behavior) Waiting for errors to happen in production (definitely not ideal!) This is where error simulation can come in handy. By intercepting HTTP requests and selectively returning error responses, we can test specific scenarios in a controlled, repeatable manner.","1-custom-transport-layer-for-injecting-errors#1. Custom transport layer for injecting errors":"CustomTransport403Error is a custom HTTP transport that intercepts requests before they reach Azure Cosmos DB. This transport examines each request and decides whether to simulate an error based on the operation type (e.g., ReadItem). If the request matches the criteria, it returns a simulated error response; otherwise, it allows the request to proceed normally.\nIt returns a 403 Forbidden HTTP response with a specific sub-status code (3 - WriteForbidden), and its wrapped in an azcore.ResponseError to closely mirror what Azure Cosmos DB would actually return. The x-ms-substatus header is particularly important for Cosmos DB applications, as it provides specific context about why an operation failed. It’s included to make sure that error handling code processes responses exactly as it would in production.\nYou can customize the error simulation logic as needed.\ntype CustomTransport403Error struct{} func (t *CustomTransport403Error) Do(req *http.Request) (*http.Response, error) { isReadItemOperation := req.Method == \"GET\" \u0026\u0026 strings.Contains(req.URL.Path, \"/docs/\") if isReadItemOperation { fmt.Printf(\"CustomTransport403Error: Simulating 403 error for ReadItem operation: %s\\n\", req.URL.String()) // Create a simulated 403 response with sub-status 3 header := make(http.Header) header.Set(\"x-ms-substatus\", \"3\") header.Set(\"x-ms-activity-id\", \"readitem-test-activity-id\") header.Set(\"x-ms-request-id\", \"readitem-test-request-id\") header.Set(\"Content-Type\", \"application/json\") response := \u0026http.Response{ StatusCode: 403, Status: \"403 Forbidden\", Header: header, Body: io.NopCloser(strings.NewReader(`{\"code\": \"Forbidden\", \"message\": \"Simulated 403 error for ReadItem with sub-status 3\"}`)), Request: req, } // Return both the response and the error so the SDK can handle it properly responseErr := azruntime.NewResponseError(response) return response, responseErr } // For all other operations, return a successful response // ... (successful response creation code) } The Go SDK for Azure Cosmos DB retries requests that return certain error codes, such as 403 (Forbidden), and others. This custom transport allows you to simulate these conditions without needing to actually hit the database.","2-observing-sdk-retries-using-custom-policies#2. Observing SDK retries using custom policies":"I covered Retry Policies in a previous blog post – How to configure and customize the Go SDK for Azure Cosmos DB.\nIn this example, a custom policy is used to provide visibility into the retry behavior. It logs detailed information about each retry attempt:\ntype RetryLoggingPolicy struct{} func (p *RetryLoggingPolicy) Do(req *policy.Request) (*http.Response, error) { // Call the next policy in the chain resp, err := req.Next() // If there's an error, log the details if err != nil { var azErr *azcore.ResponseError if errors.As(err, \u0026azErr) { subStatus := azErr.RawResponse.Header.Get(\"x-ms-substatus\") if subStatus == \"\" { subStatus = \"N/A\" } fmt.Printf(\"RetryLoggingPolicy: ResponseError during retry - Status: %d, SubStatus: %s, URL: %s\\n\", azErr.StatusCode, subStatus, req.Raw().URL.String()) } } return resp, err } This is really useful for understanding how your application behaves under error conditions. When applications encounter errors, the SDK automatically retries those requests. They might ultimately succeed after a few attempts, but you may want to have visibility into this process.\nYou can plug in a custom RetryLoggingPolicy to intercept these retries and log relevant information, such as the status code, sub-status code, and the URL of the request. This helps you understand how your application behaves during error conditions.","3-integration-and-error-handling-verification#3. Integration and error handling verification":"The main function ties everything together. It sets up the Cosmos DB client with the custom transport and retry policy, then performs a ReadItem operation. If an error occurs, it uses the handlerError function to extract and log the status code and sub-status code from the error response.\nThe custom transport and retry policy are integrated into the application as part of ClientOptions.\nThe custom transport is configured as the Transporter which represents an HTTP pipeline transport used to send HTTP requests and receive responses. The retry policy is added to the PerRetryPolicies list. Each policy is executed once per request, and for each retry of that request. func main() { opts := \u0026azcosmos.ClientOptions{ ClientOptions: azcore.ClientOptions{ PerRetryPolicies: []policy.Policy{ \u0026RetryLoggingPolicy{}, // This will log error details during retries }, Transport: \u0026CustomTransport403Error{}, // Use the custom transport }, } // ... client creation and ReadItem operation ... _, err = container.ReadItem(context.Background(), partitionKey, \"testid\", nil) handlerError(err) } func handlerError(err error) { if err != nil { var azErr *azcore.ResponseError if errors.As(err, \u0026azErr) { fmt.Printf(\"error status code: %d\\n\", azErr.StatusCode) subStatus := azErr.RawResponse.Header.Get(\"x-ms-substatus\") if subStatus == \"\" { subStatus = \"N/A\" } fmt.Printf(\"error sub-status code: %s\\n\", subStatus) } } } ","conclusion#Conclusion":"By making error scenarios easy to reproduce and test, you’re more likely to build applications that handle them gracefully. This pattern can be extended to test various scenarios such as different HTTP status codes (429 for throttling, for example), network timeouts, intermittent failures that succeed after retries, circuit breaker patterns, fallback mechanisms, and more.\nTo begin with, you can focus on combination specific operations (like read, or write) and error types that are most relevant to your application. You can gradually expand this to cover more complex scenarios, such as simulating throttling, server errors, or even network timeouts.","simulating-errors-using-pluggable-mechanisms#Simulating errors using pluggable mechanisms":"There are multiple ways to test error scenarios – from integration testing with real services to comprehensive mocking frameworks. This particular example uses the Go SDK for Azure Cosmos DB to illustrate how to simulate HTTP error conditions (403 Forbidden) for a specific operation (such as ReadItem). Note that this is just one technique in a broader toolkit of testing strategies.\nFollow the next steps if you want to run this as a standalone Go application and see how it works in practice. Or, feel free to skip to the next section for a walkthrough.\nStep 1: Copy the code below into a file named main.go:\npackage main import ( \"context\" \"errors\" \"fmt\" \"io\" \"net/http\" \"strings\" \"github.com/Azure/azure-sdk-for-go/sdk/azcore\" azlog \"github.com/Azure/azure-sdk-for-go/sdk/azcore/log\" \"github.com/Azure/azure-sdk-for-go/sdk/azcore/policy\" azruntime \"github.com/Azure/azure-sdk-for-go/sdk/azcore/runtime\" \"github.com/Azure/azure-sdk-for-go/sdk/azidentity\" \"github.com/Azure/azure-sdk-for-go/sdk/data/azcosmos\" ) func init() { azlog.SetListener(func(cls azlog.Event, msg string) { // Log retry-related events switch cls { case azlog.EventRetryPolicy: fmt.Printf(\"Retry Policy Event: %s\\n\", msg) } }) // Set logging level to include retries azlog.SetEvents(azlog.EventRetryPolicy) } // CustomTransport403Error implements policy.Transporter to simulate 403 errors only for ReadItem operations type CustomTransport403Error struct{} func (t *CustomTransport403Error) Do(req *http.Request) (*http.Response, error) { // Check if this is a ReadItem operation (typically a GET request with an item id in the path) // ReadItem URLs look like: /dbs/{db}/colls/{container}/docs/{id} isReadItemOperation := req.Method == \"GET\" \u0026\u0026 strings.Contains(req.URL.Path, \"/docs/\") if isReadItemOperation { fmt.Printf(\"CustomTransport403Error: Simulating 403 error for ReadItem operation: %s\\n\", req.URL.String()) // Create a simulated 403 response with sub-status 3 header := make(http.Header) header.Set(\"x-ms-substatus\", \"3\") header.Set(\"x-ms-activity-id\", \"readitem-test-activity-id\") header.Set(\"x-ms-request-id\", \"readitem-test-request-id\") header.Set(\"Content-Type\", \"application/json\") response := \u0026http.Response{ StatusCode: 403, Status: \"403 Forbidden\", Header: header, Body: io.NopCloser(strings.NewReader(`{\"code\": \"Forbidden\", \"message\": \"Simulated 403 error for ReadItem with sub-status 3\"}`)), Request: req, } // Return both the response and the error so the SDK can handle it properly responseErr := azruntime.NewResponseError(response) return response, responseErr } // For all other operations (like account properties), use a fake successful response fmt.Printf(\"CustomTransport403Error: Allowing operation: %s %s\\n\", req.Method, req.URL.String()) // Create a fake successful response for account properties and other operations header := make(http.Header) header.Set(\"Content-Type\", \"application/json\") header.Set(\"x-ms-activity-id\", \"success-activity-id\") header.Set(\"x-ms-request-id\", \"success-request-id\") response := \u0026http.Response{ StatusCode: 200, Status: \"200 OK\", Header: header, Body: io.NopCloser(strings.NewReader(\"\")), Request: req, } return response, nil } // RetryLoggingPolicy logs error details during retries type RetryLoggingPolicy struct{} func (p *RetryLoggingPolicy) Do(req *policy.Request) (*http.Response, error) { // fmt.Println(\"RetryLoggingPolicy: Starting retry with request URL:\", req.Raw().URL.String()) // Call the next policy in the chain resp, err := req.Next() // If there's an error, log the details if err != nil { var azErr *azcore.ResponseError if errors.As(err, \u0026azErr) { subStatus := azErr.RawResponse.Header.Get(\"x-ms-substatus\") if subStatus == \"\" { subStatus = \"N/A\" } fmt.Printf(\"RetryLoggingPolicy: ResponseError during retry - Status: %d, SubStatus: %s, URL: %s\\n\", azErr.StatusCode, subStatus, req.Raw().URL.String()) } else { fmt.Printf(\"RetryLoggingPolicy: Non-ResponseError during retry - %T: %v, URL: %s\\n\", err, err, req.Raw().URL.String()) } } else if resp != nil \u0026\u0026 resp.StatusCode \u003e= 400 { // Log HTTP error responses even if they don't result in Go errors subStatus := resp.Header.Get(\"x-ms-substatus\") if subStatus == \"\" { subStatus = \"N/A\" } fmt.Printf(\"RetryLoggingPolicy: HTTP error response - Status: %d, SubStatus: %s, URL: %s\\n\", resp.StatusCode, subStatus, req.Raw().URL.String()) } return resp, err } func main() { opts := \u0026azcosmos.ClientOptions{ ClientOptions: azcore.ClientOptions{ PerRetryPolicies: []policy.Policy{ \u0026RetryLoggingPolicy{}, // This will log error details during retries }, Transport: \u0026CustomTransport403Error{}, // Use the selective transport to simulate 403 errors only for ReadItem }, } creds, _ := azidentity.NewDefaultAzureCredential(nil) client, err := azcosmos.NewClient(\"https://i_dont_exist.documents.azure.com:443\", creds, opts) if err != nil { fmt.Printf(\"NewClient Error occurred: %v\\n\", err) return } // Test the ReadItem operation container, err := client.NewContainer(\"dummy\", \"dummy\") if err != nil { fmt.Printf(\"NewContainer Error occurred: %v\\n\", err) return } partitionKey := azcosmos.NewPartitionKeyString(\"testpk\") _, err = container.ReadItem(context.Background(), partitionKey, \"testid\", nil) handlerError(err) } func handlerError(err error) { if err != nil { fmt.Println(\"ReadItem Error occurred\") // Debug: Print the actual error type fmt.Printf(\"Error type: %T\\n\", err) // fmt.Printf(\"Error value: %v\\n\", err) var azErr *azcore.ResponseError if errors.As(err, \u0026azErr) { fmt.Println(\"Successfully unwrapped to azcore.ResponseError using errors.As\") fmt.Printf(\"error status code: %d\\n\", azErr.StatusCode) subStatus := azErr.RawResponse.Header.Get(\"x-ms-substatus\") if subStatus == \"\" { subStatus = \"N/A\" } fmt.Printf(\"error sub-status code: %s\\n\", subStatus) } } } Step 2: Use the following commands to run the application:\ngo mod init demo go mod tidy go run main.go Lets break this down and understand how it works."},"title":"Building resilient Go applications: Simple tip for mocking and testing database error responses"},"/blog/cdk-apprunner-kafka-dynamodb-eks/":{"data":{"":"A previous blog post covered how to deploy a Go Lambda function and trigger it in response to events sent to a topic in a MSK Serverless cluster.\nThis blog will take it a notch further.\nThe solution consists of a MSK Serverless cluster, a producer application on AWS App Runner and a consumer application in Kubernetes (EKS) persisting data to DynamoDB. The core components (MSK cluster, EKS and DynamoDB) and the producer application will be provisioned using Infrastructure-as-code with AWS CDK. Since the consumer application on EKS will interact with both MSK and DynamoDB, you will also need to configure appropriate IAM roles. All the components in this solution have been written in Go.\nThe MSK producer and consumer app use the franz-go library (it also supports MSK IAM authentication). The CDK stacks have been written using CDK Go library. ","conclusion#Conclusion":"You were able to deploy the end to end application using CDK. This comprised of a producer on App Runner sending data to MSK Serverless cluster and a consumer on EKS persisting data to DynamoDB. All the components were written using the Go programming language!","configure-irsa-for-consumer-application#Configure IRSA for consumer application":"Exit the cdk directory and change to the root of the project:\ncd .. Create an IAM OIDC identity provider for your cluster with eksctl export EKS_CLUSTER_NAME=\u003cEKS cluster name\u003e oidc_id=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5) aws iam list-open-id-connect-providers | grep $oidc_id eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --approve Define IAM roles for the application Configure IAM Roles for Service Accounts (also known as IRSA).\nRefer to the documentation for details\nStart by creating a Kubernetes Service Account:\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: eks-app-sa EOF To verify - kubectl get serviceaccount/eks-app-sa -o yaml\nSet your AWS Account ID and OIDC Identity provider as environment variables:\nACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) export AWS_REGION=\u003center region e.g. us-east-1\u003e OIDC_PROVIDER=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\") Create a JSON file with Trusted Entities for the role:\nread -r -d '' TRUST_RELATIONSHIP \u003c\u003cEOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:aud\": \"sts.amazonaws.com\", \"${OIDC_PROVIDER}:sub\": \"system:serviceaccount:default:eks-app-sa\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" \u003e trust.json To verify - cat trust.json\nNow, create the IAM role:\nexport ROLE_NAME=msk-consumer-app-irsa aws iam create-role --role-name $ROLE_NAME --assume-role-policy-document file://trust.json --description \"IRSA for MSK consumer app on EKS\" You will need to create and attach policy to role since we only want the consumer application to consume data from MSK cluster and put data to DynamoDB table - this needs to be fine-grained.\nIn the policy.json file, replace values for MSK cluster and DynamoDB. Create and attach the policy to the role you just created:\naws iam create-policy --policy-name msk-consumer-app-policy --policy-document file://policy.json aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/msk-consumer-app-policy Finally, associate the IAM role with the Kubernetes Service Account that you created earlier:\nkubectl annotate serviceaccount -n default eks-app-sa eks.amazonaws.com/role-arn=arn:aws:iam::$ACCOUNT_ID:role/$ROLE_NAME #confirm kubectl get serviceaccount/eks-app-sa -o yaml ","deploy-msk-consumer-application-to-eks#Deploy MSK consumer application to EKS":" You can refer to the consumer application code here.\nMake sure to update consumer application manifest (app-iam.yaml) with the MSK cluster endpoint and ECR image (obtained from the stack output).\nkubectl apply -f msk-consumer/app-iam.yaml # verify Pods kubectl get pods -l=app=msk-iam-consumer-app ","deploy-msk-producer-application-to-app-runner-using-cdk#Deploy MSK producer application to App Runner using CDK":"Deploy the second CDK stack.\nNote that in addition to deploying the producer application to App Runner, it also builds and uploads the consumer application Docker image to an ECR repository.\nMake sure to enter the MSK Serverless broker endpoint URL.\nexport MSK_BROKER=\u003center endpoint\u003e export MSK_TOPIC=test-topic cdk deploy AppRunnerServiceStack Wait for the the producer application to get deployed to App Runner. You can check its progress in the AWS CloudFormation console.\nYou can take a look at the CDK stack code here and the producer application here.\nOnce complete, make a note of the App Runner application public endpoint as well as the ECR repository for the consumer application. You should see these in the stack output as such:\nOutputs: AppRunnerServiceStack.AppURL = \u003capp URL\u003e AppRunnerServiceStack.ConsumerAppDockerImage = \u003cecr docker image\u003e .... Now, you can verify if the application is functioning properly. Get the publicly accessible URL for the App Runner application and invoke it using curl. This will create the MSK topic and send data specified in the HTTP POST body.\nexport APP_RUNNER_URL=\u003center app runner URL\u003e curl -i -X POST -d '{\"email\":\"user1@foo.com\",\"name\":\"user1\"}' $APP_RUNNER_URL Now you can deploy the consumer application to the EKS cluster. Before that, execute the steps to configure appropriate permissions for the application to interact with MSK and DynamoDB.","prerequisites#Prerequisites":"You will need the following:\nAn AWS account Install AWS CDK, AWS CLI, Docker, eksctl and curl. ","use-cdk-to-provision-msk-eks-and-dynamodb#Use CDK to provision MSK, EKS and DynamoDB":"All the code and config is present in this GitHub repo. Clone the GitHub repo and change to the right directory:\ngit clone https://github.com/abhirockzz/msk-cdk-apprunner-eks-dynamodb cd msk-cdk-apprunner-eks-dynamodb/cdk Deploy the first CDK stack:\ncdk deploy MSKDynamoDBEKSInfraStack Wait for the all the components to get provisioned, including MSK Serverless cluster, EKS cluster and DynamoDB. You can check its progress in the AWS CloudFormation console.\nYou can take a look at the CDK stack code here.","verify-end-to-end-solution#Verify end to end solution":"Continue to send records using App Runner producer application:\nexport APP_RUNNER_URL=\u003center app runner URL\u003e curl -i -X POST -d '{\"email\":\"user2@foo.com\",\"name\":\"user2\"}' $APP_RUNNER_URL curl -i -X POST -d '{\"email\":\"user3@foo.com\",\"name\":\"user3\"}' $APP_RUNNER_URL curl -i -X POST -d '{\"email\":\"user4@foo.com\",\"name\":\"user4\"}' $APP_RUNNER_URL Check consumer app logs on EKS to verify:\nkubectl logs -f $(kubectl get pods -l=app=msk-iam-consumer-app -o jsonpath='{.items[0].metadata.name}') Scale out consumer app The MSK topic created by the producer application has three topic partitions, so we can have maximum of three consumer instances. Scale out to three replicas:\nkubectl scale deployment/msk-iam-consumer-app --replicas=3 Verify the number of Pods and check logs for each of them. Notice how the data consumption is being balanced across the three instances.\nkubectl get pods -l=app=msk-iam-consumer-app "},"title":"Use CDK to deploy a complete solution with Kafka, App Runner, EKS and DynamoDB"},"/blog/cdk-msk-apprunner-eks-dynamodb/":{"data":{"":"A previous blog post covered how to deploy a Go Lambda function and trigger it in response to events sent to a topic in a MSK Serverless cluster.\nThis blog will take it a notch further.\nThe solution consists of a MSK Serverless cluster, a producer application on AWS App Runner and a consumer application in Kubernetes (EKS) persisting data to DynamoDB. The core components (MSK cluster, EKS and DynamoDB) and the producer application will be provisioned using Infrastructure-as-code with AWS CDK. Since the consumer application on EKS will interact with both MSK and DynamoDB, you will also need to configure appropriate IAM roles. All the components in this solution have been written in Go.\nThe MSK producer and consumer app use the franz-go library (it also supports MSK IAM authentication). The CDK stacks have been written using CDK Go library. ","conclusion#Conclusion":"You were able to deploy the end to end application using CDK. This comprised of a producer on App Runner sending data to MSK Serverless cluster and a consumer on EKS persisting data to DynamoDB. All the components were written using the Go programming language!","configure-irsa-for-consumer-application#Configure IRSA for consumer application":"Exit the cdk directory and change to the root of the project:\ncd .. Create an IAM OIDC identity provider for your cluster with eksctl export EKS_CLUSTER_NAME=\u003cEKS cluster name\u003e oidc_id=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5) aws iam list-open-id-connect-providers | grep $oidc_id eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --approve Define IAM roles for the application Configure IAM Roles for Service Accounts (also known as IRSA).\nRefer to the documentation for details\nStart by creating a Kubernetes Service Account:\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: eks-app-sa EOF To verify - kubectl get serviceaccount/eks-app-sa -o yaml\nSet your AWS Account ID and OIDC Identity provider as environment variables:\nACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) export AWS_REGION=\u003center region e.g. us-east-1\u003e OIDC_PROVIDER=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\") Create a JSON file with Trusted Entities for the role:\nread -r -d '' TRUST_RELATIONSHIP \u003c\u003cEOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:aud\": \"sts.amazonaws.com\", \"${OIDC_PROVIDER}:sub\": \"system:serviceaccount:default:eks-app-sa\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" \u003e trust.json To verify - cat trust.json\nNow, create the IAM role:\nexport ROLE_NAME=msk-consumer-app-irsa aws iam create-role --role-name $ROLE_NAME --assume-role-policy-document file://trust.json --description \"IRSA for MSK consumer app on EKS\" You will need to create and attach policy to role since we only want the consumer application to consume data from MSK cluster and put data to DynamoDB table - this needs to be fine-grained.\nIn the policy.json file, replace values for MSK cluster and DynamoDB. Create and attach the policy to the role you just created:\naws iam create-policy --policy-name msk-consumer-app-policy --policy-document file://policy.json aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/msk-consumer-app-policy Finally, associate the IAM role with the Kubernetes Service Account that you created earlier:\nkubectl annotate serviceaccount -n default eks-app-sa eks.amazonaws.com/role-arn=arn:aws:iam::$ACCOUNT_ID:role/$ROLE_NAME #confirm kubectl get serviceaccount/eks-app-sa -o yaml ","deploy-msk-consumer-application-to-eks#Deploy MSK consumer application to EKS":" You can refer to the consumer application code here.\nMake sure to update consumer application manifest (app-iam.yaml) with the MSK cluster endpoint and ECR image (obtained from the stack output).\nkubectl apply -f msk-consumer/app-iam.yaml # verify Pods kubectl get pods -l=app=msk-iam-consumer-app ","deploy-msk-producer-application-to-app-runner-using-cdk#Deploy MSK producer application to App Runner using CDK":"Deploy the second CDK stack.\nNote that in addition to deploying the producer application to App Runner, it also builds and uploads the consumer application Docker image to an ECR repository.\nMake sure to enter the MSK Serverless broker endpoint URL.\nexport MSK_BROKER=\u003center endpoint\u003e export MSK_TOPIC=test-topic cdk deploy AppRunnerServiceStack Wait for the the producer application to get deployed to App Runner. You can check its progress in the AWS CloudFormation console.\nYou can take a look at the CDK stack code here and the producer application here.\nOnce complete, make a note of the App Runner application public endpoint as well as the ECR repository for the consumer application. You should see these in the stack output as such:\nOutputs: AppRunnerServiceStack.AppURL = \u003capp URL\u003e AppRunnerServiceStack.ConsumerAppDockerImage = \u003cecr docker image\u003e .... Now, you can verify if the application is functioning properly. Get the publicly accessible URL for the App Runner application and invoke it using curl. This will create the MSK topic and send data specified in the HTTP POST body.\nexport APP_RUNNER_URL=\u003center app runner URL\u003e curl -i -X POST -d '{\"email\":\"user1@foo.com\",\"name\":\"user1\"}' $APP_RUNNER_URL Now you can deploy the consumer application to the EKS cluster. Before that, execute the steps to configure appropriate permissions for the application to interact with MSK and DynamoDB.","prerequisites#Prerequisites":"You will need the following:\nAn AWS account Install AWS CDK, AWS CLI, Docker, eksctl and curl. ","use-cdk-to-provision-msk-eks-and-dynamodb#Use CDK to provision MSK, EKS and DynamoDB":"All the code and config is present in this GitHub repo. Clone the GitHub repo and change to the right directory:\ngit clone https://github.com/abhirockzz/msk-cdk-apprunner-eks-dynamodb cd msk-cdk-apprunner-eks-dynamodb/cdk Deploy the first CDK stack:\ncdk deploy MSKDynamoDBEKSInfraStack Wait for the all the components to get provisioned, including MSK Serverless cluster, EKS cluster and DynamoDB. You can check its progress in the AWS CloudFormation console.\nYou can take a look at the CDK stack code here.","verify-end-to-end-solution#Verify end to end solution":"Continue to send records using App Runner producer application:\nexport APP_RUNNER_URL=\u003center app runner URL\u003e curl -i -X POST -d '{\"email\":\"user2@foo.com\",\"name\":\"user2\"}' $APP_RUNNER_URL curl -i -X POST -d '{\"email\":\"user3@foo.com\",\"name\":\"user3\"}' $APP_RUNNER_URL curl -i -X POST -d '{\"email\":\"user4@foo.com\",\"name\":\"user4\"}' $APP_RUNNER_URL Check consumer app logs on EKS to verify:\nkubectl logs -f $(kubectl get pods -l=app=msk-iam-consumer-app -o jsonpath='{.items[0].metadata.name}') Scale out consumer app The MSK topic created by the producer application has three topic partitions, so we can have maximum of three consumer instances. Scale out to three replicas:\nkubectl scale deployment/msk-iam-consumer-app --replicas=3 Verify the number of Pods and check logs for each of them. Notice how the data consumption is being balanced across the three instances.\nkubectl get pods -l=app=msk-iam-consumer-app "},"title":"Use CDK to deploy a complete solution with Kafka, App Runner, EKS and DynamoDB"},"/blog/cdk8s-k8s-go-1/":{"data":{"":"","#":"","alright-lets-get-back-on-track#Alright, lets get back on track\u0026hellip;":"","alright-lets-get-started#Alright, lets get started!":"","applying-infra-is-code-mantra-to-kubernetes#Applying \u0026ldquo;Infra-Is-Code\u0026rdquo; mantra to Kubernetes":"","before-wrapping-up#Before wrapping up\u0026hellip;":"Infrastructure as Code (IaC) is a well established paradigm and refers to the standard practice of treating infrastructure (network, disk, storage, databases, message queues etc.) in the same way as application code and applying general software engineering practices including source control versioning, testing and more. For exampple, Terraform and AWS CloudFormation are widely-adopted technologies that use configuration files/templates to represent the infrastructure components.\nInfrastructure-IS-Code - A different way of thinking about this Imagine you have an application that comprises of a Serverless function fronted by an API Gateway along with a NoSQL database as the backend. Instead of defining it in a static way (using JSON, YAML etc.), one can represent these components using standard programming language constructs such as classes, methods, etc. Here is pseudo-code example:\nDBTable table = new DBTable(\"demo-table\"); table.addPrimaryKey(\"email\", Type.String); Function function = new Function(\"demo-func\"); function.addEnvVars(\"TABLE_NAME\", table.Name()); APIGateway apigw = new APIGateway(); apigw.addFunctionIntegration(function); Notice the (hypothetical) classes DBTable, Function and APIGateway and the way they are used. For e.g. a function can reference the table object and get it’s name - all this comes to life during the program runtime and taken care of by the implementation details of the underlying framework/platform.\nBut, you don’t have to write pseudo-code for your production infrastructure\n… thanks to existing solutions such as cdk8s, AWS CDK, Pulumi, CDK for Terraform (cdktf) etc. Almost all these solutions follow a similar approach - write code to define infrastructure, then convert that into configuration, for e.g. Kubernetes manifest (YAML), AWS CloudFormation template, HCL config etc., which can then be applied using standard tooling.\nWhile we are on this topic, its hard not to mention the Go programming language and its ubiquitous presence in the cloud services and infrastructure domain. It combines the safety of a compiled language with the speed a interpreted language (like Python), has a robust standard library and compiles to a single binary. These and many more qualities have led to lots of cloud-native software (IaC, monitoring, observability etc.) written in Go, such as Prometheus, Terraform, Grafana, Jaeger etc.\n“In fact, over 75 percent of projects in the Cloud Native Computing Foundation are written in Go.”\nApplying “Infra-Is-Code” mantra to Kubernetes Over the course of multiple blog posts, I will cover how Go developers can use the cdk8s (Cloud Development Kit for Kubernetes) project for defining Kubernetes resources. It’s an open-source framework (also part of CNCF) that provides high-level abstractions which can be composed into larger Kubernetes applications. Instead of adopting YAML or other configuration/template driven approach, cdk8s supports multiple programming languages, which means you can work with Kubernetes resources using familiar concepts such as classes, methods, etc. Ultimately, cdk8s generates Kubernetes manifests which you can apply using kubectl - business as usual!\nAt the time of writing, cdk8s supports Go, Typescript, Python and Java\nThis blog post will start things off and provide a gentle yet hands-on intro to cdk8s. By the end of it, you will be familiar with the key concepts and understand how to use cdk8s Go APIs to define a Kubernetes application, deploy (using kubectl) and test it out.\nBefore you begin… Make sure you have Go (v1.16 or above) and cdk8s CLI installed. Also, you need to have access to a Kubernetes cluster. For learning and experimentation I would recommend using a single-node cluster running locally - such as minikube, kind, etc.\nI generally use minikube, so setting up a cluster is as simple as minikube start\nTo install cdk8s CLI\nYou can choose from the below options:\n#homebrew brew install cdk8s #npm npm install -g cdk8s-cli #yarn yarn global add cdk8s-cli Alright, lets get started! Although this blog post will provide step-by-step instructions, you can always refer to the complete code on Github\ncdk8s makes it really easy for you get started and bootstrap your application. You don’t need to guess and figure out how to structure your project, setup dependencies etc. since the cdk8s init command does it for you!\ncdk8s init go-app #output .... Your cdk8s Go project is ready! cat help Prints this message cdk8s synth Synthesize k8s manifests to dist/ cdk8s import Imports k8s API objects to \"imports/k8s\" Deploy: kubectl apply -f dist/ Once completed, you will get a directory structure as such:\n. ├── cdk8s.yaml ├── dist │ └── test.k8s.yaml ├── go.mod ├── go.sum ├── help ├── imports │ └── k8s │ ├── internal │ │ └── types.go │ ├── jsii │ │ ├── jsii.go │ │ └── k8s-0.0.0.tgz │ ├── k8s.go │ ├── k8s.init.go │ └── version └── main.go Update the generate go.mod file, and replace it with the following - this is to make things simpler for you.\nFeel free to use the latest version of the modules if needed.\nmodule getting-started-with-cdk8s-go go 1.16 require ( github.com/aws/constructs-go/constructs/v10 v10.1.42 github.com/aws/jsii-runtime-go v1.60.1 github.com/cdk8s-team/cdk8s-core-go/cdk8s/v2 v2.3.29 ) You’re all set to write some write some Go code! The canonical Kubernetes “hello world” is to get a nginx server up and running. The easiest option is to use simply use kubectl run e.g. kubectl run nginx --image=nginx. But, since this is imperative, we switch to a declarative way where we define our desired state (in a yaml file) and ask Kubernetes to figure things out.\nFor e.g. we can write a Deployment manifest and submit it to Kubernetes using kubectl apply -f \u003cname of the yaml file\u003e.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: hello-nginx template: metadata: labels: app: hello-nginx spec: containers: - image: nginx name: nginx-container ports: - containerPort: 8080 But we are here to minimise yaml… So, open the main.go file and copy the below Go code. Don’t worry, I will walk you through it!\npackage main import ( \"getting-started-with-cdk8s-go/imports/k8s\" \"github.com/aws/constructs-go/constructs/v10\" \"github.com/aws/jsii-runtime-go\" \"github.com/cdk8s-team/cdk8s-core-go/cdk8s/v2\" ) type NginxChartProps struct { cdk8s.ChartProps } func NewNginxChart(scope constructs.Construct, id string, props *NginxChartProps) cdk8s.Chart { var cprops cdk8s.ChartProps if props != nil { cprops = props.ChartProps } chart := cdk8s.NewChart(scope, jsii.String(id), \u0026cprops) selector := \u0026k8s.LabelSelector{MatchLabels: \u0026map[string]*string{\"app\": jsii.String(\"hello-nginx\")}} labels := \u0026k8s.ObjectMeta{Labels: \u0026map[string]*string{\"app\": jsii.String(\"hello-nginx\")}} nginxContainer := \u0026k8s.Container{Name: jsii.String(\"nginx-container\"), Image: jsii.String(\"nginx\"), Ports: \u0026[]*k8s.ContainerPort{{ContainerPort: jsii.Number(80)}}} k8s.NewKubeDeployment(chart, jsii.String(\"deployment\"), \u0026k8s.KubeDeploymentProps{ Spec: \u0026k8s.DeploymentSpec{ Replicas: jsii.Number(1), Selector: selector, Template: \u0026k8s.PodTemplateSpec{ Metadata: labels, Spec: \u0026k8s.PodSpec{ Containers: \u0026[]*k8s.Container{nginxContainer}}}}}) return chart } func main() { app := cdk8s.NewApp(nil) NewNginxChart(app, \"nginx\", nil) app.Synth() } When writing cdk8s based code in any language, you will come across a set of common concepts/terminologies - these include Construct, App and Chart. I will explain these as we walk through the code.\nSlight detour (code walk-through and concepts) Start with the main function first - we use cdk8s.NewApp to create an App.\nWell, what exactly in an App? It’s is a construct, and you can think of constructs as higher-level building blocks to represent state. The key thing to note is that these constructs are composable. What that means is that you can define levels of these constructs (each level provides/exposes a different abstraction layer) and combine them to create your desired end state - in this case it happens to be a Kubernetes manifest with objects such as Deployment, but it could be something else.\nFor e.g. an AWS CloudFormation template (if you were to use AWS CDK, not be confused with cdk8s)\nBack to the App - so, an App is also a construct. In fact you can think of it as the root in a tree (hierarchy) of constructs. So what else is there in that tree? Look the second line in the main function - NewNginxChart(app, \"getting-started\", nil) - this invokes a function NewNginxChart that returns a cdk8s.Chart which is the next component in the hierarchy. AA cdk8s App can contain multiple charts and each chart can be converted (or in precise cdk8s terminology - synthesized) into a separate Kubernetes manifest file (you will see this action very soon).\nFinally, draw your attention to the NewNginxChart function. It has a bunch of things, but notice the call to k8s.NewKubeDeployment function. This is where we actually define a Kubernetes Deployment in code (in the next section, we will also add a Service to the chart.)\nYou can define multiple Kubernetes components in a chart, such a Pod, Service, Ingress, Job etc. - what ever you need for your application to work on Kubernetes.\nTo summarise, here is a visual representation of what I just explained - remember everything is a Construct (App, Chart etc.)\nWait, what about the Kubernetes API dependencies??\nIf you’ve spent time working on accessing Kubernetes programmatically, this is an obvious (and great!) question. if you were to deal with k8s object using go, at the minimum you will need Kubernetes client-go, API machinery etc. Guess what, cdk8s has got you covered there too!\nYou actually don’t need to pull in these dependencies because cdk8s allows you to treat these Kubernetes API Objects as constructs - remember, everything is s construct! They are automatically imported to your project when you run the cdk8s init command, but you can do it explicitly using cdk8s import as well. The resulting API is available as part of the imports folder (yes, go ahead and check that again!). On the top of main.go, check the package that is imported - its just refers to the imports folder.\nThere is more to cdk8s import though. But you will have to wait for other blog posts to see that in action - we are just getting started!\nAlright, lets get back on track… .. and continue with the practical bits. It’s time to generate some yaml - you can’t eliminate it, but at least you don’t have to write it by hand! To do so, simply run:\ncdk8s synth Once that completes (should be quick!), check the dist directory to check what cdk8s has generated. To make it easier to understand, here is a diagram which has a one-to-one mapping (notice the labels 1, 2,3, etc.?) between the the cdk8s code objects/properties to their respective counterparts in yaml e.g. spec.replicas, spec.selector, template.spec etc.\nYou can now use good old kubectl to deploy this to Kubernetes since cdk8s is not going to do that for you, at least not yet ;)\nkubectl apply -f dist/ kubectl get pods -w Once the Deployment is ready, the Pod should be in Running state. Simply use port-forward to access the nginx container port locally:\nkubectl port-forward \u003center nginx pod name\u003e 8080:80 To access nginx home page, navigate to http://localhost:8080 using your browser\nYou also use a CLI tool e.g. curl localhost:8080.\nThat’s not all! Instead of port forwarding, let’s use the standard Kubernetes way of accessing applications by defining a Service resource, which is typically defined like this:\napiVersion: v1 kind: Service metadata: name: nginx-service spec: ports: - port: 9090 targetPort: 8080 selector: app: hello-nginx type: LoadBalancer But you know the rule - no yaml writing by hand! So, in the NewNginxChart function in the main.go file, add this piece of code after the part you defined the Deployment:\nk8s.NewKubeService(chart, jsii.String(\"service\"), \u0026k8s.KubeServiceProps{ Spec: \u0026k8s.ServiceSpec{ Type: jsii.String(\"LoadBalancer\"), Ports: \u0026[]*k8s.ServicePort{{Port: jsii.Number(9090), TargetPort: k8s.IntOrString_FromNumber(jsii.Number(80))}}, Selector: \u0026map[string]*string{\"app\": jsii.String(\"hello-nginx\")}}}) First, delete the existing Deployment - kubectl delete -f dist/. Then, run cdk8s synth again to create the new manifest in the dist folder.\nBoth the Service and Deployment are in the same file - this is because they are part of the same Chart.\nHow you access the service will depend on the Kubernetes cluster. If you are using a cloud provider, it will likely provision a Load Balancer service native to that cloud e.g. Application Load Balancer in AWS. Please adjust this as per your setup.\nFor minikube, you can simply follow these instructions https://minikube.sigs.k8s.io/docs/handbook/accessing/#loadbalancer-access - “Services of type LoadBalancer can be exposed via the minikube tunnel command.”\nIn a terminal, run this command (it runs as a separate process):\nminikube tunnel In another terminal, delete the existing Deployment and then apply the new manifest:\nkubectl apply -f dist/ kubectl get pods -w Check the Service:\nkubectl get svc To access the nginx server, navigate to the external IP (as per the Service). In the case of minikube, you can simply use localhost:9090 or 127.0.0.0:9090\nRemember to use port 9090 since that’s the external port we specified in the Service configuration in our code\nBefore wrapping up… .. I want to call out a couple of other useful things in cdk8s.\nReference and reuse existing manifests and Helm charts\nSay you have a Service already defined in a service.yaml file. You can include it in your cdk8s as part of a larger application/chart that you may have. Here is an example:\ncdk8s.NewInclude(chart, jsii.String(\"existing service\"), \u0026cdk8s.IncludeProps{Url: jsii.String(\"service.yaml\")}) Similarly, you can also include Helm charts. Say you wanted to add bitnami/nginx:\ncdk8s.NewHelm(chart, jsii.String(\"bitnami nginx helm chart\"), \u0026cdk8s.HelmProps{ Chart: jsii.String(\"bitnami/nginx\"), Values: \u0026map[string]interface{}{\"service.type\": \"ClusterIP\"}}) Well, you do need to have helm installed locally and also add the repo first helm repo add bitnami https://charts.bitnami.com/bitnami\nAnother handy feature is…\n… the ability to declare dependencies between any two cdk8s constructs. For instance, in the previous example, we had a Deployment and a Service. You could create a dependency as such:\ndeployment := k8s.NewKubeDeployment(...) service := k8s.NewKubeService(...) deployment.AddDependency(service) Thanks to AddDependency, the resulting manifest will be such that the Service is placed before the Deployment object.\nDependency is not limited to individual constructs in a chart. If you have multiple charts as part of your cdk8s app, you can establish dependencies across charts as well.","conclusion#Conclusion":"Awesome. So you were able to “code” your way through trouble and ignore YAML. Hope you enjoyed it! To keep things simple, I demonstrated a Deployment and Service, but you can choose from other Kubernetes components such as Ingress, Job etc. They are all exposed using a similar pattern i.e. NewKube for e.g. NewKubeJob, NewKubeIngress etc.\nBut there is still a lot of boilerplate code involved in defining Kubernetes components. Writing Go code sounds way better than YAML engineering (at least to me), it seems as if we are translating existing YAML into Go structs (and fields). In a subsequent blog post, we will explore how to improve this further.\nHappy coding!","slight-detour-code-walk-through-and-concepts#Slight detour (code walk-through and concepts)":"","thats-not-all#That\u0026rsquo;s not all!":""},"title":"Write your Kubernetes Infrastructure as Go code - Getting started with cdk8s"},"/blog/cdk8s-k8s-go-2/":{"data":{"":"","conclusion#Conclusion":"Awesome! In this blog you saw the expressiveness of cdk8s-plus. We started off with a compact and less verbose version of the Nginx deployment and ended up with a full-fledged Wordpress instance - all using Go.\nHappy coding!","how-about-a-wordpress-installation-on-kubernetes#How about a Wordpress installation on Kubernetes?":"I like this example - it’s not overly complex but realistic enough because it has multiple moving parts that includes a combination of stateless, stateful components, different kinds of services etc.\nThis post is not a deep dive into Wordpress and loosely inspired by this article in the Kubernetes documentation, which I assume folks might be familiar with.\nThe main function will give you a sense of what lies ahead:\nfunc main() { app := cdk8s.NewApp(nil) mySQLChart := NewMySQLChart(app, \"mysql\", nil) wordpressChart := NewWordpressChart(app, \"wordpress\", nil) wordpressChart.AddDependency(mySQLChart) app.Synth() } So far, we have dealt with a single chart. Our Wordpress cdk8s application has two separate charts - one for MySQL database and the other one for Wordpress. This will result in two different manifests being created as a result of cdk8s synth process.\nLet’s look the MySQL chart first\nsome code has been omitted for brevity\nWe start by defining a Kubernetes Secret to store MySQL password (with NewSecret):\nfunc NewMySQLChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //.... secretName := \"mysql-pass\" password := \"Password123\" mysqlSecret := cdk8splus22.NewSecret(chart, jsii.String(\"mysql-secret\"), \u0026cdk8splus22.SecretProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(secretName)}}) secretKey := \"password\" mysqlSecret.AddStringData(jsii.String(secretKey), jsii.String(password)) MySQL password has been declared in the code - not a best practice by any means, just for demo. Do not do this in production!\nThen we create the Deployment and provide container details. Notice how the Secret has been added as an environment variable to the container:\nFirst we got an EnvValue using EnvValue_FromSecretValue That was added to the container using Env#AddVariable dep := cdk8splus22.NewDeployment(chart, jsii.String(\"mysql-deployment-cdk8splus\"), \u0026cdk8splus22.DeploymentProps{}) containerImage := \"mysql\" mysqlContainer := dep.AddContainer(\u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"mysql-container\"), Image: jsii.String(containerImage), Port: jsii.Number(3306), }) envValFromSecret := cdk8splus22.EnvValue_FromSecretValue(\u0026cdk8splus22.SecretValue{Key: jsii.String(secretKey), Secret: mysqlSecret}, \u0026cdk8splus22.EnvValueFromSecretOptions{Optional: jsii.Bool(false)}) mySQLPasswordEnvName := \"MYSQL_ROOT_PASSWORD\" mysqlContainer.Env().AddVariable(jsii.String(mySQLPasswordEnvName), envValFromSecret) For durable storage, we create a PersistentVolumeClaim, use that to define a Volume and mount in onto the container at the path /var/lib/mysql.\nmysqlPVC := cdk8splus22.NewPersistentVolumeClaim(chart, jsii.String(\"mysql-pvc\"), \u0026cdk8splus22.PersistentVolumeClaimProps{ AccessModes: \u0026[]cdk8splus22.PersistentVolumeAccessMode{cdk8splus22.PersistentVolumeAccessMode_READ_WRITE_ONCE}, Storage: cdk8s.Size_Gibibytes(jsii.Number(2))}) mysqlVolumeName := \"mysql-persistent-storage\" mysqlVolume := cdk8splus22.Volume_FromPersistentVolumeClaim(chart, jsii.String(\"mysql-vol-pvc\"), mysqlPVC, \u0026cdk8splus22.PersistentVolumeClaimVolumeOptions{Name: jsii.String(mysqlVolumeName)}) mySQLVolumeMountPath := \"/var/lib/mysql\" mysqlContainer.Mount(jsii.String(mySQLVolumeMountPath), mysqlVolume, \u0026cdk8splus22.MountOptions{}) Finally, we create a Service:\nmySQLServiceName := \"mysql-service\" clusterIPNone := \"None\" cdk8splus22.NewService(chart, jsii.String(\"mysql-service\"), \u0026cdk8splus22.ServiceProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(mySQLServiceName)}, Selector: dep, ClusterIP: jsii.String(clusterIPNone), Ports: \u0026[]*cdk8splus22.ServicePort{{Port: jsii.Number(3306)}}, }) Unlike previous example, we create a Service explicitly and then refer to Deployment object in the service selector.\nWordpress Chart - Except for minor differences, it’s the same as the MySQL chart with Wordpress specific configuration obviously. So I won’t repeat it here - feel free to explore the code.\nThe moment of truth is here! Rinse and repeat - cdk8s synth to create the manifest and apply it with kubectl:\ncd part2-cdk8s-plus-in-action/wordpress #create manifests cdk8s synth #apply them kubectl apply -f dist/ #output - you will see something similar to: secret/mysql-pass created deployment.apps/mysql-mysql-deployment-cdk8splus-c83762d9 created persistentvolumeclaim/mysql-mysql-pvc-c8799bba created service/mysql-service created deployment.apps/wordpress-wordpress-deployment-cdk8splus-c8252da7 created service/wordpress-service created persistentvolumeclaim/wordpress-wordpress-pvc-c8334a29 created In a different terminal run (if not already running):\nminikube tunnel Use your browser to navigate to http://localhost:80. You should see the familiar Wordpress installation screen.\nGo ahead, finish the installation and log into your Wordpress instance. Feel free to experiment. Maybe try deleting the MySQL deployment and re-creating it. Thanks to the PersistentVolume, MySQL data should be recovered and wordpress will continue to work.","lets-start-by-revamping-the-nginx-deployment#Let\u0026rsquo;s start by revamping the Nginx deployment..":"The previous blog post covered how to get started with cdk8s (Cloud Development Kit for Kubernetes), that is an an open-source framework (part of CNCF) using which you can define your Kubernetes applications using regular programming languages (instead of yaml).\nYou were able to setup a simple nginx Deployment and accessed it via a Service - all this was done using Go, which was then converted to yaml (using cdk8s synth) and submitted to the cluster using kubectl. This was a good start. However, since the core cdk8s library is pretty low-level (for a good reason!) the code involved lot of boilerplate (you can refer to the code here).\ncdk8s-plus leverages building blocks from cdk8s core library, thereby helping reduce verbosity and complexity by providing higher level abstractions/APIs for all Kubernetes objects such as Deployments, Services, etc. In this blog, we will see cdk8s-plus in action and even deploy Wordpress on Kubernetes with it!\nLet’s start by revamping the Nginx deployment.. To witness how cdk8s-plus works, it’s best to look at the code.\nIt is available on Github.\nI will walk you through the code as we go along.\nfunc NewNginxChart(scope constructs.Construct, id string, props *NginxChartProps) cdk8s.Chart { var cprops cdk8s.ChartProps if props != nil { cprops = props.ChartProps } chart := cdk8s.NewChart(scope, jsii.String(id), \u0026cprops) dep := cdk8splus22.NewDeployment(chart, jsii.String(\"deployment\"), \u0026cdk8splus22.DeploymentProps{Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(\"nginx-deployment-cdk8s-plus\")}}) dep.AddContainer(\u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"nginx-container\"), Image: jsii.String(\"nginx\"), Port: jsii.Number(80)}) dep.ExposeViaService(\u0026cdk8splus22.DeploymentExposeViaServiceOptions{ Name: jsii.String(\"nginx-container-service\"), ServiceType: cdk8splus22.ServiceType_LOAD_BALANCER, Ports: \u0026[]*cdk8splus22.ServicePort{{Port: jsii.Number(9090), TargetPort: jsii.Number(80)}}}) return chart } We start by creating a Deployment, then add a container and finally expose it using a Service. This is quite intuitive and user-friendly.\nThe container details could have been provided via DeploymentProps but using AddContainer seemed more natural (at least to me).\nTo generate Kubernetes manifest, simply run cdk8s synth. This will generate a yaml in the dist folder. Here is an example (some of the names, labels etc. will be different in your case):\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment-cdk8s-plus spec: minReadySeconds: 0 progressDeadlineSeconds: 600 replicas: 1 selector: matchLabels: cdk8s.io/metadata.addr: nginx-cdk8s-plus-deployment-c84b388e strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: cdk8s.io/metadata.addr: nginx-cdk8s-plus-deployment-c84b388e spec: automountServiceAccountToken: true containers: - image: nginx imagePullPolicy: Always name: nginx-container ports: - containerPort: 80 securityContext: privileged: false readOnlyRootFilesystem: false runAsNonRoot: false dnsPolicy: ClusterFirst securityContext: fsGroupChangePolicy: Always runAsNonRoot: false setHostnameAsFQDN: false --- apiVersion: v1 kind: Service metadata: name: nginx-container-service spec: externalIPs: [] ports: - port: 9090 targetPort: 80 selector: cdk8s.io/metadata.addr: nginx-cdk8s-plus-deployment-c84b388e type: LoadBalancer Both the Deployment and Service are present in the same manifest, since they were declared in the same Chart.\nIt’s worth noting that there was no need to specify any Pod label selectors, template labels (in Deployment code) or Service selector. cdk8s-plus took care of it by auto-generating cdk8s.io/metadata.addr: nginx-cdk8s-plus-deployment-c84b388e, which was used in spec.selector.matchLabels and spec.template.metadata.labels, along with the Service selector in nginx-container-service\nA note on dependencies\ngo.mod lists all the modules:\nrequire ( github.com/aws/constructs-go/constructs/v10 v10.1.42 github.com/aws/jsii-runtime-go v1.61.0 github.com/cdk8s-team/cdk8s-core-go/cdk8s/v2 v2.3.31 github.com/cdk8s-team/cdk8s-plus-go/cdk8splus22/v2 v2.0.0-rc.23 ) Note that we are using cdk8splus22. The reason for this naming convention is because each cdk8s-plus library is separately vended to target a specific Kubernetes version - the 22 at the end signifies that this dependency will work with Kubernetes 1.22\nI would recommend reading the FAQs to get further clarity\nTo test this locally…\n… you can use minikube, kind, etc.\ngit clone https://github.com/abhirockzz/cdk8s-for-go-developers cd part2-cdk8s-plus-in-action/nginx-example # make sure cluster is running minikube start # create the resources kubectl apply -f dist/ kubectl get pods -w Once Pod is running, check the Service:\nkubectl get svc In a terminal, run this command (it runs as a separate process):\nminikube tunnel To access the nginx server, navigate to the external IP (as per the Service). In the case of minikube, you can simply use localhost:9090 or 127.0.0.0:9090"},"title":"Write your Kubernetes Infrastructure as Go code - cdk8s-plus in action!"},"/blog/cdk8s-k8s-go-3/":{"data":{"":"","#":"","time-to-wrap-up#Time to wrap up\u0026hellip;":"cdk8s (Cloud Development Kit for Kubernetes) is an an open-source framework (part of CNCF) using which you can define your Kubernetes applications with regular programming languages (instead of yaml). Some of the previous blogs on this topic covered the getting started experience and using cdk8s-plus library to further improve upon the core cdk8s library features. We are going to continue and push cdk8s even further. This blog post will demonstrate how you can use Kubernetes Custom Resource Definitions with cdk8s. We will start off with a simple Nginx example and then you will use the combination of Strimzi project CRDs along with Go and cdk8s to define and deploy a Kafka cluster on Kubernetes!\nI am assuming that you’ve have some knowledge of Kubernetes Custom Resource Definitions and have probably even used a few in the form of Operators. If not, that’s ok! The Kubernetes documentation covers it quite well. You can always refer to it, come back here and follow along!\ncdk8s lets you use Kubernetes API objects directly in your code, without having to import individual Go client packages, all thanks to cdk8s import. (also mentioned in the “Wait, what about the Kubernetes API dependencies??” section of a previous blog post). But you can also use it for Custom Resource Definitions! Let’s see this in action.\nBefore you begin… Make sure you have Go (v1.16 or above) and cdk8s CLI installed. Also, you need to have access to a Kubernetes cluster. For learning and experimentation I would recommend using a single-node cluster running locally - such as minikube, kind, etc.\nI generally use minikube, so setting up a cluster is as simple as minikube start\nTo install cdk8s CLI\nYou can choose from the below options:\n#homebrew brew install cdk8s #npm npm install -g cdk8s-cli #yarn yarn global add cdk8s-cli Alright, lets get started… Although this blog post will provide step-by-step instructions, you can always refer to the complete code on Github\ncdk8s makes it really easy for you get started and bootstrap your application. You don’t need to guess and figure out how to structure your project, setup dependencies etc. since the cdk8s init command does it for you!\ncdk8s init go-app #output .... Your cdk8s Go project is ready! cat help Prints this message cdk8s synth Synthesize k8s manifests to dist/ cdk8s import Imports k8s API objects to \"imports/k8s\" Deploy: kubectl apply -f dist/ Update the generate go.mod file, and replace it with the following - this is to make things simpler for you.\nFeel free to use the latest version of the modules if needed.\nmodule cdk8s-crd go 1.16 require ( github.com/aws/constructs-go/constructs/v10 v10.1.42 github.com/aws/jsii-runtime-go v1.61.0 github.com/cdk8s-team/cdk8s-core-go/cdk8s/v2 v2.3.34 ) To start with, let’s work with a really (really!) simple Custom Resource Definition\nI am going to use a sample CRD from the Kubernetes example. To be honest, it doesn’t really do anything. But, since we’re just getting started, this should suffice!\nFirst, install/register the CRD resource itself:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/sample-controller/master/artifacts/examples/crd.yaml Confirm whether the CRD was installed:\nkubectl get crd # output NAME CREATED AT foos.samplecontroller.k8s.io 2022-07-08T09:28:46Z kubectl get foos.samplecontroller.k8s.io #output (as expected) No resources found in default namespace. So, we just installed a CRD with the name foos.samplecontroller.k8s.io and type Foo. Its possible to create an instance of this using yaml… but…\nWe are here to write Go code!\nTo do that, first import the CRD as an API using cdk8s - this will automatically create the corresponding Go API representations (structs etc.):\ncdk8s import https://raw.githubusercontent.com/kubernetes/sample-controller/master/artifacts/examples/crd.yaml Check the imports directory, an additional folder should have been created.\nimports/ └── samplecontrollerk8sio ├── internal │ └── types.go ├── jsii │ ├── jsii.go │ └── samplecontrollerk8sio-0.0.0.tgz ├── samplecontrollerk8sio.go ├── samplecontrollerk8sio.init.go └── version We can now use the CRD just like any other Kubernetes resource/API (like Deployment) and import it in the cdk8s Go code. Create a new file called foo.go and copy the following code:\npackage main import ( \"cdk8s-crd/imports/samplecontrollerk8sio\" \"github.com/aws/constructs-go/constructs/v10\" \"github.com/aws/jsii-runtime-go\" \"github.com/cdk8s-team/cdk8s-core-go/cdk8s/v2\" ) type FooChartProps struct { cdk8s.ChartProps } func NewFooChart(scope constructs.Construct, id string, props *FooChartProps) cdk8s.Chart { var cprops cdk8s.ChartProps if props != nil { cprops = props.ChartProps } chart := cdk8s.NewChart(scope, jsii.String(id), \u0026cprops) samplecontrollerk8sio.NewFoo(chart, jsii.String(\"foo1\"), \u0026samplecontrollerk8sio.FooProps{Spec: \u0026samplecontrollerk8sio.FooSpec{DeploymentName: jsii.String(\"foo1-dep\"), Replicas: jsii.Number(2)}}) return chart } See how we created an instance of samplecontrollerk8sio.Foo:\nImported the autogenerated CRD API from the cdk8s-crd/imports/samplecontrollerk8sio package, Used the NewFoo function and provided the metadata via FooProps Replace the contents of main.go with the following:\npackage main import ( \"github.com/cdk8s-team/cdk8s-core-go/cdk8s/v2\" ) type MyChartProps struct { cdk8s.ChartProps } func main() { app := cdk8s.NewApp(nil) NewFooChart(app, \"FooApp\", nil) app.Synth() } All we is include the Chart that we defined just now (in foo.go) and include it in the cdk8s App.\nTo create the Foo resource…\nRun cdk8s synth - this will result in a manifest in the dist folder:\napiVersion: samplecontroller.k8s.io/v1alpha1 kind: Foo spec: deploymentName: foo1-dep replicas: 2 metadata: name: fooapp-foo1-c80094ac To create it in Kubernetes:\nkubectl apply -f dist You can confirm by running :\nkubectl get foo kubectl get foos.samplecontroller.k8s.io To introspect further, you can use the name of the created resource e.g. kubectl describe foo/fooapp-foo1-c80094ac\nAlright, now that you’ve seen a simple example, we can move on to something slightly more advanced.\nSetup Kafka on Kubernetes using Strimzi, cdk8s and Go Strimzi is an open-source CNCF project and one of my personal favourites! If you don’t know about Strimzi, that’s ok. It’s enough to understand that it provides a way to run an Apache Kafka on Kubernetes with the help of Custom Resource Definitions and corresponding Operators for components such as Kafka cluster, Kafka Connect topic, users, Kafka Mirror etc.\nHere is a high-level diagram of how the different Strimzi components interact. Since a Strimzi deep-dive is out of scope, I would recommend that you refer its (excellent!) documentation for details.\nAs before, we need to first install the CRD itself (you can also refer to the Strimzi Quickstart)\nkubectl create namespace kafka kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka # wait for the Operator Pod to start up (Running) kubectl get pod -n kafka --watch You can also check the Operator logs using kubectl logs deployment/strimzi-cluster-operator -n kafka -f\nEach supported Kafka component (cluster, connect, user etc.) has a corresponding Custom Resource Definition - for the purposes of this blog post, we will just use the Kafka cluster and topic CRDs. Let’s import them as an API:\ncdk8s import https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/main/install/cluster-operator/040-Crd-kafka.yaml cdk8s import kafkatopic:=https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/main/install/cluster-operator/043-Crd-kafkatopic.yaml Note that I’ve prepended kafkatopic to the module name for Kafka topic CRD\nCheck the imports folder - you should see two additional folders named kafkastrimziio and kafkatopic_kafkastrimziio.\nTime for some Go code, again\nCreate a kafka_strimzi.go file and copy the code from Github repo:\nOr you can also simply do this: curl -o kafka.go https://raw.githubusercontent.com/abhirockzz/cdk8s-for-go-developers/master/part3-crd/kafka_strimzi.go\nI will walk you through the important parts of the code here. Start with the NewKafkaChart function that creates a new Chart.\nfunc NewKafkaChart(scope constructs.Construct, id string, props *KafkaChartProps) cdk8s.Chart { //.... ommitted for brevity chart := cdk8s.NewChart(scope, jsii.String(id), \u0026cprops) See how the Kafka cluster is defined using kafkastrimziio.KafkaProps struct (for a deep-dive into each of these components you can refer to Strimzi documentation). We specify the Kafka version, number of nodes/replicas (we will stick to a single node replica) how to expose the cluster etc.\n//.... \u0026kafkastrimziio.KafkaProps{ Spec: \u0026kafkastrimziio.KafkaSpec{ Kafka: \u0026kafkastrimziio.KafkaSpecKafka{ Version: jsii.String(\"3.2.0\"), Replicas: jsii.Number(1), Listeners: \u0026[]*kafkastrimziio.KafkaSpecKafkaListeners{ { Name: jsii.String(\"plain\"), Port: jsii.Number(9092), Type: kafkastrimziio.KafkaSpecKafkaListenersType_INTERNAL, Tls: jsii.Bool(false), }, }, //.... Then we add required config for the Kafka cluster (in-line with the fact that we have a single node cluster only) as well as storage (ephemeral storage will work for this example).\n//... Config: map[string]interface{}{ \"offsets.topic.replication.factor\": 1, \"transaction.state.log.replication.factor\": 1, \"transaction.state.log.min.isr\": 1, \"default.replication.factor\": 1, \"min.insync.replicas\": 1, \"inter.broker.protocol.version\": \"3.2\", }, Storage: \u0026kafkastrimziio.KafkaSpecKafkaStorage{ Type: kafkastrimziio.KafkaSpecKafkaStorageType_EPHEMERAL, }, //... Finally, we configure Zookeeper as well as the Entity operator that handles Kafka topics (as well as users, although we don’t use it here)\n//... Zookeeper: \u0026kafkastrimziio.KafkaSpecZookeeper{ Replicas: jsii.Number(1), Storage: \u0026kafkastrimziio.KafkaSpecZookeeperStorage{ Type: kafkastrimziio.KafkaSpecZookeeperStorageType_EPHEMERAL, }, }, EntityOperator: \u0026kafkastrimziio.KafkaSpecEntityOperator{ TopicOperator: \u0026kafkastrimziio.KafkaSpecEntityOperatorTopicOperator{}, }}}) //... To wire it up, update the main.go file:\nfunc main() { app := cdk8s.NewApp(nil) //NewFooChart(app, \"FooApp\", nil) NewKafkaChart(app, \"KafkaApp\", nil) app.Synth() } To create a Kafka cluster using the CRD…\nFollow the the usual workflow:\n# generate manifest (check it in dist folder) cdk8s synth # apply it (note the kafka namespace) kubectl apply -f dist/ -n kafka Wait for the resources to be created:\nKAFKA_CRD_INSTANCE_NAME=$(kubectl get kafka -n kafka -o=jsonpath='{.items[0].metadata.name}') kubectl wait kafka/$KAFKA_CRD_INSTANCE_NAME --for=condition=Ready --timeout=300s -n kafka Once all the Kafka cluster resources are created, you should see the following message - kafka.kafka.strimzi.io/\u003cname of your Kafka CRD instance\u003e condition met. The Kafka cluster is now ready and we can test it using the good old Kafka CLI based producer and consumer (instructions in Strimzi quickstart).\nBOOSTRAP_SERVER=$(kubectl get kafka -n kafka -o=jsonpath='{.items[0].metadata.name}')-kafka-bootstrap kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.29.0-kafka-3.2.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --bootstrap-server $BOOSTRAP_SERVER:9092 --topic test-topic kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.29.0-kafka-3.2.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server $BOOSTRAP_SERVER:9092 --topic test-topic --from-beginning That’s all for now!\nTime to wrap up… You learnt how to combine Kubernetes Custom Resource definition with cdk8s. This is really powerful and means that you can continue to use code (in this case, written in Go) to define built-in Kubernetes resources (like Deployments etc.) as well as Custom resources!\nDid you like what you tried?\nWell, you can continue learning! Couple of suggestions:\nYou can try updating the existing code to add a Deployment resource that refers to a Kafka client app (you have to write it and package it as a Docker container first) and can access the Kafka cluster you created. Explore how you can get the connectivity parameters.. The Kafka cluster we created was configured to have Internal access only. Explore options to expose it externally (refer to Strimzi documentation) and update the code to implement that (should be a small change). Which Kubernetes objects will be affected by it? Happy coding!"},"title":"Write your Kubernetes Infrastructure as Go code - Using Custom Resource Definitions with cdk8s"},"/blog/cdk8s-k8s-go-4/":{"data":{"":"","#":"Build a Wordpress deployment as a cdk8s construct\nConstructs are the fundamental building block of cdk8s (Cloud Development Kit for Kubernetes) - an open-source framework (part of CNCF) with which you can define your Kubernetes applications using regular programming languages (instead of yaml). In Getting started with cdk8s, you saw how to use the core cdk8s library.\nYou can also use the cdk8s-plus library (also covered this a previous blog) to reduce the amount of boilerplate code you need to write. With cdk8s-plus, creating a Kubernetes Deployment, specifying it’s container (and other properties) and exposing it via a Service is three function calls away.\nFor example, to setup and access Nginx, you simply need this:\n//... deployment := cdk8splus22.NewDeployment(chart, jsii.String(\"deployment\"), \u0026cdk8splus22.DeploymentProps{Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(\"nginx-deployment-cdk8s-plus\")}}) deployment.AddContainer(\u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"nginx-container\"), Image: jsii.String(\"nginx\"), Port: jsii.Number(80)}) deployment.ExposeViaService(\u0026cdk8splus22.DeploymentExposeViaServiceOptions{ Name: jsii.String(\"nginx-container-service\"), ServiceType: cdk8splus22.ServiceType_LOAD_BALANCER, Ports: \u0026[]*cdk8splus22.ServicePort{{Port: jsii.Number(9090), TargetPort: jsii.Number(80)}}}) //... But things can get even better!\nInstead of writing the same logic over and over, you can package it in the form of a reusable component that can be invoked just like other built-in cdk8s functions (e.g. NewDeployment, NewService etc.). Although it might not sound as useful for the simple application(s), this approach is invaluable for a large project, team or organisation who want to scale their engineering efforts. In fact, there is already a pool of ready-to-use components available at constructs.dev. These include constructs contributed by the community, AWS and others as well, across multiple programming languages.\nTo better understand how this might look in practice … let’s look at the code. I will continue to use Wordpress as an example, like I did in the previous blog post. Here is a code snippet that shows how everything is wired together (with implementation walk-through in the next section):\nYou can refer to the complete code on Github\n//... func NewMyChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //.... NewWordpressStack(chart, jsii.String(\"wordpress-stack\"), \u0026WordpressProps{//....) return chart } func main() { app := cdk8s.NewApp(nil) NewMyChart(app, \"wordpress-custom-stack\", nil) app.Synth() } NewWordpressStack gives us a construct that represents an entire Wordpress installation (single line of code!) We simply configure it as per our requirements (with WordpressProps) Include this as part of a cdk8s.Chart which is then included in the cdk8s.App (as with any other cdk8s application) There is lot of flexibility in terms of how you want to build a custom construct, depending on your requirements. But, at its very core, the basic concept is to define a way to create a new construct.Construct. You would want to provide a way to add metadata to further configure/refine your Construct - typically, thats done through properties (cdk8s.ChartProps).\nFirst we define WordpressProps - this encapsulates/externalises the attributes of the Wordpress installation. Since this is just an example, I have provided limited attributes such as MySQL/Wordpress Docker images, MySQL password, and required storage.\ntype WordpressProps struct { MySQLImage *string MySQLPassword *string MySQLStorage *float64 WordpressImage *string WordpressStorage *float64 } Then we have a function that will allow other charts/constructs to instantiate Wordpress. This is where the entire implementation resides.\nfunc NewWordpressStack(scope constructs.Construct, id *string, props *WordpressProps) constructs.Construct { ... } The props *WordpressProps parameter allows other constructs to influence the Wordpress stack creation e.g. you can define how much storage you need, maybe use a different Docker image for Wordpress/MySQL. The actual code for this function is similar to the one you saw here (with required adjustments), so there is no point repeating it here. I will simply highlight the important bits - specifically the ones that use the props to configure the required components.\nThis sample construct used cdk8splus22 library. The reason for this naming convention is because each cdk8s-plus library is separately vended to target a specific Kubernetes version - the 22 at the end signifies that this dependency will work with Kubernetes 1.22. You can use the library corresponding to your Kubernetes version and refer to the FAQs for more info.\nWe use the MySQL password from props and use that to create the Secret.\n//... password := props.MySQLPassword mysqlSecret := cdk8splus22.NewSecret(wordpressConstruct, jsii.String(\"mysql-secret\"), \u0026cdk8splus22.SecretProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(secretName)}}) secretKey := \"password\" mysqlSecret.AddStringData(jsii.String(secretKey), password) //... The container images for MySQL and Wordpress are referenced via their respective Deployments:\n//... containerImage := props.MySQLImage mysqlContainer := dep.AddContainer(\u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"mysql-container\"), Image: containerImage, Port: jsii.Number(3306), }) //... wordpressContainer := wordPressDeployment.AddContainer(\u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"wordpress-container\"), Image: props.WordpressImage, Port: jsii.Number(80), }) We also use the passed in storage as well - this is used to configure the PersistentVolumeClaim request.\n... mysqlPVC := cdk8splus22.NewPersistentVolumeClaim(wordpressConstruct, jsii.String(\"mysql-pvc\"), \u0026cdk8splus22.PersistentVolumeClaimProps{ AccessModes: \u0026[]cdk8splus22.PersistentVolumeAccessMode{cdk8splus22.PersistentVolumeAccessMode_READ_WRITE_ONCE}, Storage: cdk8s.Size_Gibibytes(props.MySQLStorage)}) ... wordpressPVC := cdk8splus22.NewPersistentVolumeClaim(wordpressConstruct, jsii.String(\"wordpress-pvc\"), \u0026cdk8splus22.PersistentVolumeClaimProps{ AccessModes: \u0026[]cdk8splus22.PersistentVolumeAccessMode{cdk8splus22.PersistentVolumeAccessMode_READ_WRITE_ONCE}, Storage: cdk8s.Size_Gibibytes(props.WordpressStorage)}) Finally, we call NewWordpressStack from another cdk8s.Chart and pass in the attributes we want to configure.\nfunc NewMyChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { var cprops cdk8s.ChartProps if props != nil { cprops = props.ChartProps } chart := cdk8s.NewChart(scope, jsii.String(id), \u0026cprops) NewWordpressStack(chart, jsii.String(\"wordpress-stack\"), \u0026WordpressProps{ MySQLImage: jsii.String(\"mysql\"), MySQLPassword: jsii.String(\"Password123\"), MySQLStorage: jsii.Number(3), WordpressImage: jsii.String(\"wordpress:4.8-apache\"), WordpressStorage: jsii.Number(2)}) return chart } Use this to install Wordpress To test it locally…\n… you can use minikube, kind, etc.\n# make sure cluster is running minikube start git clone https://github.com/abhirockzz/cdk8s-for-go-developers cd part4-custom-construct Create manifest and inspect all the resources (see dist directory):\ncdk8s synth To deploy them:\nkubectl apply -f dist/ # output (might differ in your case) secret/mysql-pass created deployment.apps/mysql-mysql-deployment-cdk8splus-c83762d9 created persistentvolumeclaim/mysql-mysql-pvc-c8799bba created service/mysql-service created deployment.apps/wordpress-wordpress-deployment-cdk8splus-c8252da7 created service/wordpress-service created persistentvolumeclaim/wordpress-wordpress-pvc-c8334a29 created Check the Kubernetes Service (called wordpress-service) which exposes the wordpress Deployment.\nkubectl get svc wordpress-service If you’re using minikube, in a different terminal run (if not already running):\nminikube tunnel Use your browser to navigate to http://localhost:80. You should see the familiar Wordpress installation screen.\nGo ahead, finish the installation and log into your Wordpress instance. Feel free to experiment with it.","conclusion#Conclusion":"cdk8s is a powerful tool itself but it also provides you the ability to extend and build other abstraction on top of it. You saw how to write a custom construct in Go and used it deploy Wordpress on Kubernetes. This can be further used as a foundation for other re-usable components.\nHappy coding!"},"title":"Write your Kubernetes Infrastructure as Go code - Extend cdk8s with custom Constructs"},"/blog/cdk8s-k8s-go-5/":{"data":{"":"","dont-forget-to-delete-resources#Don\u0026rsquo;t forget to delete resources..":"Deploy DynamoDB and a client app using cdk8s along with AWS Controller for Kubernetes\nAWS Controllers for Kubernetes (also known as ACK) are built around the Kubernetes extension concepts of Custom Resource and Custom Resource Definitions. You can use ACK to define and use AWS services directly from Kubernetes. This helps you take advantage of managed AWS services for your Kubernetes applications without needing to define resources outside of the cluster.\nSay you need to use a AWS S3 Bucket in your application that’s deployed to Kubernetes. Instead of using AWS console, AWS CLI, AWS CloudFormation etc., you can define the AWS S3 Bucket in a YAML manifest file and deploy it using familiar tools such as kubectl. The end goal is to allow users (Software Engineers, DevOps engineers, operators etc.) to use the same interface (Kubernetes API in this case) to describe and manage AWS services along with native Kubernetes resources such as Deployment, Service etc.\nHere is a diagram from the ACK documentation, that provides a high level overview:\nWhat’s covered in this blog post? ACK supports many AWS services, including Amazon DynamoDB. One of the topics that this blog post covers is how to use ACK on Amazon EKS for managing DynamoDB. But, just creating a DynamoDB table isn’t going to be all that interesting!\nIn addition to it, you will also work with and deploy a client application - this is a trimmed down version of the URL shortener app covered in a previous blog post. While the first half of the blog will involve manual steps to help you understand the mechanics and get started, in the second half, we will switch to cdk8s and achieve the same goals using nothing but Go code.\ncdk8s? what, why? Because, Infrastructure IS code cdk8s (Cloud Development Kit for Kubernetes) is an open-source framework (part of CNCF) that allows you to define your Kubernetes applications using regular programming languages (instead of yaml). We will continue on the same path i.e. push yaml to the background and use the Go programming language to define the core infrastructure (that happens to be DynamoDB in this example, but could be so much more) as well as the application components (Kubernetes Deployment, Service etc.).\nThis is made possible due to the following cdk8s features:\ncdk8s support for Kubernetes Custom Resource definitions that lets us magically import CRD as APIs. cdk8s-plus library that helps reduce/eliminate a lot of boilerplate code while working with Kubernetes resources in our Go code (or any other language for that matter) Before you dive in, please ensure you complete the prerequisites in order to work through the tutorial.\nThe code is available on GitHub\nPrerequisites To follow along step-by-step, in addition to an AWS account, you will need to have AWS CLI, cdk8s CLI, kubectl, helm and the Go programming language installed.\nThere are a variety of ways in which you can create an Amazon EKS cluster. I prefer using eksctl CLI because of the convenience it offers!\nOk let’s get started. The first thing we need to do is…\nSet up the DynamoDB controller Most of the below steps are adapted from the ACK documentation - Install an ACK Controller\nInstall it using Helm:\nexport SERVICE=dynamodb export RELEASE_VERSION=`curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep '\"tag_name\":' | cut -d'\"' -f4` export ACK_SYSTEM_NAMESPACE=ack-system # you can change the region if required export AWS_REGION=us-east-1 aws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws helm install --create-namespace -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller \\ oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION --set=aws.region=$AWS_REGION To confirm, run:\nkubectl get crd # output (multiple CRDs) tables.dynamodb.services.k8s.aws fieldexports.services.k8s.aws globaltables.dynamodb.services.k8s.aws # etc.... Since the DynamoDB controller has to interact with AWS Services (make API calls), we need to configure IAM Roles for Service Accounts (also known as IRSA).\nRefer to Configure IAM Permissions for details\nIRSA configuration\nFirst, create an OIDC identity provider for your cluster.\nexport EKS_CLUSTER_NAME=\u003cname of your EKS cluster\u003e export AWS_REGION=\u003ccluster region\u003e eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --region $AWS_REGION --approve The goal is to create an IAM role and attach appropriate permissions via policies. We can then create a Kubernetes Service Account and attach the IAM role to it. Thus, the controller Pod will be able to make AWS API calls. Note that we are using providing all DynamoDB permissions to our control via the arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess policy.\nThanks to eksctl, this can be done with a single line!\nexport SERVICE=dynamodb export ACK_K8S_SERVICE_ACCOUNT_NAME=ack-$SERVICE-controller # recommend using the same name export ACK_SYSTEM_NAMESPACE=ack-system export EKS_CLUSTER_NAME=\u003center EKS cluster name\u003e export POLICY_ARN=arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess # IAM role has a format - do not change it. you can't use any arbitrary name export IAM_ROLE_NAME=ack-$SERVICE-controller-role eksctl create iamserviceaccount \\ --name $ACK_K8S_SERVICE_ACCOUNT_NAME \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --cluster $EKS_CLUSTER_NAME \\ --role-name $IAM_ROLE_NAME \\ --attach-policy-arn $POLICY_ARN \\ --approve \\ --override-existing-serviceaccounts The policy is per https://github.com/aws-controllers-k8s/dynamodb-controller/blob/main/config/iam/recommended-policy-arn\nTo confirm, you can check whether the IAM role was created and also introspect the Kubernetes service account\naws iam get-role --role-name=$IAM_ROLE_NAME --query Role.Arn --output text kubectl describe serviceaccount/$ACK_K8S_SERVICE_ACCOUNT_NAME -n $ACK_SYSTEM_NAMESPACE # you will see similar output Name: ack-dynamodb-controller Namespace: ack-system Labels: app.kubernetes.io/instance=ack-dynamodb-controller app.kubernetes.io/managed-by=eksctl app.kubernetes.io/name=dynamodb-chart app.kubernetes.io/version=v0.1.3 helm.sh/chart=dynamodb-chart-v0.1.3 k8s-app=dynamodb-chart Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::\u003cyour AWS account ID\u003e:role/ack-dynamodb-controller-role meta.helm.sh/release-name: ack-dynamodb-controller meta.helm.sh/release-namespace: ack-system Image pull secrets: \u003cnone\u003e Mountable secrets: ack-dynamodb-controller-token-bbzxv Tokens: ack-dynamodb-controller-token-bbzxv Events: \u003cnone\u003e For IRSA to take effect, you need to restart the ACK Deployment:\n# Note the deployment name for ACK service controller from following command kubectl get deployments -n $ACK_SYSTEM_NAMESPACE kubectl -n $ACK_SYSTEM_NAMESPACE rollout restart deployment ack-dynamodb-controller-dynamodb-chart Confirm that the Deployment has restarted (currently Running) and the IRSA is properly configured:\nkubectl get pods -n $ACK_SYSTEM_NAMESPACE kubectl describe pod -n $ACK_SYSTEM_NAMESPACE ack-dynamodb-controller-dynamodb-chart-7dc99456c6-6shrm | grep \"^\\s*AWS_\" # The output should contain following two lines: AWS_ROLE_ARN=arn:aws:iam::\u003cAWS_ACCOUNT_ID\u003e:role/\u003cIAM_ROLE_NAME\u003e AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token Now that we’re done with the configuration …\nWe can move on to the fun parts! Start by creating the DynamoDB table\nHere is what the manifest looks like:\napiVersion: dynamodb.services.k8s.aws/v1alpha1 kind: Table metadata: name: dynamodb-urls-ack spec: tableName: urls attributeDefinitions: - attributeName: shortcode attributeType: S billingMode: PAY_PER_REQUEST keySchema: - attributeName: email keyType: HASH Clone the project, change to the correct directory and apply the manifest:\ngit clone https://github.com/abhirockzz/dynamodb-ack-cdk8s cd dynamodb-ack-cdk8s # create table kubectl apply -f manual/dynamodb-ack.yaml You can check the DynamoDB table in the AWS console, or use the AWS CLI (aws dynamodb list-tables) to confirm. Our table is ready, now we can deploy our URL shortener application.\nBut, before that, we need to create a Docker image and push it to a private repository in Amazon Elastic Container Registry (ECR).\nCreate private repository in Amazon ECR\nLogin to ECR:\naws ecr get-login-password --region \u003center region\u003e | docker login --username AWS --password-stdin \u003center aws_account_id\u003e.dkr.ecr.\u003center region\u003e.amazonaws.com Create repository:\naws ecr create-repository \\ --repository-name dynamodb-app \\ --region \u003center AWS region\u003e Build image and push to ECR\n# if you're on Mac M1 #export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker build -t dynamodb-app . docker tag dynamodb-app:latest \u003center aws_account_id\u003e.dkr.ecr.\u003center region\u003e.amazonaws.com/dynamodb-app:latest docker push \u003center aws_account_id\u003e.dkr.ecr.\u003center region\u003e.amazonaws.com/dynamodb-app:latest Just like the controller, our application also needs IRSA to be able to execute GetItem and PutItem API calls on DynamoDB.\nLet’s create another IRSA for the application\n# you can change the policy name. make sure yo use the same name in subsequent commands aws iam create-policy --policy-name dynamodb-irsa-policy --policy-document file://manual/policy.json eksctl create iamserviceaccount --name eks-dynamodb-app-sa --namespace default --cluster \u003center EKS cluster name\u003e --attach-policy-arn arn:aws:iam::\u003center AWS account ID\u003e:policy/dynamodb-irsa-policy --approve kubectl describe serviceaccount/eks-dynamodb-app-sa # output Name: eks-dynamodb-app-sa Namespace: default Labels: app.kubernetes.io/managed-by=eksctl Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::\u003cAWS account ID\u003e:role/eksctl-eks-cluster-addon-iamserviceaccount-d-Role1-2KTGZO1GJRN Image pull secrets: \u003cnone\u003e Mountable secrets: eks-dynamodb-app-sa-token-5fcvf Tokens: eks-dynamodb-app-sa-token-5fcvf Events: \u003cnone\u003e Finally, we can deploy our application!\nIn the manual/app.yaml file, make sure you replace the following attributes as per your environment:\nService account name - the above example used eks-dynamodb-app-sa Docker image Container environment variables for AWS region (e.g. us-east-1) and table name (this will be urls since that’s the name we used) kubectl apply -f app.yaml # output deployment.apps/dynamodb-app configured service/dynamodb-app-service created This will create a Deployment as well as Service to access the application.\nSince the Service type is LoadBalancer, an appropriate AWS Load Balancer will be provisioned to allow for external access.\nCheck Pod and Service:\nkubectl get pods kubectl get service/dynamodb-app-service # to get the load balancer IP APP_URL=$(kubectl get service/dynamodb-app-service -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\") echo $APP_URL # output example a0042d5b5b0ad40abba9c6c42e6342a2-879424587.us-east-1.elb.amazonaws.com You have deployed the application and know the endpoint over which it’s publicly accessible.\nYou can try out the URL shortener service\ncurl -i -X POST -d 'https://abhirockzz.github.io/' $APP_URL:9090/ # output HTTP/1.1 200 OK Date: Thu, 21 Jul 2022 11:03:40 GMT Content-Length: 25 Content-Type: text/plain; charset=utf-8 {\"ShortCode\":\"ae1e31a6\"} If you get a Could not resolve host error while accessing the LB URL, wait for a minute or so and re-try\nYou should receive a JSON response with a short code. Enter the following in your browser http://\u003center APP_URL\u003e:9090/\u003cshortcode\u003e e.g. http://a0042d5b5b0ad40abba9c6c42e6342a2-879424587.us-east-1.elb.amazonaws.com:9090/ae1e31a6 - you will be re-directed to the original URL.\nYou can also use curl:\n# example curl -i http://a0042d5b5b0ad40abba9c6c42e6342a2-879424587.us-east-1.elb.amazonaws.com:9090/ae1e31a6 # output HTTP/1.1 302 Found Location: https://abhirockzz.github.io/ Date: Thu, 21 Jul 2022 11:07:58 GMT Content-Length: 0 Enough of YAML I guess! Like I promised earlier, the second half will demonstrate how to achieve the same using cdk8s and Go.\nKubernetes infrastructure as Go code with cdk8s Assuming you’ve already cloned the project (as per above instructions), change to a different directory:\ncd dynamodb-cdk8s This is a pre-created cdk8s project that you can use. The entire logic is present in main.go file. We will first try it out and confirm that it works the same way. After that we will dive into the nitty gritty of the code.\nDelete the previously created DynamoDB table and along with the Deployment (and Service) as well:\n# you can also delete the table directly from AWS console aws dynamodb delete-table --table-name urls # this will delete Deployment and Service (as well as AWS Load Balancer) kubectl delete -f manual/app.yaml Use cdk8s synth to generate the manifest for DynamoDB table and the application. We can then apply it using kubectl\nSee the difference? Earlier, we defined the DynamoDB table, Deployment (and Service) manifests manually. cdk8s does not remove YAML altogether, but it provides a way for us to leverage regular programming languages (Go in this case) to define the components of our solution.\nexport TABLE_NAME=urls export SERVICE_ACCOUNT=eks-dynamodb-app-sa export DOCKER_IMAGE=\u003center ECR repo that you created earlier\u003e cdk8s synth ls -lrt dist/ #output 0000-dynamodb.k8s.yaml 0001-deployment.k8s.yaml You will see two different manifests being generated by cdk8s since we defined two separate cdk8s.Charts in the code - more on this soon.\nWe can deploy them one by one:\nkubectl apply -f dist/0000-dynamodb.k8s.yaml #output table.dynamodb.services.k8s.aws/dynamodb-dynamodb-ack-cdk8s-table-c88d874d created configmap/export-dynamodb-urls-info created fieldexport.services.k8s.aws/export-dynamodb-tablename created fieldexport.services.k8s.aws/export-dynamodb-region created As always, you can check the DynamoDB table either in console or AWS CLI - aws dynamodb describe-table --table-name urls. Looking at the output, the DynamoDB table part seems familiar…\nBut what’s fieldexport.services.k8s.aws?? … And why do we need a ConfigMap? I will give you the gist here.\nIn the previous iteration, we hard-coded the table name and region in manual/app.yaml. While this works for this sample app, it is not scalable and might not even work for a few resources in case the metadata (like name etc.) is randomly generated. That’s why there is this concept of a FieldExport that can “export any spec or status field from an ACK resource into a Kubernetes ConfigMap or Secret”\nYou can read up on the details in the ACK documentation along with some examples. What you will see here is how to define a FieldExport and ConfigMap along with the Deployment which needs to be configured to accept its environment variables from the ConfigMap - all this in code, using Go (more on this during code walk-through).\nCheck the FieldExport and ConfigMap:\nkubectl get fieldexport #output NAME AGE export-dynamodb-region 19s export-dynamodb-tablename 19s kubectl get configmap/export-dynamodb-urls-info -o yaml We started out with a blank ConfigMap (as per cdk8s logic), but ACK magically populated it with the attributes from the Table custom resource.\napiVersion: v1 data: default.export-dynamodb-region: us-east-1 default.export-dynamodb-tablename: urls immutable: false kind: ConfigMap #....omitted We can now use the second manifest - no surprises here. Just like in the previous iteration, all it contains is the application Deployment and the Service. Check Pod and Service:\nkubectl apply -f dist/0001-deployment.k8s.yaml #output deployment.apps/dynamodb-app created service/dynamodb-app-service configured kubectl get pods kubectl get svc The entire setup is ready, just like it was earlier and you can test it the same way. I will not repeat the steps here. Instead, I will move to something more interesting.\ncdk8s code walk-through The logic is divided into two Charts. I will only focus on key sections of the code and rest will be ommitted for brevity.\nDynamoDB and configuration\nWe start by defining the DynamoDB table (name it urls) as well as the ConfigMap (note that it does not have any data at this point):\nfunc NewDynamoDBChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //... table := ddbcrd.NewTable(chart, jsii.String(\"dynamodb-ack-cdk8s-table\"), \u0026ddbcrd.TableProps{ Spec: \u0026ddbcrd.TableSpec{ AttributeDefinitions: \u0026[]*ddbcrd.TableSpecAttributeDefinitions{ {AttributeName: jsii.String(primaryKeyName), AttributeType: jsii.String(\"S\")}}, BillingMode: jsii.String(billingMode), TableName: jsii.String(tableName), KeySchema: \u0026[]*ddbcrd.TableSpecKeySchema{ {AttributeName: jsii.String(primaryKeyName), KeyType: jsii.String(hashKeyType)}}}}) //... cfgMap = cdk8splus22.NewConfigMap(chart, jsii.String(\"config-map\"), \u0026cdk8splus22.ConfigMapProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{ Name: jsii.String(configMapName)}}) Then we move on to the FieldExports - one each for the AWS region and the table name. As soon as these are created, the ConfigMap is populated with the required data as per from and to configuration in the FieldExport.\n//... fieldExportForTable = servicesk8saws.NewFieldExport(chart, jsii.String(\"fexp-table\"), \u0026servicesk8saws.FieldExportProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(fieldExportNameForTable)}, Spec: \u0026servicesk8saws.FieldExportSpec{ From: \u0026servicesk8saws.FieldExportSpecFrom{Path: jsii.String(\".spec.tableName\"), Resource: \u0026servicesk8saws.FieldExportSpecFromResource{ Group: jsii.String(\"dynamodb.services.k8s.aws\"), Kind: jsii.String(\"Table\"), Name: table.Name()}}, To: \u0026servicesk8saws.FieldExportSpecTo{ Name: cfgMap.Name(), Kind: servicesk8saws.FieldExportSpecToKind_CONFIGMAP}}}) fieldExportForRegion = servicesk8saws.NewFieldExport(chart, jsii.String(\"fexp-region\"), \u0026servicesk8saws.FieldExportProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(fieldExportNameForRegion)}, Spec: \u0026servicesk8saws.FieldExportSpec{ From: \u0026servicesk8saws.FieldExportSpecFrom{ Path: jsii.String(\".status.ackResourceMetadata.region\"), Resource: \u0026servicesk8saws.FieldExportSpecFromResource{ Group: jsii.String(\"dynamodb.services.k8s.aws\"), Kind: jsii.String(\"Table\"), Name: table.Name()}}, To: \u0026servicesk8saws.FieldExportSpecTo{ Name: cfgMap.Name(), Kind: servicesk8saws.FieldExportSpecToKind_CONFIGMAP}}}) //... The application chart\nThe core of our application is the Deployment itself:\nfunc NewDeploymentChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //... dep := cdk8splus22.NewDeployment(chart, jsii.String(\"dynamodb-app-deployment\"), \u0026cdk8splus22.DeploymentProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{ Name: jsii.String(\"dynamodb-app\")}, ServiceAccount: cdk8splus22.ServiceAccount_FromServiceAccountName( chart, jsii.String(\"aws-irsa\"), jsii.String(serviceAccountName))}) The next important part is the container and it’s configuration. We specify the ECR image repository along with the environment variables - they reference the ConfigMap we defined in the previous chart (everything is connected!):\n//... container := dep.AddContainer( \u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"dynamodb-app-container\"), Image: jsii.String(image), Port: jsii.Number(appPort)}) container.Env().AddVariable(jsii.String(\"TABLE_NAME\"), cdk8splus22.EnvValue_FromConfigMap( cfgMap, jsii.String(\"default.\"+*fieldExportForTable.Name()), \u0026cdk8splus22.EnvValueFromConfigMapOptions{Optional: jsii.Bool(false)})) container.Env().AddVariable(jsii.String(\"AWS_REGION\"), cdk8splus22.EnvValue_FromConfigMap( cfgMap, jsii.String(\"default.\"+*fieldExportForRegion.Name()), \u0026cdk8splus22.EnvValueFromConfigMapOptions{Optional: jsii.Bool(false)})) Finally, we define the Service (type LoadBalancer) which enables external application access and tie it all together in the main function:\n//... dep.ExposeViaService( \u0026cdk8splus22.DeploymentExposeViaServiceOptions{ Name: jsii.String(\"dynamodb-app-service\"), ServiceType: cdk8splus22.ServiceType_LOAD_BALANCER, Ports: \u0026[]*cdk8splus22.ServicePort{ {Protocol: cdk8splus22.Protocol_TCP, Port: jsii.Number(lbPort), TargetPort: jsii.Number(appPort)}}}) //... func main() { app := cdk8s.NewApp(nil) dynamodDB := NewDynamoDBChart(app, \"dynamodb\", nil) deployment := NewDeploymentChart(app, \"deployment\", nil) deployment.AddDependency(dynamodDB) app.Synth() } That’s all I have for you in this blog!\nDon’t forget to delete resources.. # delete DynamoDB table, Deployment, Service etc. kubectl delete -f dist/ # to uninstall the ACK controller export SERVICE=dynamodb helm uninstall -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller # delete the EKS cluster. if created via eksctl: eksctl delete cluster --name \u003center name of eks cluster\u003e ","kubernetes-infrastructure-as-go-code-with-cdk8s#Kubernetes infrastructure as Go code with cdk8s":"","prerequisites#Prerequisites":"","set-up-the-dynamodb-controller#Set up the DynamoDB controller":"","we-can-move-on-to-the-fun-parts#We can move on to the fun parts!":"","whats-covered-in-this-blog-post#What\u0026rsquo;s covered in this blog post?":"","wrap-up#Wrap up..":"AWS Controllers for Kubernetes help bridge the gap between traditional Kubernetes resources and AWS services by allowing you to manage both from a single control plane. In this blog you saw how to do this in the context of DynamoDB and a URL shortener application (deployed to Kubernetes). I encourage you to try out other AWS services that ACK supports - here is a complete list.\nThe approach presented here will work well if just want to use cdk8s. However, depending on your requirements, there is another way this can done by bringing in AWS CDK into the picture. I want to pause right here since this is something I might cover in a future blog post.\nUntil then, Happy Building!"},"title":"Write your Kubernetes Infrastructure as Go code - Manage AWS services"},"/blog/cdk8s-k8s-go-6/":{"data":{"":"In an earlier blog post you saw how to use cdk8s with AWS Controllers for Kubernetes (also known as ACK), thanks to the fact that you can import existing Kubernetes Custom Resource Definitions using cdk8s! This made is possible to deploy DynamoDB along with a client application, from using cdk8s and Kubernetes.\nBut, what if you continue using AWS CDK for AWS infrastructure and harness the power cdk8s (and cdk8s-plus!) to define Kubernetes resources using regular code? Thanks to the native integration between the AWS EKS module and cdk8s, you can have the best of both worlds!\nThe goal of this blog post is to demonstrate that with a few examples. We will start off with a simple (nginx based) example before moving on to a full-fledged application stack (including DynamoDB etc.). Both will be using the Go programming language which is well supported in AWS CDK as well as cdk8s.\nAll the code discussed in this blog is available in this GitHub repo","end-to-end-example-dynamodb-along-with-an-application-on-eks#End to end example: DynamoDB along with an application on EKS":"Instead of creating a new EKS cluster from scratch, we will re-use the existing cluster created as a result of the previous example - this is a good opportunity to take a look at how you can reference an existing EKS cluster in your CDK code. As expected, we need to create DynamoDB table as well.\nJust like in the previous example, let’s try out the solution first. Change into the right directory first:\ncd part6-cdk-eks-cdk8s/cdk-cdk8s-dynamodb-app-eks Since the URL shortener application has to make API calls to DynamoDB, we need to configure IAM Roles for Service Accounts (also known as IRSA).\nRefer to https://docs.aws.amazon.com/eks/latest/userguide/create-service-account-iam-policy-and-role.html\nDefine IAM roles for the application\nStart by creating a Kubernetes Service Account:\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: eks-dynamodb-app-sa EOF To confirm - kubectl get serviceaccount/eks-dynamodb-app-sa -o yaml\nSet your AWS Account ID and OIDC Identity provider as environment variables:\nACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) export EKS_CLUSTER_NAME=\u003center cluster name\u003e export AWS_REGION=\u003center region e.g. us-east-1\u003e OIDC_PROVIDER=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\") Create a JSON file with Trusted Entities for the role:\nread -r -d '' TRUST_RELATIONSHIP \u003c\u003cEOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:aud\": \"sts.amazonaws.com\", \"${OIDC_PROVIDER}:sub\": \"system:serviceaccount:default:eks-dynamodb-app-sa\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" \u003e trust.json Check - cat trust.json\nNow, create the IAM role:\nexport ROLE_NAME=dynamodb-app-irsa aws iam create-role --role-name $ROLE_NAME --assume-role-policy-document file://trust.json --description \"IRSA for DynamoDB app on EKS\" We will need to create and attach policy to role since we only want to allow PutItem and GetItem operations from our application. Here is the policy JSON file:\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PutandGet\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:PutItem\", \"dynamodb:GetItem\" ], \"Resource\": \"arn:aws:dynamodb:*:*:table/urls\" } ] } Create and attach the policy to the role we just created:\naws iam create-policy --policy-name dynamodb-irsa-policy --policy-document file://policy.json aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn=arn:aws:iam::\u003center AWS account ID\u003e:policy/dynamodb-irsa-policy Finally, we need to associate the IAM role and Service Account:\nkubectl annotate serviceaccount -n default eks-dynamodb-app-sa eks.amazonaws.com/role-arn=arn:aws:iam::\u003center AWS account ID\u003e:role/dynamodb-app-irsa Get the EKS kubectl role ARN\nTo reference existing EKS cluster in AWS CDK, you need EKS cluster name and kubectl role ARN.\nYou can find the role ARN in the Outputs section of the AWS Cloud Formation stack.\nWe are ready to deploy the application using CDK. Set the required environment variables followed by cdk deploy:\nyou can also use cdk synth to generate and inspect the Cloud Formation template first\nexport EKS_CLUSTER_NAME=\u003center name of EKS cluster\u003e export KUBECTL_ROLE_ARN=\u003center kubectl role ARN\u003e export SERVICE_ACCOUNT_NAME=eks-dynamodb-app-sa export APP_PORT=8080 export AWS_REGION=\u003center region e.g. us-east-1\u003e cdk deploy CDK (and cdk8s) will do all the heavy lifting (we will look at the code very soon):\nNew DynamoDB table will be created The docker image for our application will be built and pushed to ECR Kubernetes resources for the URL shortener application will be deployed to the existing EKS cluster Once the stack creation is complete, check the Kubernetes Deployment and Service:\nkubectl get deployment/dynamodb-app kubectl get pods kubectl get service/dynamodb-app-service Testing the URL shortener service is easy. But I will not repeat it here since its already covered in [this blog](https://dev.to/abhirockzz/write-your-kubernetes-infrastructure-as-go-code-manage-aws-services-3pgi. All you need is the load balancer URL to access the service and use yout browser or curl to save and access URLs.\nBack to exploring Go code again\nWithin the stack, we define the DynamoDB table (using awsdynamodb.NewTable) along with the docker image for our application (with awsecrassets.NewDockerImageAsset)\nfunc NewDynamoDBAppStack(scope constructs.Construct, id string, props *CdkStackProps) awscdk.Stack { //... table := awsdynamodb.NewTable(stack, jsii.String(\"dynamodb-table\"), \u0026awsdynamodb.TableProps{ TableName: jsii.String(tableName), PartitionKey: \u0026awsdynamodb.Attribute{ Name: jsii.String(dynamoDBPartitionKey), Type: awsdynamodb.AttributeType_STRING, }, BillingMode: awsdynamodb.BillingMode_PAY_PER_REQUEST, RemovalPolicy: awscdk.RemovalPolicy_DESTROY, }) appDockerImage := awsecrassets.NewDockerImageAsset(stack, jsii.String(\"app-image\"), \u0026awsecrassets.DockerImageAssetProps{ Directory: jsii.String(appDirectory)}) //... Then comes the interesting part where we get the reference to our exsiting EKS cluster and use AddCdk8sChart (just like before) to deploy the application to EKS.\n//... eksCluster := awseks.Cluster_FromClusterAttributes(stack, jsii.String(\"existing cluster\"), \u0026awseks.ClusterAttributes{ ClusterName: jsii.String(eksClusterName), KubectlRoleArn: jsii.String(kubectlRoleARN)}) app := cdk8s.NewApp(nil) appProps := NewAppChartProps(appDockerImage.ImageUri(), table.TableName()) eksCluster.AddCdk8sChart(jsii.String(\"dynamodbapp-chart\"), NewDynamoDBAppChart(app, \"dynamodb-cdk8s\", \u0026appProps), nil) The NewDynamoDBAppChart function defines the Deployment and Service. Unlike the earlier Nginx example which had static values, this application takes in dynamic values - specifically the DynamoDB table name (which is used as the container environment variable TABLE_NAME). Also notice the fact that we explicitly add the the name of the Kubernetes service account (for IRSA) that we had created in the previous step.\nfunc NewDynamoDBAppChart(scope constructs.Construct, id string, props *AppChartProps) cdk8s.Chart { //... dep := cdk8splus22.NewDeployment(chart, jsii.String(\"dynamodb-app-deployment\"), \u0026cdk8splus22.DeploymentProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{ Name: jsii.String(\"dynamodb-app\")}, ServiceAccount: cdk8splus22.ServiceAccount_FromServiceAccountName( chart, jsii.String(\"aws-irsa\"), jsii.String(props.serviceAccountName))}) container := dep.AddContainer(//.. omitted for brevity) container.Env().AddVariable(jsii.String(\"TABLE_NAME\"), cdk8splus22.EnvValue_FromValue(props.tableName)) container.Env().AddVariable(jsii.String(\"AWS_REGION\"), cdk8splus22.EnvValue_FromValue(\u0026props.region)) dep.ExposeViaService(//.. omitted for brevity) return chart } ","keeping-it-simple-with-nginx-on-eks#Keeping it simple with Nginx on EKS":"As with most things in life, there are two ways - the easy way or the hard way ;) You will see both of them! Let’s try things out first, see them working and then look at the code.\nTo start off, clone the repo and change to the right directory:\ngit clone https://github.com/abhirockzz/cdk8s-for-go-developers cd cdk8s-for-go-developers/part6-cdk-eks-cdk8s/cdk-cdk8s-nginx-eks To setup everything, all you need is a single command:\ncdk deploy you can also use cdk synth to generate and inspect the Cloud Formation template first\nYou will be prompted to confirm. Once you do that, the process will kick off - it will take some time since lots of AWS resources will be created, including VPC, EKS cluster etc.\nFeel free to check the AWS Cloud Formation console to track the progress.\nOnce the process is complete, you need to connect to the EKS cluster using kubectl. The command required for this will be available as a result of the cdk deploy process (in the terminal) or you can refer to the Outputs section of the AWS Cloud Formation stack.\nOnce you’ve configured kubectl to point to your EKS cluster, you can check the Nginx Deployment and Service.\nkubectl get deployment # output NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment-cdk8s 1/1 1 1 1m nginx-deployment-cdk 1/1 1 1 1m You will see that two Deployments have been created - more on this soon. Similiarly, if you check the Service (kubectl get svc), you should see two of them - nginx-service-cdk and nginx-service-cdk8s.\nTo access Nginx, pick the EXTERNAL-IP of any of the two Services. For example:\nAPP_URL=$(kubectl get service/nginx-service-cdk -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\") echo $APP_URL # to access nginx (notice we are using port 9090) curl -i http://$APP_URL:9090 If you get a Could not resolve host error while accessing the LB URL, wait for a minute or so and re-try\nBehind the scenes Let’s look at the code now - this will clarify why we have two Nginx Deployments.\nThanks to AWS CDK, VPC creation is a one liner with awsec2.NewVpc function and creating an EKS cluster isn’t too hard either!\nfunc NewNginxOnEKSStack(scope constructs.Construct, id string, props *CdkStackProps) awscdk.Stack { //... vpc := awsec2.NewVpc(stack, jsii.String(\"demo-vpc\"), nil) eksSecurityGroup := awsec2.NewSecurityGroup(stack, jsii.String(\"eks-demo-sg\"), \u0026awsec2.SecurityGroupProps{ Vpc: vpc, SecurityGroupName: jsii.String(\"eks-demo-sg\"), AllowAllOutbound: jsii.Bool(true)}) eksCluster := awseks.NewCluster(stack, jsii.String(\"demo-eks\"), \u0026awseks.ClusterProps{ ClusterName: jsii.String(\"demo-eks-cluster\"), Version: awseks.KubernetesVersion_V1_21(), Vpc: vpc, SecurityGroup: eksSecurityGroup, VpcSubnets: \u0026[]*awsec2.SubnetSelection{ {Subnets: vpc.PrivateSubnets()}}, DefaultCapacity: jsii.Number(2), DefaultCapacityInstance: awsec2.InstanceType_Of(awsec2.InstanceClass_BURSTABLE3, awsec2.InstanceSize_SMALL), DefaultCapacityType: awseks.DefaultCapacityType_NODEGROUP, OutputConfigCommand: jsii.Bool(true), EndpointAccess: awseks.EndpointAccess_PUBLIC()}) //... ","nginx-on-kubernetes-the-hard-way#Nginx on Kubernetes, the hard way!":"Now we look at two different ways of creating Nginx, starting with the “hard” way. In this case, we use AWS CDK (not cdk8s) to define the Deployment and Service resources.\nfunc deployNginxUsingCDK(eksCluster awseks.Cluster) { appLabel := map[string]*string{ \"app\": jsii.String(\"nginx-eks-cdk\"), } deployment := map[string]interface{}{ \"apiVersion\": jsii.String(\"apps/v1\"), \"kind\": jsii.String(\"Deployment\"), \"metadata\": map[string]*string{ \"name\": jsii.String(\"nginx-deployment-cdk\"), }, \"spec\": map[string]interface{}{ \"replicas\": jsii.Number(1), \"selector\": map[string]map[string]*string{ \"matchLabels\": appLabel, }, \"template\": map[string]interface{}{ \"metadata\": map[string]map[string]*string{ \"labels\": appLabel, }, \"spec\": map[string][]map[string]interface{}{ \"containers\": { { \"name\": jsii.String(\"nginx\"), \"image\": jsii.String(\"nginx\"), \"ports\": []map[string]*float64{ { \"containerPort\": jsii.Number(80), }, }, }, }, }, }, }, } service := map[string]interface{}{ \"apiVersion\": jsii.String(\"v1\"), \"kind\": jsii.String(\"Service\"), \"metadata\": map[string]*string{ \"name\": jsii.String(\"nginx-service-cdk\"), }, \"spec\": map[string]interface{}{ \"type\": jsii.String(\"LoadBalancer\"), \"ports\": []map[string]*float64{ { \"port\": jsii.Number(9090), \"targetPort\": jsii.Number(80), }, }, \"selector\": appLabel, }, } eksCluster.AddManifest(jsii.String(\"app-deployment\"), \u0026service, \u0026deployment) } Finally, to create this in EKS we invoke AddManifest (think of its like the programmatic equivalent of kubectl apply). This works, but there are a few gaps in this approach:\nWe are not able to reap the benefits of Go which is a strongly typed language. That’s because the API is loosely typed, thanks to map[string]interface{} everywhere. This makes it very error prone (I made a few mistakes too!) The verbosity is apparent as well. It seems as if we are writing YAML in Go - not too much of an improvement! Is there a better way..? Let’s look at the second function deployNginxUsingCDK8s - by the name its obvious that we used cdk8s, not just CDK)\nfunc deployNginxUsingCDK8s(eksCluster awseks.Cluster) { app := cdk8s.NewApp(nil) eksCluster.AddCdk8sChart(jsii.String(\"nginx-eks-chart\"), NewNginxChart(app, \"nginx-cdk8s\", nil), nil) } This looks “too easy” to to be true! But it’s made possible due to the inter-operability between CDK and cdk8s. What this implies is that, you can use define Kubernetes resources using cdk8s Charts and apply them to an EKS cluster created with CDK (this makes it sort of a hybrind system).\nThe hero of our story is AddCdk8sChart function, which accepts a constructs.Construct (remember, everything is a construct!). In this case, the Construct happens to be a cdk8s.Chart thats returned by NewNginxChart function - so lets take a look at that.\nfunc NewNginxChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //.... dep := cdk8splus22.NewDeployment(chart, jsii.String(\"nginx-deployment\"), \u0026cdk8splus22.DeploymentProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{ Name: jsii.String(\"nginx-deployment-cdk8s\")}}) dep.AddContainer(\u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"nginx-container\"), Image: jsii.String(\"nginx\"), Port: jsii.Number(80)}) dep.ExposeViaService(\u0026cdk8splus22.DeploymentExposeViaServiceOptions{ Name: jsii.String(\"nginx-service-cdk8s\"), ServiceType: cdk8splus22.ServiceType_LOAD_BALANCER, Ports: \u0026[]*cdk8splus22.ServicePort{{ Port: jsii.Number(9090), TargetPort: jsii.Number(80)}}}) return chart } If you have worked with cdk8s (and Go) or read some of my previous blogs on this topic, this should look familiar - a strongly-typed, compact and expressive API! I don’t even need to walk you through this since its so readable - we use cdk8s-plus to create a Nginx Deployment, add the container info and finally expose it via a Service so that we can access the Nginx from outside of EKS.\nThis was a simple enough example to help bring out difference between the two approaches. The next scenario is different - in addition to the EKS cluster it has DynamoDB along with a URL shortener application that will be deployed to EKS.","prerequisites#Prerequisites":"To follow along step-by-step, in addition to an AWS account, you will need following CLIs - AWS CLI, cdk8s CLI and kubectl. Also, dont’ forget to install AWS CDK, the Go programming language (v1.16 or above) as well as Docker, if you don’t have them already.","wrap-up#Wrap up":"That’s all for this blog! We started off with a simple example to showcase the integration between AWS CDK and cdk8s and how easy it makes things (compared to just using CDK to deploy apps to EKS). Then, we moved on to explore a full fledged scenario where you deployed the infrastructure (DynamoDB etc.) along with the client application on EKS.\nHappy Building!"},"title":"Write your Kubernetes Infrastructure as Go code - Combine cdk8s with AWS CDK"},"/blog/conf42-go-redis-better-together/":{"data":{"":"I recently presented this talk at the Conf42 Golang 2023 and I thought it might be a good idea to turn it into a blog post for folks who don’t want to spend 40+ mins watching the talk (it’s ok, I understand 😉) or just staring at slides trying to imagine what I was saying.\nSo here you go….\nBy the way, you are still welcome to watch the talk or download the slides! 🙌 There are a lot of great talks that you can get from this playlist.\nThis talk was geared towards folks who are looking to get started with Redis and Go. Or perhaps you are already experienced with both these topics – in that case, it might be a good refresher!\nTo that extent, I had a very simple agenda.\nI started off by setting the context about Redis and Go Provide an overview of Go and Redis ecosystem, including the client options you’ve got.. Followed by some hand-on stuff and, Wrap with some tips/tricks and resources. ","client-ecosystem#Client ecosystem":"\nNow lets take a look from an ecosystem perspective. To clarify, the point here is not chest thumping based on GitHub stars - its just to give you a sense of things.\nFor folks not using Go and Redis, the popularity of the Go client might come as a surprise. Java workloads form a huge chunk of the Redis workloads and Jedis is the bread and butter client when it comes to Java apps with Redis (and it’s pretty old). But I was surprised to redisson topping the charts, followed by go-redis (yay!), (two) Nodejs clients, Python and finally back to Java.\nAnother thing to note is that I was looking for more than 10000 stars. At the time of writing, the phpredis client was close to 9600 stars.\nTime for some practical stuff….","demos#Demos":"\nDuring the demo, I covered:\nA walk though of the Go Redis client:\nBasics such as connecting to Redis Using common data types like string with TTL Use hash and struct support in go-redis How to use set as well as pipelining technique. Hyperloglog and how does it differ from a Set:\nfunc hllandset() { pipe := client.Pipeline() ips := []string{} for i := 1; i \u003c= 10_00_000; i++ { ips = append(ips, fake.IP(fake.WithIPv4())) } pipe.SAdd(context.Background(), \"set_of_ips\", ips) pipe.PFAdd(context.Background(), \"hll_of_ips\", ips) pipe.Exec(ctx) //redis-cli MEMORY usage set_of_ips fmt.Println(\"no. of unique views (SCARD) -\", client.SCard(ctx, \"set_of_ips\").Val()) //redis-cli MEMORY usage hll_of_ips fmt.Println(\"no. of unique views (PFCOUNT) -\", client.PFCount(ctx, \"hll_of_ips\").Val()) Chat application using pub/sub (below is a trimmed down version of the code):\npackage main import ( //omitted ) var client *redis.Client var Users map[string]*websocket.Conn var sub *redis.PubSub var upgrader = websocket.Upgrader{} const chatChannel = \"chats\" func init() { Users = map[string]*websocket.Conn{} } func main() { //...connect to redis (omitted) broadcast() http.HandleFunc(\"/chat/\", chat) server := http.Server{Addr: \":8080\", Handler: nil} //...start server (omitted) exit := make(chan os.Signal, 1) signal.Notify(exit, syscall.SIGTERM, syscall.SIGINT) \u003c-exit ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() //... clean up all connected connections, unsubscribe and shutdown server (omitted) } func chat(w http.ResponseWriter, r *http.Request) { user := strings.TrimPrefix(r.URL.Path, \"/chat/\") upgrader.CheckOrigin = func(r *http.Request) bool { return true } // 1. create websocket connection c, err := upgrader.Upgrade(w, r, nil) if err != nil { log.Print(\"upgrade:\", err) return } // 2. associate the user (name) with the actual connection Users[user] = c fmt.Println(user, \"in chat\") for { _, message, err := c.ReadMessage() if err != nil { //error handling and disconnect (omitted) } // 3. when a messages comes in via the connection, publish messages to redis channel client.Publish(context.Background(), chatChannel, user+\":\"+string(message)) } } func broadcast() { go func() { sub = client.Subscribe(context.Background(), chatChannel) messages := sub.Channel() for message := range messages { from := strings.Split(message.Payload, \":\")[0] // 3. if a messages is received on the redis channel, broadcast it to all connected sessions (users) for user, peer := range Users { if from != user { peer.WriteMessage(websocket.TextMessage, []byte(message.Payload)) } } } }() } ","go-clients-for-redis#Go clients for Redis":"\n👆🏼👆🏼 These are the most popular Go clients for Redis.\ngo-redis is by far the most popular client. It has what you’d expect.. Features, decent documentation, active community. Moving this under the official Redis GitHub org is just icing on the cake!\nredigo is a fairly stable and tenured client that supports all the standard Redis data types and features such as transactions, pipelining etc. It is also used to implement other Go client libraries such as redisearch-go and Redis TimeSeries Go client. That being said, its API is a bit too flexible. While some may prefer that, but I feel that it’s not a good fit when I am using a type-safe language like Go (that’s just my personal opinion).\nBut the biggest drawback to me is that it does not support Redis Cluster!\nrueidis is relatively new (at the time of writing) but quickly evolving client library. It supports RESP3 protocol, client-side caching and supports a variety of Redis Modules. As far as the API is concerned, this client adopts an interesting approach. It provides a Do function as well (like redigo client), but the way it creates the command is via a builder pattern - this retains strong type checking (unlike redigo).\nTo be honest with you, I haven’t used this client a lot, and its looks like its packed with a lot of features. So, I cant complain much as of now!\nFor those who are looking for deeper performance numbers – this benchmark comparison with the go-redis library might be interesting.","i-love-redis-and-go#I love Redis and Go!":"\nSince its release in 2009, it did not take Redis too long to win the hearts and minds of the developer community! As per db-engines trends statistics, Redis has been topping the charts since 2013. And on Stack Overflow annual survey, it’s been voted most loved database for 5 years in a row.\nGo has become the language of the Cloud. It powers many cloud-native projects (apparently, 75% of CNCF projects are written in Go) including Docker, Kubernetes, Prometheus, Terraform etc. In fact there many databases written in Go - like Influxdb, etcd, Vitess, TiDB etc.\nGo also caters to a wide variety of general purpose use cases:\nWeb apps and APIs, Data processing pipelines, Infrastructure as code, SRE/DevOps solutions, Command line apps (this is a really popular one!) and more… No wonder Go has become such a popular programming language!\nNow you might be thinking - “Hey Go is down at the bottom.” But if you notice carefully, it is the only statically-typed lang after Rust (of course there are C# and Kotlin down there as well) And this is from 2022 - if you look at data from 2021 to 2018, you will notice that Go has maintained its top 5 spot.\nGo and Redis have a few things in common, but to me “Simplicity” is the one that really stands out to me.","redis-101#Redis 101":"\nQuick intro to Redis (some of the key points):\nData structure server - At its core, Redis is nothing but a key value store, where the key can be string or even a binary. The important thing to note is that as far as the value is concerned, you can choose from a variety of data structures such as strings, hashes, lists, sets etc. Redis is not just a cache - It’s a really solid messaging platform as well HA - You can configure redis to be highly avaialbe by setting up primary-replica replication, or take it a step further with Redis Cluster. Persistence: Redis is primarily in-memory but you can configure it to persist to disk as well. But there are solutions like Amazon MemoryDB that can actually take it a notch further (thanks to it’s distributed transactional log) Since its open-source and wildy popular, you can get offerings from pretty much every cloud provider – big or small or even run it on Kubernetes, on cloud, on-prem or hybrid mode. If you want to put Redis in production, you can be rest assured that there are no dearth of options for you to run and operate it. Redis data types What you see here is a list of core data structures:\nA string seems really simple, but are quite powerful. They can be used for something as simple as storing a key-value pair to implementing advanced patterns like distributed locking and rate-limiting A hash is similar to a map in Java, or dictionary in Python. It is used to store object like structures like user profile, customer info etc. A set behaves like its mathematical version - helps you maintain unique set of items along with ability to list them, count them, execute unions, intersection and so on. Think of sorted sets are like this big brother to a set - makes it possible to implement things like leaderboards which is very useful in areas like gaming. For e.g. you can store player scores in a sorted set and when you need to get the top 10 players, you can simply invoke specific sorted set commands and get that data. The beauty is that sorting happens on the database side, so there is no client side logic you need to apply. Lists are a really versatile data structure as well. You can use them to store many things, but using it as a worker queue is very common. There are popular open source solution such as sidekiq and celery that already support Redis as a backend for job queuing solutions. Redis streams (added in Redis 5) is used for streaming use-cases And also ephemeral messaging with pub/sub - it’s a publish-broadcast mechanism where you can send/receive messages to/from channels. There is also Geospatial data structure and a really cool one called Hyperloglog which is an alternative to a traditional set. It can store millions of items while optimizing for data storage and you can count the number of unique items with really high accuracy. ","resources#Resources":"… and finally I wrapped up with some resources, including the Discord channel for Go Redis community.\nHappy Building!","simplicity#Simplicity":"\nRedis is a key value store, but the values can be any of these data structures that you see. These are all data structures that we as developers use every day - lists, sets, maps etc. Redis just feels like an extension to these core programming language constructs.\nWith Go, this comes in various forms…\nExcellent tooling, Or a comprehensive standard library, Easy to use concurrency primitives… And sometimes it’s in the form of not bloating the language with unnecessary features. To cite an example, it took a while for generics to be added to the language. Now I am not trying to trick you into thinking that Go is simple.. Or for that matter any programming language is simple.\nBut with Go, the goal is to give you simple components to help build complex things, and hide complexity behind a simple facade.\nAnd there are folks who have explained this in great detail (and much better than I can!). I would really encourage you to check out this talk by Rob Pike (and the slides), the co-creator of Go (its from 2015 but still very much applicable to the essence and spirit of Go).","tips-and-tricks#Tips and tricks":"In this section, I covered some of the points from Using Redis on Cloud? Here are ten things you should know.\nThese include:\nConnecting to Redis - common mistakes Scalability options - Vertical, Horizontal Using read-replicas Influence how your keys are distributed across a Redis cluster Execute bulk operations across Redis Cluster Sharded Pub/Sub "},"title":"Go and Redis, Better Together 🤝"},"/blog/cosmos-kafka-docker/":{"data":{"":"Having a local development environment is quite handy when trying out a new service or technology. Docker has emerged as the de-facto choice in such cases. It is specially useful in scenarios where you’re trying to integrate multiple services and gives you the ability to to start fresh before each run.\nThis blog post is a getting started guide for the Kafka Connector for Azure Cosmos DB. All the components (including Azure Cosmos DB) will run on your local machine, thanks to:\nThe Azure Cosmos DB Linux Emulator which can be used for local development and testing purposes without creating an Azure subscription or incurring any costs.\nAnd, Docker Compose which is a tool for defining and running multi-container Docker applications. It will orchestrate all the components required by our setup including Azure Cosmos DB emulator, Kafka, Zookeeper, Kafka connectors etc.\nTo make things easier, we will pick single-focused scenarios and go step by step:\nStep 0 — A simple scenario to check if our setup if functional.\nHow to handle streaming JSON data\nHow to handle streaming JSON data which is not compatible with Azure Cosmos DB\nHow to handle Avro data using Schema Registry\nIt is assumed that you’re comfortable with Kafka and have an understanding of Kafka Connect","before-you-start#Before you start…":"Make sure you have Docker and docker-compose installed.\nAlso, clone the project from GitHub:\ngit clone [https://github.com/Azure-Samples/cosmosdb-kafka-connect-docker](https://github.com/Azure-Samples/cosmosdb-kafka-connect-docker) cd cosmosdb-kafka-connect-docker ","clean-up#Clean up":"Once you have finished, you can delete the connectors:\ncurl -X DELETE http://localhost:8080/connectors/datagen-orders/ curl -X DELETE http://localhost:8083/connectors/orders-sink/ To stop all the Docker components:\ndocker-compose -p cosmosdb-kafka-docker down -v ","conclusion#Conclusion":"Although we covered simple scenarios for demonstration purposes, it goes to show how you can leverage off-the shelf solutions (Connectors, Transformers, Schema Registry etc.) and focus on the heavy lifting that your Azure Cosmos DB based application or data-pipeline requires. Since this example adopts a Docker based approach for local development, it is cost-effective (well, free!) and can be easily customised to suit your requirements.\nFor production scenarios, you would need to setup, configure and operate these connectors. Kafka Connect workers are simply JVM processes, thus inherently stateless (all the state handling is offloaded to Kafka). There is a lot of flexibility in terms of your overall architecture as well as orchestration — for instance, you could run them in Kubernetes for fault-tolerance and scalability.","configure-azure-cosmos-db-emulator-certificates#Configure Azure Cosmos DB Emulator Certificates":"Execute this command to store the certificate in the Java certificate store (using docker exec):\ndocker exec --user root -it cosmosdb-kafka-docker_cosmosdb-connector_1 /bin/bash # execute the below command inside the container curl -k https://cosmosdb:8081/_explorer/emulator.pem \u003e ~/emulatorcert.crt \u0026\u0026 keytool -noprompt -storepass changeit -keypass changeit -keystore /usr/lib/jvm/zulu11-ca/lib/security/cacerts -importcert -alias emulator_cert -file ~/emulatorcert.crt You should see this output — Certificate was added to keystore\nAnd, one last thing before we proceed…","create-azure-cosmos-db-database-and-containers#Create Azure Cosmos DB database and containers":"Access the Azure Cosmos DB emulator portal at https://localhost:8081/_explorer/index.html and create the below resources:\nDatabase named testdb\nContainers — inventory, orders, orders_avro (ensure that the partition key for all the containers is /id)","first-things-first#First things first…":"… here is a quick overview of the the Azure Cosmos DB Emulator and the Kafka Connector.\nThe Azure Cosmos DB connector allows you to move data between Azure Cosmos DB and Kafka. It’s available as a source as well as a sink. The Azure Cosmos DB Sink connector writes data from a Kafka topic to an Azure Cosmos DB container and the Source connector writes changes from an Azure Cosmos DB container to a Kafka topic. At the time of writing, the connector is in pre-production mode. You can read more about it on the GitHub repo or install/download it from the Confluent Hub.\nThe Azure Cosmos DB Linux Emulator provides a local environment that emulates the Azure Cosmos DB service for development purposes (currently, it only supports SQL API). It provides a high-fidelity emulation of the Azure Cosmos DB service and supports functionality such as creating data, querying data, provisioning and scaling containers, and executing stored procedures and triggers.\nAt the time of writing, the Azure Cosmos DB Linux Emulator is in preview.\nYou can dive into how to use the emulator on macOS or Linux, how is it different from the Azure Cosmos DB cloud service, troubleshoot issues, etc.","lets-explore-all-the-scenarios#Let’s explore all the scenarios":"To start off let’s look at the basic scenario. Before trying out other things, we want to make sure everything is functional.\n1. Hello world! Start the inventory data connector for Cosmos DB:\ncurl -X POST -H \"Content-Type: application/json\" -d @cosmosdb-inventory-connector_1.json http://localhost:8083/connectors # to check the connector status curl http://localhost:8083/connectors/inventory-sink/status To test the end to end flow, send a few records to the inventory_topic topic in Kafka:\ndocker exec -it kafka bash -c 'cd /usr/bin \u0026\u0026 kafka-console-producer --topic inventory_topic --bootstrap-server kafka:29092' Once the prompt is ready, send the JSON records one by one:\n{\"id\": \"5000\",\"quantity\": 100,\"productid\": 42} {\"id\": \"5001\",\"quantity\": 99,\"productid\": 43} {\"id\": \"5002\",\"quantity\": 98,\"productid\": 44} Check Cosmos DB container to confirm if records have been saved. Navigate to the the portal https://localhost:8081/_explorer/index.html and check the inventory container:\nOk, it worked! Let’s go ahead and do something slightly more useful. Before moving on, delete the inventory connector.\ncurl -X DELETE http://localhost:8083/connectors/inventory-sink/ 2. Sync streaming data (JSON format) from Kafka to Azure Cosmos DB For the remaining scenarios, we will use a producer component to generate records. The Kafka Connect Datagen connector is our friend. It is meant for generating mock data for testing, so let’s put it to good use!\nStart an instance of the Azure Cosmos DB connector:\ncurl -X POST -H \"Content-Type: application/json\" -d @cosmosdb-inventory-connector_2.json http://localhost:8083/connectors # to check the connector status curl http://localhost:8083/connectors/inventory-sink/status Once it’s ready, go ahead and start the Datagen connector which will generate mock inventory data in JSON format:\ncurl -X POST -H \"Content-Type: application/json\" -d @datagen-inventory-connector.json http://localhost:8080/connectors # to check the connector status curl http://localhost:8080/connectors/datagen-inventory/status Note that we use port 8080 for the Datagen connector since it’s running in a separate Kafka Connect container\nTo look at the data produced by the Datagen connector, check the inventory_topic1 Kafka topic:\ndocker exec -it kafka bash -c 'cd /usr/bin \u0026\u0026 kafka-console-consumer --topic inventory_topic1 --bootstrap-server kafka:29092' Notice the data (it maybe different in your case):\n{\"id\":5,\"quantity\":5,\"productid\":5} {\"id\":6,\"quantity\":6,\"productid\":6} {\"id\":7,\"quantity\":7,\"productid\":7} ... Note that id has a Integer value\nCheck Azure Cosmos DB containers to confirm if records have been saved. Navigate to the the portal https://localhost:8081/_explorer/index.html and check the inventory container:\nThe records in Cosmos DB have an id attribute of String data type. The original data in the Kafka topic had an id attribute of Integer type – but that would not have worked, since Azure Cosmos DB requires id to be a unique user-defined string. This conversion was made possible by a Kafka Connect transform – Cast updates fields (or the entire key or value) to a specific type, updating the schema if one is present.\nHere is the part in the connector configuration which did the trick:\n\"transforms\": \"Cast\", \"transforms.Cast.type\": \"org.apache.kafka.connect.transforms.Cast$Value\", \"transforms.Cast.spec\": \"id:string\" Before moving on, delete the Cosmos DB and Datagen inventory connectors.\ncurl -X DELETE http://localhost:8080/connectors/datagen-inventory curl -X DELETE http://localhost:8083/connectors/inventory-sink/ 3. Push streaming Orders data (JSON format) from Kafka to Azure Cosmos DB Now, let’s switch gears and use the same data (JSON formatted) data, but with a slight twist. We will use a variant of the Datagen connector to generate mock orders data and adjust the Cosmos DB connector as well.\nTo install a different instance of the Azure Cosmos DB connector:\ncurl -X POST -H \"Content-Type: application/json\" -d @cosmosdb-orders-connector_1.json http://localhost:8083/connectors # to check the connector status curl http://localhost:8083/connectors/orders-sink/status Install the Datagen orders connector:\ncurl -X POST -H \"Content-Type: application/json\" -d @datagen-orders-connector.json http://localhost:8080/connectors # to check the connector status curl http://localhost:8080/connectors/datagen-orders/status To look at the data produced by the Datagen connector, check the orders Kafka topic:\ndocker exec -it kafka bash -c 'cd /usr/bin \u0026\u0026 kafka-console-consumer --topic orders_topic --bootstrap-server kafka:29092' Notice the data (it maybe different in your case):\n{\"ordertime\":1496251410176,\"orderid\":3,\"itemid\":\"Item_869\",\"orderunits\":3.2897805449886226,\"address\":{\"city\":\"City_99\",\"state\":\"State_46\",\"zipcode\":50570}} {\"ordertime\":1500129505219,\"orderid\":4,\"itemid\":\"Item_339\",\"orderunits\":3.6719921257659918,\"address\":{\"city\":\"City_84\",\"state\":\"State_55\",\"zipcode\":88573}} {\"ordertime\":1498873571020,\"orderid\":5,\"itemid\":\"Item_922\",\"orderunits\":8.4506812669258,\"address\":{\"city\":\"City_48\",\"state\":\"State_66\",\"zipcode\":55218}} {\"ordertime\":1513855504436,\"orderid\":6,\"itemid\":\"Item_545\",\"orderunits\":7.82561522361042,\"address\":{\"city\":\"City_44\",\"state\":\"State_71\",\"zipcode\":87868}} ... I chose Orders data on purpose since it is different compared to the Inventory data. Notice that JSON record produced by the Datagen connector has a orderid attribute (Integer data type), but no id attribute – but we know that Azure Cosmos DB won’t work without one.\nCheck Cosmos DB containers to confirm if records have been saved. Navigate to the the portal https://localhost:8081/_explorer/index.html and check the orders container:\nNotice that there is no orderid attribute in the records stored in Azure Cosmos DB. In fact, it has been replaced by the id attribute (with a String value). This achieved by the ReplaceField transformer.\nHere is the part in the connector configuration which made this possible:\n\"transforms\": \"RenameField,Cast\", \"transforms.RenameField.type\": \"org.apache.kafka.connect.transforms.ReplaceField$Value\", \"transforms.RenameField.renames\": \"orderid:id\", \"transforms.Cast.type\": \"org.apache.kafka.connect.transforms.Cast$Value\", \"transforms.Cast.spec\": \"id:string\" Depending on your use case, removing/renaming a field altogether may not be an ideal solution. However, it’s good to know that there are options. Also, remember that the original data in Kafka topics is still there, untouched. Other downstream apps can still leverage it.\nBefore moving on, delete the Cosmos DB and Datagen inventory connectors.\ncurl -X DELETE http://localhost:8080/connectors/datagen-orders curl -X DELETE http://localhost:8083/connectors/orders-sink/ 4. Push streaming Orders data (AVRO format) from Kafka to Azure Cosmos DB So far we dealt with JSON, a commonly used data format. But, Avro is heavily used in production due to its compact format which leads to better performance and cost savings. To make it easier to deal with Avro data schema, there is Confluent Schema Registry which provides a serving layer for your metadata along with a RESTful interface for storing and retrieving your Avro (as well as JSON and Protobuf schemas). We will use the Docker version for the purposes of this blog post.\nInstall a new instance of the Azure Cosmos DB connector that can handle Avro data:\ncurl -X POST -H \"Content-Type: application/json\" -d @cosmosdb-orders-connector_2.json http://localhost:8083/connectors # to check the connector status curl http://localhost:8083/connectors/orders-sink/status Install Datagen connector which will generate mock orders data in Avro format:\ncurl -X POST -H \"Content-Type: application/json\" -d @datagen-orders-connector-avro.json http://localhost:8080/connectors # to check the connector status curl http://localhost:8080/connectors/datagen-orders/status To look at the Avro data produced by the Datagen connector, check the orders_avro_topic Kafka topic:\ndocker exec -it kafka bash -c 'cd /usr/bin \u0026\u0026 kafka-console-consumer --topic orders_avro_topic --bootstrap-server kafka:29092' Since the Avro data in binary format, it’s not human readable:\n�����VItem_185lqf�@City_61State_73�� ����WItem_219[�C��@City_74State_77�� �����VItem_7167Ix�dF�?City_53State_53�� ���֩WItem_126*���?@City_58State_21�� �����VItem_329X�2,@City_49State_79�� �����XItem_886��\u003e�|�@City_88State_27�� ��V Item_956�r#�!@City_45State_96�� �ѼҕW\"Item_157E�)$���?City_96State_63�� ... Check Cosmos DB containers to confirm if records have been saved. Navigate to the the portal https://localhost:8081/_explorer/index.html and check the orders_avro container:\nGreat, things work as expected! The connector configuration was updated to handle this:\n\"value.converter\": \"io.confluent.connect.avro.AvroConverter\", \"value.converter.schemas.enable\": \"true\", \"value.converter.schema.registry.url\": \"http://schema-registry:8081\", ... The changes include choosing the AvroConverter, enabling schemas and pointing to Schema Registry (in our case running locally in Docker).\nThat is all for the use cases covered in this blog post. We only covered the Sink connector, but feel free to explore and experiment further! For example, you could extend the current setup to include the Source connector and configure it to send records from Azure Cosmos DB containers to Kafka.","start-all-the-services#Start all the services":"All the components are defined in the docker-compose file:\nAzure Cosmos DB emulator\nKafka and Zookeeper\nAzure Cosmos DB and Datagen connectors (run as separate Kafka Connect workers)\nConfluent Schema Registry\nThanks to Docker Compose, the environment can be brought up with a single command. When you run this for this the first time, it may take a while for the containers to be downloaded (subsequent executions are faster). Optionally, you can also download the images separately before starting Docker Compose:\n(optional) docker pull confluentinc/cp-zookeeper:latest docker pull confluentinc/cp-kafka:latest docker pull confluentinc/cp-schema-registry:latest To start all the services:\ndocker-compose -p cosmosdb-kafka-docker up --build After a couple of minutes, check the containers:\ndocker-compose -p cosmosdb-kafka-docker ps Once all the services as up and running, the next logical step is to install the connector, right? Well, there a couple of things we need to take care of. For Java apps to connect to the Azure Cosmos DB emulator, you need to have certificates installed in the Java certificate store. In this case, we will seed the certificate from the Azure Cosmos DB emulator container to the Cosmos DB Kafka Connect container.\nAlthough this process can be automated, I am doing it manually to make things clear."},"title":"Getting started with Kafka Connector for Azure Cosmos DB using Docker"},"/blog/cosmosdb-emulator-devcontainer/":{"data":{"":"","#":"This blog post provides a quick overview and demo of how you can use the Azure Cosmos DB Linux Emulator on Docker (in preview at the time of writing) along with Visual Studio Code in order to enhance your local development experience.\nSince the Azure Cosmos DB Linux Emulator is readily available as a Docker image (simply docker pull mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator), it’s easy to incorporate it within your existing setup. For instance, it could be in a docker-compose file as a part of a larger stack (here is an example of how to use it with Apache Kafka). However, complementing it with the Visual Studio Code Remote - Containers extension, gives you the ability to leverage a Docker container as a full-fledged development environment.\nSay you want to build an app with Azure Cosmos DB Core (SQL) API and the Java SDK, and you have VS Code and Docker (optionally Docker Compose) available. Just create a JSON configuration file (called devcontainer.json) to define your stack. You can then spin up an environment on-demand with just a few clicks - this will constitute one or more Docker containers along with the entire operating system, programming language runtime (Java in this case), toolset, etc. which you originally specified in devcontainer.json.\nSome of the obvious benefits of using the emulator include: it’s cost-effective (free!), convenient for scenarios when your app has other components (e.g. messaging systems) and great for iterative development/prototyping/demos since setting up and tearing down the environment is easy, consistent and error-free (in most cases!) How to use the Azure Cosmos DB Linux emulator with VS Code Use the instructions in this GitHub repo to get started quickly. It is based on the original sample code repo for Azure Cosmos DB Java SDK for SQL API with added files to enable “Visual Studio Code Remote - Containers” experience.\nYou can also follow along with the video that shows all of this in action:\nThis example can be extended to any Java application. All you need to do is add the .devcontainer folder to your current project and (maybe) tweak a few things depending on your requirement for e.g. the Java version.\nWhat this demonstrates is just the tip of the iceberg and you could leverage it in so many interesting ways. I look forward to hearing how you use it!\nMore resources Find more ways to dev/test free with Azure Cosmos DB Download the Azure Cosmos DB local emulator Review technical documentation for Azure Cosmos DB "},"title":"Enhance local development experience using the Azure Cosmos DB Linux emulator and VS Code"},"/blog/cosmosdb-fault-tolerant-apps/":{"data":{"":"Azure Cosmos DB is a resource governed system that allows you to execute a certain number of operations per second based on the provisioned throughput you have configured. If clients exceed that limit and consume more request units than what was provisioned, it leads to rate limiting of subsequent requests and exceptions being thrown – they are also referred to as 429 errors.\nWith the help of a practical example, I’ll demonstrate how to incorporate fault-tolerance in your Go applications by handling and retrying operations affected by these rate limiting errors. To help you follow along, the sample application code for this blog is available on GitHub - it uses the gocql driver for Apache Cassandra.\nIn this post, we’ll go through:\nInitial setup and configuration before running the sample application Execution of various load test scenarios and analyze the results A quick overview of the Retry Policy implementation. One way of tackling rate limiting is by adjusting provisioned throughput to meet your application requirements. There are multiple ways to do this, including using Azure portal, Azure CLI, and CQL (Cassandra Query Language) commands.\nBut, what if you wanted to handle these errors in the application itself?\nThe good thing is that the Cassandra API for Azure Cosmos DB translates the rate limiting exceptions into overloaded errors on the Cassandra native protocol. Since the gocql driver allows you to plugin your own RetryPolicy, you can write a custom implementation to intercept these errors and retry them after a certain (cool down) time period. This policy can then be applied to each Query or at a global level using a ClusterConfig.\nThe Azure Cosmos DB extension library makes it quite easy to use Retry Policies in your Java applications. An equivalent Go version is available on GitHub and has been used in the sample application for this blog post.","behind-the-scenes#Behind the scenes":"CosmosRetryPolicy adheres to the gocql.RetryPolicy interface by implementing the Attempt and GetRetry functions.\ntype CosmosRetryPolicy struct { MaxRetryCount int FixedBackOffTimeMs int GrowingBackOffTimeMs int numAttempts int } Retry is initiated only if the number of retry attempts for that query are less than or equal to max retry config or max retry config is set to -1 (infinite retries)\nfunc (crp *CosmosRetryPolicy) Attempt(rq gocql.RetryableQuery) bool { crp.numAttempts = rq.Attempts() return rq.Attempts() \u003c= crp.MaxRetryCount || crp.MaxRetryCount == -1 } GetRetryType function detects the type of error and in the case or a rate-limited error (HTTP 429), it tries to extract the value for RetryAfterMs field (from the error message) and uses that to sleep before retrying the query.\nfunc (crp *CosmosRetryPolicy) GetRetryType(err error) gocql.RetryType { switch err.(type) { default: retryAfterMs := crp.getRetryAfterMs(err.Error()) if retryAfterMs == -1 { return gocql.Rethrow } time.Sleep(retryAfterMs) return gocql.Retry //other case statements have been omitted for brevity } Azure Cosmos DB provides you the flexibility to not only configure and adjust your throughput requirements using a variety of ways but also provides the basic primitive that allows applications to handle rate limiting errors, thereby making them robust and fault-tolerant. This blog post demonstrated how you can do this for Go applications, but the concepts are applicable to any language and its respective CQL compatible driver that you choose for working with the Cassandra API for Azure Cosmos DB.","retry-policy-in-action#Retry Policy in action":"As promised, you will walk through the entire process using a simple yet practical example. The sample application used to demonstrate the concepts is a service that exposes a REST endpoint to POST orders data which is persisted to a Cassandra table in Azure Cosmos DB.\nYou will run a few load tests on this API service to see how rate limiting manifests itself and how it’s handled.\nPre-requisites Start by installing hey, a load testing program. You can download OS specific binaries (64-bit) for Linux, Mac and Windows (please refer to the GitHub repo for latest information in case you face issues downloading the utility)\nYou can use any other tool that allows you to generate load on an HTTP endpoint\nClone this GitHub repo and change into the right directory:\ngit clone github.com/abhirockzz/cosmos-go-rate-limiting cd cosmos-go-rate-limiting Setup Azure Cosmos DB Create an Azure Cosmos DB account with the Cassandra API option selected\nTo create a Keyspace and Table, use the following CQL:\nCREATE KEYSPACE ordersapp WITH REPLICATION = {'class' : 'SimpleStrategy'}; CREATE TABLE ordersapp.orders ( id uuid PRIMARY KEY, amount int, state text, time timestamp ); Start the application Open a terminal and set the environment variables for the application:\nexport COSMOSDB_CASSANDRA_CONTACT_POINT=.cassandra.cosmos.azure.com export COSMOSDB_CASSANDRA_PORT=10350 export COSMOSDB_CASSANDRA_USER= export COSMOSDB_CASSANDRA_PASSWORD= #optional (default: 5) #export MAX_RETRIES= To start the application:\ngo run main.go //wait for this output Connected to Azure Cosmos DB To test whether the application is working as expected, insert a few orders by invoking the REST endpoint (once for each order) from a different terminal:\ncurl http://localhost:8080/orders The application generates random data so you don’t have to enter it while invoking the endpoint\nConfirm that the order was successfully stored. You can use the hosted CQL shell in the Azure portal and execute the below query:\nselect count(*) from ordersapp.orders; // you should see this output system.count(*) ----------------- 1 (1 rows) You’re all set.\nLet the load tests begin! Invoke the REST endpoint with 300 requests. This is enough to overload the system since you only have 400 RU/s allocated by default.\nTo start the load test:\nhey -t 0 -n 300 http://localhost:8080/orders Notice the logs in the application terminal. In the beginning, you will see that the orders are being successfully created. For example:\nAdded order ID 25a8cec1-e67a-11ea-9c17-7f242c2eeac0 Added order ID 25a8f5ef-e67a-11ea-9c17-7f242c2eeac0 Added order ID 25a8f5ea-e67a-11ea-9c17-7f242c2eeac0 ... After a while, as the throughput degrades and eventually exceeds the provisioned limit, Azure Cosmos DB will rate limit the application requests. This will manifest itself in the form of an error which looks similar to this:\nRequest rate is large: ActivityID=ac78fac3-5c36-4a20-8ad7-4b2d0768ffe4, RetryAfterMs=112, Additional details='Response status code does not indicate success: TooManyRequests (429); Substatus: 3200; ActivityId: ac78fac3-5c36-4a20-8ad7-4b2d0768ffe4; Reason: ({ \"Errors\": [ \"Request rate is large. More Request Units may be needed, so no changes were made. Please retry this request later. Learn more: http://aka.ms/cosmosdb-error-429\" ] }); In the error message above, notice the following: TooManyRequests (429) and RetryAfterMs=112\nObserving Query errors\nTo keep things simple, we will use the log output for testing/diagnostic purposes. Any error (related to rate–limiting in this case) encountered during query execution is intercepted by a gocql.QueryObserver. The randomly generated order ID is also logged with each error message so that you can check the logs to confirm if the failed order has been re–tried and (eventually) stored in Azure Cosmos DB.\nHere is the code snippet:\n.... type OrderInsertErrorLogger struct { orderID string } // implements gocql.QueryObserver func (l OrderInsertErrorLogger) ObserveQuery(ctx context.Context, oq gocql.ObservedQuery) { err := oq.Err if err != nil { log.Printf(\"Query error for order ID %sn%v\", l.orderID, err) } } .... // the Observer is associated with each query rid, _ := uuid.GenerateUUID() err := cs.Query(insertQuery).Bind(rid, rand.Intn(200)+50, fixedLocation, time.Now()).Observer(OrderInsertErrorLogger{orderID: rid}).Exec() How many orders made it through?\nSwitch back to the load testing terminal and check some of the statistics (output has been redacted for brevity)\nSummary: Total: 2.8507 secs Slowest: 1.3437 secs Fastest: 0.2428 secs Average: 0.5389 secs Requests/sec: 70.1592 .... Status code distribution: [200] 300 responses The numbers will differ in your specific case depending on multiple factors.\nThis is not a raw benchmarking test and neither do we have a production grade application, so you can ignore the Requests/sec etc. But draw our attention to the Status code distribution attribute which shows that our application responded with a HTTP 200 for all the requests.\nLet’s confirm the final numbers. Open the Cassandra Shell in the Azure Cosmos DB portal and execute the same query:\nselect count(*) from ordersapp.orders; //output system.count(*) ----------------- 301 You should see 300 additional rows (orders) have been inserted. The key takeaway is that all the orders were successfully stored in Azure Cosmos DB de–spite the rate limiting errors because our application code transparently retried them based on the Retry Policy that we configured (with a single line of code!)\nclusterConfig.RetryPolicy = retry.NewCosmosRetryPolicy(numRetries) A note on dynamic throughput management If your application spends most of its time operating at about 60-70% of it’s throughput, using Autoscale provisioned throughput can help optimize your RU/s and cost usage by scaling down when not in use – you only pay for the resources that your workloads need on a per-hour basis.\nSo, what happens without the Retry Policy?\nDeactivate the policy to see the difference Stop the application (press control+c in the terminal), set an environment variable and re-start the application:\nexport USE_RETRY_POLICY=false go run main.go Before running the load test again, make a note of the number of rows in the orders table using select count(*) from ordersapp.orders;\nhey -t 0 -n 300 http://localhost:8080/orders In the application logs, you will notice the same rate limiting errors. In the terminal where you ran the load test, at the end of the output summary, you will see that some the requests failed to complete successfully i.e. they returned a response other than HTTP 200\n... Status code distribution: [200] 240 responses [429] 60 responses Because the Retry Policy was not enforced, the application no longer re–tried the requests that failed due to rate-limiting.\nIncrease provisioned throughput You can increase the Request Units using the Azure Portal (for example, double it to 800 RU/s) and run the same load test\nhey -t 0 -n 300 http://localhost:8080/orders You will not see the rate limiting (HTTP ``429) errors now and relatively low numbers for latency, requests per second etc.\nTry increasing the number of requests (use the -n flag) to see when the throughput threshold is breached for the application to get rate limited. As expected, all the orders will be persisted successfully (without any errors or retries)\nThe next section briefly covers how the custom Retry Policy works.\nThis is an experimental implementation, and you should write custom policies to suit fault-tolerance and performance requirements of your applications.","to-learn-more#To learn more:":"Check out some of these resources from the official documentation:\nUse cases and benefits of Autoscale provisioned throughput Details of the Cassandra API support in Azure Cosmos DB Get up and running with a Go application and Cassandra API for Azure Cosmos DB Frequently asked questions about the Cassandra API in Azure Cosmos DB Request Units concepts "},"title":"Build fault tolerant applications with Cassandra API for Azure Cosmos DB"},"/blog/cosmosdb-partial-doc-update/":{"data":{"":"","benefits-of-partial-document-update#Benefits of partial document update":"","document-updates--then-and-now#Document updates – then and now":"","learn-more#Learn more":"Originally posted on https://devblogs.microsoft.com/cosmosdb/partial-document-update-ga/\nWe’re excited to announce the general availability of partial document update for the Azure Cosmos DB Core (SQL) API, which was announced at Microsoft Ignite! This new feature makes it possible to perform path-level updates to specific fields/properties in a single document without needing to perform a full document read-replace operation. Partial document update is currently supported in Azure Cosmos DB .NET SDK, Java SDK, Node SDK, and stored procedures.\nDocument updates – then and now To put things in context, here is a refresher on how one would typically use a document replace operation: Read the document, update it locally (client side) including any optimistic concurrency control (OCC) checks if necessary and, finally call the replace operation along with the updated document.\nHere is a trimmed down example of how one would use partial document update (using the Java SDK):\n//step 1 UserInfo user = container.readItem(user.getId(), new PartitionKey(user.getEmail()), UserInfo.class); //step 2 CosmosPatchOperations patchOps = CosmosPatchOperations.create().add(\"/phone/2\",12345).set(\"/address\",\"123 Foobar\"); //step 3 container.patchItem(user.getId(), new PartitionKey(user.getEmail()), patchOps, reqOps, UserInfo.class); Read the document (this is the same as replace) with readItem method. Define the updates you want to make (in form of a CosmosPatchOperations object – in this case we add a phone number (an array) and set the address to a different one. Invoke the patchItem method. Benefits of partial document update When compared with the replace operation, the overhead of serializing the entire document is eliminated since the client application only needs to deal with properties to be updated (phone and address in this example). This becomes significant when your application makes frequent updates to only a few properties in your documents, such as incrementing counters, toggling true/false flags or similar types of changes.\nNot having to send the entire document over the wire impacts overall application performance – reduced network bandwidth usage, lower end-to end-latency and savings of CPU cycles on the Azure Cosmos DB SDK client hosts.\nSome of the other developer productivity benefits that are a by-product of the flexible programming model enabled by partial document update:\nCombining multiple operations: Partial document update supports different types of operations (see next section). Depending on your requirements, you can combine multiple such operations as part of a single invocation as opposed to incurring cost of round trips for separate operation type. Conditional update: If you want the partial update to depend on a pre-condition, you can define it using a SQL-like filter predicate (for example, from c where c.taskNum = 3). The partial update operation will fail if the pre-condition specified in the predicate is not satisfied. Transactions support: Partial document update works in the context of a Transactional batch as well. This means that multiple documents within the same partition key can be patched (partially updated) as part of a transaction (along with other operations such as create). The transaction will be committed only if all the operations succeed. If any operation fails, the entire transaction is rolled back. Transparent conflict resolution: If your Azure Cosmos DB account is configured with multiple write regions, conflicts and respective resolution policies are applicable at the document level (with Last Write Wins being the default conflict resolution policy). This works differently in case of partial document updates – conflicts that occur due to concurrent patch operations to the same document across multiple regions will be detected and resolved at the path-level. This means that as long as you’re updating different properties (paths) in the same document, they will be merged successfully. Partial document update operations Although Partial document update is a top-level operation (just like Replace), it supports sub-operations (the code snippet above used add and set). You can refer to the documentation for details, but here is a summary:\nAdd: Creates a new element (if it does not already exist). Set: Updates an element (creates one if it does not already exist). Replace: Updates an element only if it already exists. Remove: Deletes an existing element. Increment: Increases/decreases by specified value (use negative value to decrease). Learn more Find out more about Azure Cosmos DB partial document update:\nRead the concepts Code samples to get you started Add/Set/Replace operations – how are they similar, yet different Frequently asked questions Get started free with Azure Cosmos DB.","partial-document-update-operations#Partial document update operations":""},"title":"Now Generally Available – Partial Document Update in Azure Cosmos DB"},"/blog/create-data/":{"data":{"":"","in-god-we-trust-all-others-must-bring-data#\u0026ldquo;In God we trust, all others must bring data\u0026rdquo;":"“In God we trust, all others must bring data” William Edwards Deming\nWell, Microsoft is bringing to you, Data Week 🙌 A celebration of Data \u0026 Data Technologies, running throughout the week, starting December 7, 2020!\nIt kicks off with Create: Data, a completely FREE online event. Register at https://aka.ms/createdata ! Been wondering which database to pick for your next project/product? Tim Berglund (who by the way, I admire a lot!) from Confluent will be joining me in a conversation about “Picking the Right Distributed Database”. Databases are a critical part of any business. But, how do you pick the right one given the multitude of options at your disposal and new ones coming up quite frequently. Should you stick to good-old RDBMS, opt for NoSQL variants, or go multi-model? Is there really a right answer?\nWait, that’s not all! Check out what’s in store for the entire week in this post by Adi Polak which covers the complete agenda!\nWant to keep track of these events and conversations? Follow @MicrosoftCreate on Twitter!"},"title":"Picking the Right Distributed Database"},"/blog/deploy-go-apps-apprunner/":{"data":{"":"…With the managed runtime for Go\nIn this blog post you will learn how to run a Go application to AWS App Runner using the Go platform runtime. You will start with an existing Go application on GitHub and deploy it to AWS App Runner. The application is based on the URL shortener application (with some changes) that persists data in DynamoDB.\nWhat’s covered?\nPrerequisites Create a GitHub repo for the URL shortener application Create a DynamoDB table to store URL information Create an IAM role with DynamoDB specific permissions Deploy the application to AWS App Runner Verify URL shortener functionality Clean up Conclusion AWS App Runner can create and manage services based on two types of service sources:\nSource code (covered in this blog post) Source image Source code is nothing but your application code that App Runner will build and deploy. All you need to do is point App Runner to a source code repository and choose a suitable runtime that corresponds to a programming platform version. App Runner provides platform-specific managed runtimes (for Python, Node.js, Java, Go etc.).\nThe AWS App Runner Go platform runtime makes it easy to build and run containers with web applications based on a Go version. You don’t need to provide container configuration and build instructions such as a Dockerfile. When you use a Go runtime, App Runner starts with a managed Go runtime image which is based on the Amazon Linux Docker image and contains the runtime package for a version of Go and some tools. App Runner uses this managed runtime image as a base image, and adds your application code to build a Docker image. It then deploys this image to run your web service in a container.","clean-up#Clean up":"Once you complete this tutorial, don’t forget to delete the following:\nDynamoDB table App Runner service ","conclusion#Conclusion":"In this blog post, you learned how to go from a Go application in your GitHub repository to a complete URL shortener service deployed to AWS App Runner!","create-a-dynamodb-table-to-store-url-information#Create a DynamoDB table to store URL information":"Create a table named urls. Choose the following:\nPartition key named shortcode (data type String) On-Demand capacity mode ","create-a-github-repo-for-the-url-shortener-application#Create a GitHub repo for the URL shortener application":"Clone this GitHub repo and then upload it to a GitHub repository in your account (keep the same repo name i.e. apprunner-go-runtime-app)\ngit clone https://github.com/abhirockzz/apprunner-go-runtime-app ","create-an-iam-role-with-dynamodb-specific-permissions#Create an IAM role with DynamoDB specific permissions":" export IAM_ROLE_NAME=apprunner-dynamodb-role aws iam create-role --role-name $IAM_ROLE_NAME --assume-role-policy-document file://apprunner-trust-policy.json Before creating the policy, update the dynamodb-access-policy.json file to reflect the DynamoDB table ARN name.\naws iam put-role-policy --role-name $IAM_ROLE_NAME --policy-name dynamodb-crud-policy --policy-document file://dynamodb-access-policy.json ","deploy-the-application-to-aws-app-runner#Deploy the application to AWS App Runner":" If you have an existing AWS App Runner GitHub connection and want to use that, skip to the Repository selection step.\nCreate AWS App Runner GitHub connection\nOpen the App Runner console and Choose Create service.\nOn the Source and deployment page, in the Source section, for Repository type, choose Source code repository. Under Connect to GitHub choose Add new, and then, if prompted, provide your GitHub credentials.\nIn the Install AWS Connector for GitHub dialog box, if prompted, choose your GitHub account name. If prompted to authorize the AWS Connector for GitHub, choose Authorize AWS Connections. Choose Install.\nYour account name appears as the selected GitHub account/organization. You can now choose a repository in your account.\nRepository selection\nFor Repository, choose the repository you created - apprunner-go-runtime-app. For Branch, choose the default branch name of your repository (for example, main).\nConfigure your deployment: In the Deployment settings section, choose Automatic, and then choose Next.\nConfigure application build\nOn the Configure build page, for Configuration file, choose Configure all settings here.\nProvide the following build settings:\nRuntime – Choose Go 1 Build command – Enter go build main.go Start command – Enter ./main Port – Enter 8080 Choose Next.\nConfigure your service.\nUnder Environment variables, add an environment variable. For Key, enter TABLE_NAME, and for Value, enter the name of the DynamoDB table (urls) that you created before.\nUnder Security \u003e Permissions, choose the IAM role that you had created earlier (apprunner-dynamodb-role).\nChoose Next.\nOn the Review and create page, verify all the details you’ve entered, and then choose Create and deploy. If the service is successfully created, the console shows the service dashboard, with a Service overview of the application.","prerequisites#Prerequisites":"You will need an AWS account and install AWS CLI.\nLet’s get started….","verify-url-shortener-functionality#Verify URL shortener functionality":"The application exposes two endpoints:\nTo create a short link for a URL Access the original URL via the short link First, export the App Runner service endpoint as an environment variable,\nexport APP_URL=\u003center App Runner service URL\u003e # example export APP_URL=https://jt6jjprtyi.us-east-1.awsapprunner.com Invoke it with a URL that you want to access via a short link.\ncurl -i -X POST -d 'https://abhirockzz.github.io/' $APP_URL # output HTTP/1.1 200 OK Date: Thu, 21 Jul 2022 11:03:40 GMT Content-Length: 25 Content-Type: text/plain; charset=utf-8 {\"ShortCode\":\"ae1e31a6\"} You should get a JSON response with a short code and see an item in the DynamoDB table as well.\nYou can continue to test the application with a few other URLs.\nTo access the URL associated with the short code\nEnter the following in your browser http://\u003center APP_URL\u003e/\u003cshortcode\u003e\nFor example, when you enter https://jt6jjprtyi.us-east-1.awsapprunner.com/ae1e31a6, you will be re-directed to the original URL.\nYou can also use curl. Here is an example:\nexport APP_URL=https://jt6jjprtyi.us-east-1.awsapprunner.com curl -i $APP_URL/ae1e31a6 # output HTTP/1.1 302 Found Location: https://abhirockzz.github.io/ Date: Thu, 21 Jul 2022 11:07:58 GMT Content-Length: 0 "},"title":"Deploying Go Applications to AWS App Runner: A Step-by-Step Guide"},"/blog/dynamodb-go-sdk-scan-batch-ops/":{"data":{"":"","#":"The DynamoDB Scan API accesses every items in a table (or secondary index). It is the equivalent of a select * from query. One of the things I will cover in this blog is how to use Scan API with the DynamoDB Go SDK.\nTo scan a table, we need some data to begin with! So in the process, I will also go into how to use the Batch API to write bulk data in DynamoDB. You can use the BatchWriteItem API to create or delete items in batches (of twenty five) and it’s possible to you can combine these operations across multiple tables.\nWe will start simple and gradually improve our approach to use the APIs efficiently. I will also go over some of the basic tests that I ran to demonstrate incremental improvements. Finally I will wrap up by highlighting some of the considerations while using these operations.\nYou can refer to the code on GitHub\nBefore you proceed… … make sure to create a DynamoDB table called users with:\npartition key email (data type String) and On-Demand capacity mode. Also, there are a few things I want to call a few things to set the context:\nThe table was created in us-east-1 and tests were executed from an EC2 instance in us-east-1 as well Since these are general tests instead of specialised benchmarks, I did not do any special tuning (at any level). These are just Go functions that were executed with different inputs, keeping things as simple as possible. The tests include marshalling (converting Go struct to DynamoDB data types) for BatchWriteItem operations and un-marshalling (converting from DynamoDB data types back to Go struct) for Scan operation. Lets start off by exploring the BatchWriteItem API. This way we will have data to work with the Scan operations as well.\nWin-win!\nImporting data in batches Since you can combine 25 items in a single invocation, using a batch approach for bulk data imports is much better compared to invoking the PutItem in a loop (or even in parallel).\nHere is a basic example of how you would use BatchWriteItem:\nfunc basicBatchImport() { startTime := time.Now() cities := []string{\"NJ\", \"NY\", \"ohio\"} batch := make(map[string][]types.WriteRequest) var requests []types.WriteRequest for i := 1; i \u003c= 25; i++ { user := User{Email: uuid.NewString() + \"@foo.com\", Age: rand.Intn(49) + 1, City: cities[rand.Intn(len(cities))]} item, _ := attributevalue.MarshalMap(user) requests = append(requests, types.WriteRequest{PutRequest: \u0026types.PutRequest{Item: item}}) } batch[table] = requests op, err := client.BatchWriteItem(context.Background(), \u0026dynamodb.BatchWriteItemInput{ RequestItems: batch, }) if err != nil { log.Fatal(\"batch write error\", err) } else { log.Println(\"batch insert done\") } if len(op.UnprocessedItems) != 0 { log.Println(\"there were\", len(op.UnprocessedItems), \"unprocessed records\") } log.Println(\"inserted\", (25 - len(op.UnprocessedItems)), \"records in\", time.Since(startTime).Seconds(), \"seconds\") } With BatchWriteItemInput, we can define the operations we want to perform in the batch - here we are just going to perform PutRequests (which is encapsulated within another type called WriteRequest).\nWe assemble the WriteRequests in a slice and finally put them in a map with key being the table name - this is exactly what the RequestItems attribute in BatchWriteItemInput needs.\nIn this case we are dealing with a single table but you could execute operations on multiple tables.\nIn this example we just dealt with one batch of 25 records (maximum permitted batch size). If we want to import more records, all we need to do is split them into batches of 25 and execute them one (sub)batch at a time. Simple enough - here is an example:\nfunc basicBatchImport2(total int) { startTime := time.Now() cities := []string{\"NJ\", \"NY\", \"ohio\"} batchSize := 25 processed := total for num := 1; num \u003c= total; num = num + batchSize { batch := make(map[string][]types.WriteRequest) var requests []types.WriteRequest start := num end := num + 24 for i := start; i \u003c= end; i++ { user := User{Email: uuid.NewString() + \"@foo.com\", Age: rand.Intn(49) + 1, City: cities[rand.Intn(len(cities))]} item, _ := attributevalue.MarshalMap(user) requests = append(requests, types.WriteRequest{PutRequest: \u0026types.PutRequest{Item: item}}) } batch[table] = requests op, err := client.BatchWriteItem(context.Background(), \u0026dynamodb.BatchWriteItemInput{ RequestItems: batch, }) if err != nil { log.Fatal(\"batch write error\", err) } if len(op.UnprocessedItems) != 0 { processed = processed - len(op.UnprocessedItems) } } log.Println(\"all batches finished. inserted\", processed, \"records in\", time.Since(startTime).Seconds(), \"seconds\") if processed != total { log.Println(\"there were\", (total - processed), \"unprocessed records\") } } I tried this with 50000 records (which means 2000 batches) and it took approximately 15 seconds. But we can do much better!\nParallel batch import\nInstead of processing each batch sequentially, we can spin up a goroutine for each batch:\nfunc parallelBatchImport(numRecords int) { startTime := time.Now() cities := []string{\"NJ\", \"NY\", \"ohio\"} batchSize := 25 var wg sync.WaitGroup processed := numRecords for num := 1; num \u003c= numRecords; num = num + batchSize { start := num end := num + 24 wg.Add(1) go func(s, e int) { defer wg.Done() batch := make(map[string][]types.WriteRequest) var requests []types.WriteRequest for i := s; i \u003c= e; i++ { user := User{Email: uuid.NewString() + \"@foo.com\", Age: rand.Intn(49) + 1, City: cities[rand.Intn(len(cities))]} item, err := attributevalue.MarshalMap(user) if err != nil { log.Fatal(\"marshal map failed\", err) } requests = append(requests, types.WriteRequest{PutRequest: \u0026types.PutRequest{Item: item}}) } batch[table] = requests op, err := client.BatchWriteItem(context.Background(), \u0026dynamodb.BatchWriteItemInput{ RequestItems: batch, }) if err != nil { log.Fatal(\"batch write error\", err) } if len(op.UnprocessedItems) != 0 { processed = processed - len(op.UnprocessedItems) } }(start, end) } log.Println(\"waiting for all batches to finish....\") wg.Wait() log.Println(\"all batches finished. inserted\", processed, \"records in\", time.Since(startTime).Seconds(), \"seconds\") if processed != numRecords { log.Println(\"there were\", (numRecords - processed), \"unprocessed records\") } } The results improved by a good margin. Here is what I got. On an average:\nInserting 50000 records took ~ 2.5 seconds inserted 100000 records in ~ 4.5 to 5 seconds inserted 150000 records in less than 9.5 seconds inserted 200000 records in less than 11.5 seconds There maybe unprocessed records in a batch. This example detects these records, but the retry logic has been skipped to keep things simple. Ideally you should have a (exponential back-off based) retry mechanism for handling unprocessed records as well.\nTo insert more data, I ran the parallelBatchImport function (above) in loops. For example:\nfor i := 1; i \u003c= 100; i++ { parallelBatchImport(50000) } Alright, let’s move ahead. Now that we have some data, let’s try …\n… the Scan API This is what basic usage looks like:\nfunc scan() { startTime := time.Now() op, err := client.Scan(context.Background(), \u0026dynamodb.ScanInput{ TableName: aws.String(table), ReturnConsumedCapacity: types.ReturnConsumedCapacityTotal, }) if err != nil { log.Fatal(\"scan failed\", err) } for _, i := range op.Items { var u User err := attributevalue.UnmarshalMap(i, \u0026u) if err != nil { log.Fatal(\"unmarshal failed\", err) } } if op.LastEvaluatedKey != nil { log.Println(\"all items have not been scanned\") } log.Println(\"scanned\", op.ScannedCount, \"items in\", time.Since(startTime).Seconds(), \"seconds\") log.Println(\"consumed capacity\", *op.ConsumedCapacity.CapacityUnits) } Just provide the table (or secondary index) name and you are good to go! But, there are chances that you might not be able to get all items because of API limits (1 MB worth of data per invocation). In my case took about 0.5 secs for approximately 15000 records - rest of the items were skipped because the 1 MB limit was breached.\nUsing Pagination\nTo handle the limitation around data, the Scan API returns LastEvaluatedKey in its output to point to the last processed record. All you need to do is invoke Scan again, with the value for ExclusiveStartKey attribute set to the one for LastEvaluatedKey.\nUsing paginated scan approach took me approximately 100 secs to scan ~ 7.5 million records.\nParallel Scan\nPagination helps, but it’s still a sequential process. There is lot of scope for improvement. Thankfully, Scan allows you to adopt a parallelized approach i.e. you can use multiple workers (goroutines in this case) to process data in parallel!\nfunc parallelScan(pageSize, totalWorkers int) { log.Println(\"parallel scan with page size\", pageSize, \"and\", totalWorkers, \"goroutines\") startTime := time.Now() var total int var wg sync.WaitGroup wg.Add(totalWorkers) for i := 0; i \u003c totalWorkers; i++ { // start a goroutine for each segment go func(segId int) { var segTotal int defer wg.Done() lastEvaluatedKey := make(map[string]types.AttributeValue) scip := \u0026dynamodb.ScanInput{ TableName: aws.String(table), Limit: aws.Int32(int32(pageSize)), Segment: aws.Int32(int32(segId)), TotalSegments: aws.Int32(int32(totalWorkers)), } for { if len(lastEvaluatedKey) != 0 { scip.ExclusiveStartKey = lastEvaluatedKey } op, err := client.Scan(context.Background(), scip) if err != nil { log.Fatal(\"scan failed\", err) } segTotal = segTotal + int(op.Count) for _, i := range op.Items { var u User err := attributevalue.UnmarshalMap(i, \u0026u) if err != nil { log.Fatal(\"unmarshal failed\", err) } } if len(op.LastEvaluatedKey) == 0 { log.Println(\"[ segment\", segId, \"] finished\") total = total + segTotal log.Println(\"total records processsed by segment\", segId, \"=\", segTotal) return } lastEvaluatedKey = op.LastEvaluatedKey } }(i) } log.Println(\"waiting...\") wg.Wait() log.Println(\"done...\") log.Println(\"scanned\", total, \"items in\", time.Since(startTime).Seconds(), \"seconds\") } Segment and TotalSegments attributes are the key to how Scan API enables parallelism. TotalSegments is nothing but the number of threads/goroutines/worker-processes that need to be spawned and Segment is a unique identifier for each of them.\nIn my tests, the Scan performance remained (almost) constant at 37-40 seconds (average) for about ~ 7.5 million records (I tried a variety of page size and goroutine combinations).\nHow many TotalSegments do I need to configure???\nTo tune appropriate number of parallel threads/workers, you might need to experiment a bit. A lot might depend on your client environment.\nDo you have enough compute resources? Some environments/runtimes might have managed thread-pools, so you will have to comply with those So, you will need to try things out to find the optimum parallelism for your. one way to think about it could be to choose one segment (single worker/thread/goroutine) per unit of data (say a segment for every GB of data you want to scan).\nWrap up - API considerations Both Batch and Scan APIs are quite powerful, but there are nuances you should be aware of. My advise is to read up the API documentation thoroughly.\nWith Batch APIs:\nThere are certain limits: No more than 25 requests in a batch Individual item in a batch should not exceeds 400KB Total size of items in a single BatchWriteItem cannot be more than 16MB BatchWriteItem cannot update items You cannot specify conditions on individual put and delete requests It does not return deleted items in the response If there are failed operations, you can access them via the UnprocessedItems response parameter Use Scan wisely\nSince a Scan operation goes over the entire table (or secondary index), it’s highly likely that it consumes a large chunk of the provisioned throughput, especially if it’s a large table. That being said, Scan should be your last resort. Check whether Query API (or BatchGetItem) works for your use-case.\nThe same applies to parallel Scan.\nThere are a few ways in which you can further narrow down the results by using a Filter Expression, a Limit parameter (as demonstrated earlier) or a ProjectionExpression to return only a subset of attributes.\nThat’s all for this blog. I hope you found it useful.\nUntil next time, Happy coding!"},"title":"DynamoDB Go SDK: How to use the Scan and Batch operations efficiently"},"/blog/dynamodb-go-sdk-type-conversions/":{"data":{"":"","#":"Learn with practical code samples\nDynamoDB provides a rich set of data types including Strings, Numbers, Sets, Lists, Maps etc. In the Go SDK for DynamoDB, the types package contains Go representations of these data types and the attributevalue module provides functions to work with Go and DynamoDB types.\nThis blog post will demonstrate how to handle conversions between Go types in your application and DynamoDB. We will start off with simple code snippets to introduce some of the API constructs and wrap up with a example of how to use these Go SDK features in the context of a complete application (including a code walk though).\nYou can refer to the complete code on GitHub\nTo begin with, go through a few examples.\nPlease note that error handling has been purposely omitted in the below code snippets to keep them concise.\nConverting from Go to DynamoDB types The Marshal family of functions takes care of this. It works with basic scalars (int, uint, float, bool, string), maps, slices, and structs.\nTo work with scalar types, just use the (generic) Marshal function:\nfunc marshalScalars() { av, err := attributevalue.Marshal(\"foo\") log.Println(av.(*types.AttributeValueMemberS).Value) av, err = attributevalue.Marshal(true) log.Println(av.(*types.AttributeValueMemberBOOL).Value) av, err = attributevalue.Marshal(42) log.Println(av.(*types.AttributeValueMemberN).Value) av, err = attributevalue.Marshal(42.42) log.Println(av.(*types.AttributeValueMemberN).Value) } Marshal converts a Go data type into a AttributeValue. But AttributeValue itself is just an interface and requires you to cast it to a concrete type such as AttributeValueMemberS (for string), AttributeValueMemberBOOL (for boolean) etc.\nIf you try to cast incompatible types, the SDK responds with a helpful error message. For example, panic: interface conversion: types.AttributeValue is *types.AttributeValueMemberN, not *types.AttributeValueMemberS\nWhen working with slices and maps, you are better off using specific functions such as MarshalList and MarshalMap:\nfunc marshalSlicesAndMaps() { avl, err := attributevalue.MarshalList([]string{\"foo\", \"bar\"}) for _, v := range avl { log.Println(v.(*types.AttributeValueMemberS).Value) } avm, err := attributevalue.MarshalMap(map[string]interface{}{\"foo\": \"bar\", \"boo\": \"42\"}) for k, v := range avm { log.Println(k, \"=\", v.(*types.AttributeValueMemberS).Value) } } The above examples gave you a sense of how to work with simple data types in isolation. In a real world application, you will make use of composite data types to represent your domain model - most likely they will be in the form of Go structs. So let’s look at a few examples of that.\nWorking with Go structs Here is a simple one:\ntype User struct { Name string Age string } func marshalStruct() { user := User{Name: \"foo\", Age: \"42\"} av, err := attributevalue.Marshal(user) avm := av.(*types.AttributeValueMemberM).Value log.Println(\"name\", avm[\"Name\"].(*types.AttributeValueMemberS).Value) log.Println(\"age\", avm[\"Age\"].(*types.AttributeValueMemberS).Value) avMap, err := attributevalue.MarshalMap(user) for name, value := range avMap { log.Println(name, \"=\", value.(*types.AttributeValueMemberS).Value) } } Notice how convenient it is to use MarshalMap (instead of Marshal) when dealing Go structs especially if your application does not know all the attribute names.\nSo far, it seems like we can handle simple use-cases. But we can do better. This example had a homogenous data type i.e. the struct had only string type making it easy to iterate over the result map and cast the value to a *types.AttributeValueMemberS - if that were not the case, you would have to iterate over each and every attribute value type and type cast it to the appropriate Go type. This will be evident when working with rest of the DynamoDB APIs. For example, the result of a GetItem invocation (GetItemOutput) contains a map[string]types.AttributeValue.\nThe SDK provides a way for us to make this much easier!\nConverting from DynamoDB to Go types The Unmarshal family of functions takes care of this. Here is another example:\ntype AdvancedUser struct { Name string Age int IsOnline bool Favourites []string Contact map[string]string RegisteredOn time.Time } func marshalUnmarshal() { user := AdvancedUser{ Name: \"abhishek\", Age: 35, IsOnline: false, Favourites: []string{\"Lost In Translation, The Walking Dead\"}, Contact: map[string]string{\"mobile\": \"+919718861200\", \"email\": \"abhirockzz@gmail.com\"}, RegisteredOn: time.Now(), } avMap, err := attributevalue.MarshalMap(user) var result AdvancedUser err = attributevalue.UnmarshalMap(avMap, \u0026result) log.Println(\"\\nname\", result.Name, \"\\nage\", result.Age, \"\\nfavs\", result.Favourites) } With MarshalMap, we converted an instance of AdvancedUser struct into a map[string]types.AttributeValue (imagine you get this as a response to a GetItem API call). Now, instead of iterating over individual AttributeValues, we simply use UnmarshalMap to convert it back a Go struct.\nThere is more! Utility functions like UnmarshalListOfMaps make it convenient to work with multiple slices of Go structs.\nfunc marshalUnmarshalMultiple() { user1 := User{Name: \"user1\", Age: \"42\"} user1Map, err := attributevalue.MarshalMap(user1) if err != nil { log.Fatal(err) } user2 := User{Name: \"user2\", Age: \"24\"} user2Map, err := attributevalue.MarshalMap(user2) if err != nil { log.Fatal(err) } var users []User err = attributevalue.UnmarshalListOfMaps([]map[string]types.AttributeValue{user1Map, user2Map}, \u0026users) if err != nil { log.Fatal(err) } for _, user := range users { log.Println(\"name\", user.Name, \"age\", user.Age) } } Using struct tags for customization Marshal and Unmarshal functions support the dynamodbav struct tag to control conversion between Go types and DynamoDB AttributeValue. Consider the following struct:\ntype User struct { Email string `dynamodbav:\"email\" json:\"user_email\"` Age int `dynamodbav:\"age,omitempty\" json:\"age,omitempty\"` City string `dynamodbav:\"city\" json:\"city\"` } Couple of common scenarios where the dynamodbav comes in handy.\nCustomize attribute name\nSay, we have a table with email as the partition key. Without the dynamodbav:\"email\" tag, when we marshal the User struct and try to save in table, it will use Email (upper case) as the attribute name - DynamoDB will not accept this since attribute names are case sensitive - “All names must be encoded using UTF-8, and are case-sensitive.”\nNotice that we have combined json tags as well (this is perfectly valid) - it’s not used by DynamoDB but the json library while encoding and decoding data\nHandle missing attributes\nDynamoDB is a NoSQL database and tables don’t have a fixed schema (except for partition key and an optional sort key). For example, a user item may not include the age attribute.By using dynamodbav:\"age,omitempty\", if the Age field is missing, it won’t be sent to DynamoDB (it will be ignored)\nIn the absence of this tag, our DynamoDB record will have Age attribute set to 0 - depending on your use case this may or may not\nTo look at all the usage patterns of this struct tag, refer to the Marshal API documentation.\nAs promised before, let’s explore how to put all these APIs to use within the scope of an…\n… End-to-end example We will look at a Go application that exposes a REST API with a few endpoints. It combines the CRUD APIs (PutItem, GetItem etc.) together with all the functions/APIs mentioned above.\nTest the application\nBefore we see the code, let’s quickly review and test the endpoints exposed by the application. You will need to have Go installed, clone the application and change to the right directory.\ngit clone https://github.com/abhirockzz/dynamodb-go-sdk-type-conversion cd dynamodb-go-sdk-type-conversion First, create a DynamoDB table (you can name it users). Use city as the Partition key, email as the Sort key.\nYou need some test data. You can do so manually, but I have included a simply utility to seed some test data during application startup. To use it, simply set the SEED_TEST_DATA variable at application startup:\nexport SEED_TEST_DATA=true go run main.go # output started http server... This will create 100 items. Check DynamoDB table to confirm:\nYour application should be available at port 8080. You can use curl or any other HTTP client to invoke the endpoints:\n# to get all users curl -i http://localhost:8080/users/ # to get all users in a particular city curl -i http://localhost:8080/users/London # to get a specific user curl -i \"http://localhost:8080/user?city=London\u0026email=user11@foo.com\" To better understand how the above APIs are used, let’s briefly review key parts of the code:\nCode walk through Add new item to a DynamoDB table\nStarting with the HTTP handler for adding a User:\nfunc (h Handler) CreateUser(rw http.ResponseWriter, req *http.Request) { var user model.User err := json.NewDecoder(req.Body).Decode(\u0026user) if err != nil {// handle error} err = h.d.Save(user) if err != nil {// handle error} err = json.NewEncoder(rw).Encode(user.Email) if err != nil {// handle error} } First, we convert the JSON payload into a User struct which we then pass to the Save function.\nfunc (d DB) Save(user model.User) error { item, err := attributevalue.MarshalMap(user) if err != nil {// handle error} _, err = d.client.PutItem(context.Background(), \u0026dynamodb.PutItemInput{ TableName: aws.String(d.table), Item: item}) if err != nil {// handle error} return nil } Notice how MarshalMap is used to convert the User struct to a map[string]types.AttributeValue that the PutItem API can accept:\nGet a single item from DynamoDB\nSince our table has a composite primary key (city is the partition key and email is the sort key), we will need to provide both of them to locate a specific user item:\nfunc (h Handler) FetchUser(rw http.ResponseWriter, req *http.Request) { email := req.URL.Query().Get(\"email\") city := req.URL.Query().Get(\"city\") log.Println(\"getting user with email\", email, \"in city\", city) user, err := h.d.GetOne(email, city) if err != nil {// handle error} err = json.NewEncoder(rw).Encode(user) if err != nil {// handle error} } We extract the email and city from the query parameters in the HTTP request and pass it on to the database layer (GetOne function).\nfunc (d DB) GetOne(email, city string) (model.User, error) { result, err := d.client.GetItem(context.Background(), \u0026dynamodb.GetItemInput{ TableName: aws.String(d.table), Key: map[string]types.AttributeValue{ \"email\": \u0026types.AttributeValueMemberS{Value: email}, \"city\": \u0026types.AttributeValueMemberS{Value: city}}, }) if err != nil {// handle error} if result.Item == nil { return model.User{}, ErrNotFound } var user model.User err = attributevalue.UnmarshalMap(result.Item, \u0026user) if err != nil {// handle error} return user, nil } We invoke GetItem API and get back the result in form of a map[string]types.AttributeValue (via the Item attribute in GetItemOutput). This is converted back into the Go (User) struct using UnmarshalMap.\nNotice that the Key attribute in GetItemInput also accepts a map[string]types.AttributeValue, but we don’t use MarshalMap to create it\nFetch multiple items\nWe can choose to query for all users in a specific city - this is a perfectly valid access pattern since city is the partition key.\nThe HTTP handler function accepts the city as a path parameter, which is passed on to the database layer.\nfunc (h Handler) FetchUsers(rw http.ResponseWriter, req *http.Request) { city := mux.Vars(req)[\"city\"] log.Println(\"city\", city) log.Println(\"getting users in city\", city) users, err := h.d.GetMany(city) if err != nil { http.Error(rw, err.Error(), http.StatusInternalServerError) return } err = json.NewEncoder(rw).Encode(users) if err != nil { http.Error(rw, err.Error(), http.StatusInternalServerError) return } } From there on, GetMany function does all the work:\nfunc (d DB) GetMany(city string) ([]model.User, error) { kcb := expression.Key(\"city\").Equal(expression.Value(city)) kce, _ := expression.NewBuilder().WithKeyCondition(kcb).Build() result, err := d.client.Query(context.Background(), \u0026dynamodb.QueryInput{ TableName: aws.String(d.table), KeyConditionExpression: kce.KeyCondition(), ExpressionAttributeNames: kce.Names(), ExpressionAttributeValues: kce.Values(), }) if err != nil { log.Println(\"Query failed with error\", err) return []model.User{}, err } users := []model.User{} if len(result.Items) == 0 { return users, nil } err = attributevalue.UnmarshalListOfMaps(result.Items, \u0026users) if err != nil { log.Println(\"UnmarshalMap failed with error\", err) return []model.User{}, err } return users, nil } Pay attention to two things:\nHow a KeyConditionExpression is being used (this is from the expressions package) And more interestingly, the usage of UnmarshalListOfMaps function to directly convert a []map[string]types.AttributeValue (slice of items from DynamoDB) into a slice of User struct. If not for this function, we would need to extract each item from the result i.e. a map[string]types.AttributeValue and call UnmarshalMap for each of them. So this is pretty handy! Finally - just get everything!\nThe GetAll function uses Scan operation to retrieve all the records in the DynamoDB table.\nA Scan operation goes over the entire table (or secondary index) and it’s highly likely that it will end up consuming a large chunk of the provisioned throughput, especially if it’s a large table. It should be your last resort - check whether Query API (or BatchGetItem) works for your use case.\nfunc (d DB) GetAll() ([]model.User, error) { result, err := d.client.Scan(context.Background(), \u0026dynamodb.ScanInput{ TableName: aws.String(d.table), }) if err != nil { log.Println(\"Scan failed with error\", err) return []model.User{}, err } users := []model.User{} err = attributevalue.UnmarshalListOfMaps(result.Items, \u0026users) if err != nil { log.Println(\"UnmarshalMap failed with error\", err) return []model.User{}, err } return users, nil } ","wrap-up#Wrap up":"I hope you found this useful and now you are aware of the APIs in DynamoDB Go SDK to work with simple Go types as well as structs, maps, slices etc. I would encourage you to explore some of the other nuances such as how to customize the Marshal and Unmarshal features using MarshalWithOptions and UnmarshalWithOptions respectively."},"title":"How to handle type conversions with the DynamoDB Go SDK"},"/blog/dynamodb-streams-lambda-go/":{"data":{"":"","-lets-get-right-to-it#.. let\u0026rsquo;s get right to it!":"","code-walk-through#Code walk-through":"Since we will only focus on the important bits, lot of the code (print statements, error handling etc.) omitted/commented out for brevity.\nInfra-IS-code with AWS CDK and Go!\nYou can refer to the CDK code here\nWe start by creating a DynamoDB table and ensure that DynamoDB Streams is enabled.\nsourceDynamoDBTable := awsdynamodb.NewTable(stack, jsii.String(\"source-dynamodb-table\"), \u0026awsdynamodb.TableProps{ PartitionKey: \u0026awsdynamodb.Attribute{ Name: jsii.String(\"email\"), Type: awsdynamodb.AttributeType_STRING}, Stream: awsdynamodb.StreamViewType_NEW_AND_OLD_IMAGES}) sourceDynamoDBTable.ApplyRemovalPolicy(awscdk.RemovalPolicy_DESTROY) Then, we handle the Lambda function (this will take care of building and deploying the function) and make sure we provide it appropriate permissions to write to the DynamoDB table.\ncreateUserFunction := awscdklambdagoalpha.NewGoFunction(stack, jsii.String(\"create-function\"), \u0026awscdklambdagoalpha.GoFunctionProps{ Runtime: awslambda.Runtime_GO_1_X(), Environment: \u0026map[string]*string{envVarName: sourceDynamoDBTable.TableName()}, Entry: jsii.String(createFunctionDir)}) sourceDynamoDBTable.GrantWriteData(createUserFunction) The API Gateway (HTTP API) is created, along with the HTTP-Lambda Function integration as well as the appropriate route.\napi := awscdkapigatewayv2alpha.NewHttpApi(stack, jsii.String(\"http-api\"), nil) createFunctionIntg := awscdkapigatewayv2integrationsalpha.NewHttpLambdaIntegration(jsii.String(\"create-function-integration\"), createUserFunction, nil) api.AddRoutes(\u0026awscdkapigatewayv2alpha.AddRoutesOptions{ Path: jsii.String(\"/\"), Methods: \u0026[]awscdkapigatewayv2alpha.HttpMethod{awscdkapigatewayv2alpha.HttpMethod_POST}, Integration: createFunctionIntg}) We also need the target DynamoDB table - note that this table has a composite Primary Key (state and city):\ntargetDynamoDBTable := awsdynamodb.NewTable(stack, jsii.String(\"target-dynamodb-table\"), \u0026awsdynamodb.TableProps{ PartitionKey: \u0026awsdynamodb.Attribute{ Name: jsii.String(\"state\"), Type: awsdynamodb.AttributeType_STRING}, SortKey: \u0026awsdynamodb.Attribute{ Name: jsii.String(\"city\"), Type: awsdynamodb.AttributeType_STRING}, }) targetDynamoDBTable.ApplyRemovalPolicy(awscdk.RemovalPolicy_DESTROY) Finally, we create the second Lambda function which is responsible for data replication, grant it the right permissions and most importantly, add the DynamoDB as the event source.\nreplicateUserFunction := awscdklambdagoalpha.NewGoFunction(stack, jsii.String(\"replicate-function\"), \u0026awscdklambdagoalpha.GoFunctionProps{ Runtime: awslambda.Runtime_GO_1_X(), Environment: \u0026map[string]*string{envVarName: targetDynamoDBTable.TableName()}, Entry: jsii.String(replicateFunctionDir)}) replicateUserFunction.AddEventSource(awslambdaeventsources.NewDynamoEventSource(sourceDynamoDBTable, \u0026awslambdaeventsources.DynamoEventSourceProps{StartingPosition: awslambda.StartingPosition_LATEST, Enabled: jsii.Bool(true)})) targetDynamoDBTable.GrantWriteData(replicateUserFunction) Lambda Function - create user\nYou can refer to the Lambda Function code here\nThe function logic is pretty straightforward - it converts the incoming JSON payload into a Go struct and then invokes DynamoDB PutItem API to persist the data.\nfunc handler(ctx context.Context, req events.APIGatewayV2HTTPRequest) (events.APIGatewayV2HTTPResponse, error) { payload := req.Body var user User err := json.Unmarshal([]byte(payload), \u0026user) if err != nil {//..handle} item := make(map[string]types.AttributeValue) item[\"email\"] = \u0026types.AttributeValueMemberS{Value: user.Email} item[\"state\"] = \u0026types.AttributeValueMemberS{Value: user.State} item[\"city\"] = \u0026types.AttributeValueMemberS{Value: user.City} item[\"zipcode\"] = \u0026types.AttributeValueMemberN{Value: user.Zipcode} item[\"active\"] = \u0026types.AttributeValueMemberBOOL{Value: true} _, err = client.PutItem(context.Background(), \u0026dynamodb.PutItemInput{ TableName: aws.String(table), Item: item, }) if err != nil {//..handle} return events.APIGatewayV2HTTPResponse{StatusCode: http.StatusCreated}, nil } Lambda Function - replicate data\nYou can refer to the Lambda Function code here\nThe handler for the data replication function accepts DynamoDBEvent as a parameter. It extracts the new added record and creates a new record which can be saved to the target DynamoDB table. The data type for each attribute is checked and handled accordingly. Although the code just shows String and Boolean types, this can be used for other DynamoDB data types such as Maps, Sets etc.\nfunc handler(ctx context.Context, e events.DynamoDBEvent) { for _, r := range e.Records { item := make(map[string]types.AttributeValue) for k, v := range r.Change.NewImage { if v.DataType() == events.DataTypeString { item[k] = \u0026types.AttributeValueMemberS{Value: v.String()} } else if v.DataType() == events.DataTypeBoolean { item[k] = \u0026types.AttributeValueMemberBOOL{Value: v.Boolean()} } } _, err := client.PutItem(context.Background(), \u0026dynamodb.PutItemInput{ TableName: aws.String(table), Item: item}) if err != nil {//..handle} } } Here are some things you can try out:\nInsert more data in the source table - look for ways to do bulk inserts into a DynamoDB table Execute queries in the target table based on state, city or both. ","end-to-end-solution#End to end solution\u0026hellip;":"Replicate DynamoDB data from one table to another\nThis blog post will help you get quickly started with DynamoDB Streams and AWS Lambda using Go. It will cover how to deploy the entire solution using AWS CDK.\nThe use case presented here is pretty simple. There are a couple of DynamoDB tables and the goal is to capture the data in one of those tables (also referred to as the source table) and replicate them to another table (also referred to as the target table) so that it can serve different queries. To demonstrate an end-to-end flow, there is also an Amazon API Gateway that front ends a Lambda function which persists data in the source DynamoDB table. Changes in this table will trigger another Lambda function (thanks to DynamoDB Streams) which will finally replicate the data into the target table.\nGlobal or Local Secondary Index offer similar capability.\nNow that you have an overview of what we are trying to achieve here…\n.. let’s get right to it! Before you proceed, make sure you have the Go programming language (v1.16 or higher) and AWS CDK installed.\nClone the project and change to the right directory:\ngit clone https://github.com/abhirockzz/dynamodb-streams-lambda-golang cd cdk To start the deployment…\n.. all you need to do is run a single command (cdk deploy), and wait for a bit. You will see a list of resources that will be created and will need to provide your confirmation to proceed.\nDon’t worry, in the next section I will explain what’s happening.\ncdk deploy # output Bundling asset DynamoDBStreamsLambdaGolangStack/ddb-streams-function/Code/Stage... ✨ Synthesis time: 5.94s This deployment will make potentially sensitive changes according to your current security approval level (--require-approval broadening). Please confirm you intend to make the following modifications: //.... omitted Do you wish to deploy these changes (y/n)? y This will start creating the AWS resources required for our application.\nIf you want to see the AWS CloudFormation template which will be used behind the scenes, run cdk synth and check the cdk.out folder\nYou can keep track of the progress in the terminal or navigate to AWS console: CloudFormation \u003e Stacks \u003e DynamoDBStreamsLambdaGolangStack\nOnce all the resources are created, you can try out the application. You should have:\nTwo Lambda functions Two DynamoDB tables (source and target) One API Gateway (also route, integration) along with a few others (like IAM roles etc.) Before you proceed, get the API Gateway endpoint that you will need to use. It’s available in the stack output (in the terminal or the Outputs tab in the AWS CloudFormation console for your Stack):\nEnd to end solution… Start by creating a few users in the (source) DynamoDB table\nTo do this, invoke the API Gateway (HTTP) endpoint with the appropriate JSON payload:\n# export the API Gateway endpoint export APIGW_ENDPOINT=\u003creplace with API gateway endpoint above\u003e # for example: export APIGW_ENDPOINT=https://gy8gxsx9x7.execute-api.us-east-1.amazonaws.com/ # invoke the endpoint with JSON data curl -i -X POST -d '{\"email\":\"user1@foo.com\", \"state\":\"New York\",\"city\":\"Brooklyn\",\"zipcode\": \"11208\"}' -H 'Content-Type: application/json' $APIGW_ENDPOINT curl -i -X POST -d '{\"email\":\"user2@foo.com\", \"state\":\"New York\",\"city\":\"Staten Island\",\"zipcode\": \"10314\"}' -H 'Content-Type: application/json' $APIGW_ENDPOINT curl -i -X POST -d '{\"email\":\"user3@foo.com\", \"state\":\"Ohio\",\"city\":\"Cincinnati\",\"zipcode\": \"45201\"}' -H 'Content-Type: application/json' $APIGW_ENDPOINT curl -i -X POST -d '{\"email\":\"user4@foo.com\", \"state\":\"Ohio\",\"city\":\"Cleveland\",\"zipcode\": \"44101\"}' -H 'Content-Type: application/json' $APIGW_ENDPOINT curl -i -X POST -d '{\"email\":\"user5@foo.com\", \"state\":\"California\",\"city\":\"Los Angeles\",\"zipcode\": \"90003\"}' -H 'Content-Type: application/json' $APIGW_ENDPOINT Navigate to the DynamoDB table in the AWS console and ensure that records have been created:\nIf you the AWS CLI handy, you can also try aws dynamodb scan --table \u003cname of table\u003e\nIf all goes well, our replication function should also work. To confirm, you need to check the target DynamoDB table.\nNotice that the zipcode attribute is missing - this is done on purpose for this demo. You can pick and choose the attributes you want to include in the target table and write your function logic accordingly.\nThe target DynamoDB table has state as a partition key and city as the sort key, you can query it in a different way (as compared to the source table which you could query only based on email).\nDon’t forget to clean up! Once you’re done, to delete all the services, simply use:\ncdk destroy #output prompt (choose 'y' to continue) Are you sure you want to delete: DynamoDBStreamsLambdaGolangStack (y/n)? Awesome! You were able to setup and try the complete solution. Before we wrap up, let’s quickly walk through some of important parts of the code to get a better understanding of what’s going the behind the scenes.","wrap-up#Wrap up":"In this blog, you saw a simple example of how to leverage DynamoDB Streams to react to table data changes in near-real time using a combination of DynamoDB Streams and Lambda functions. You also used AWS CDK to deploy the entire infrastructure including API Gateway, Lambda Functions, DynamoDB tables, integrations as well as Lambda event source mappings.\nAll this was done using the Go programming language, which is very well supported in DynamoDB, AWS Lambda and AWS CDK.\nHappy building!"},"title":"Learn how to use DynamoDB Streams with AWS Lambda and Go"},"/blog/eventhubs-rbac/":{"data":{"":"","-security-configuration#\u0026hellip; security configuration":"","conclusion#Conclusion":"Hopefully this was useful in demonstrating how to use RBAC for Event Hubs applications using Azure Active Directory. In a future blog post, I will try to cover Managed Identity as well. Until then, stay tuned!","lets-try-it-out#Let\u0026rsquo;s try it out":"Azure Event Hubs is streaming platform and event ingestion service that can receive and process millions of events per second. In this blog, we are going to cover one of the security aspects related to Azure Event Hubs.\nShared Access Signature (SAS) is a commonly used authentication mechanism for Azure Event Hubs which can be used to enforce granular control over the type of access you want to grant - it works by configuring rules on Event Hubs resources (namespace or topic). However, it is recommended that you use Azure AD credentials (over SAS) whenever possible since it provides similar capabilities without the need to manage SAS tokens or worry about revoking a compromised SAS.\nTo read more about this topic, check out “Authenticate access to Event Hubs resources using shared access signatures (SAS)”.\nThere a couple of ways in which you can do this:\nusing Managed Identity or by using service principal in your client applications We will explore the second option i.e. how to use Azure Active Directory based authentication in your Azure Event Hubs client applications. With a practical example, you will learn:\nOverview of Azure Event Hubs roles How to use Azure CLI to configure Service Principal and RBAC policies for Event Hubs Event Hubs supports multiple programming languages with specific SDKs and we’ll see how to use RBAC with Java and Go clients The code is available in this GitHub repo - https://github.com/abhirockzz/azure-eventhubs-rbac-example\nOverview This section provides a high level overview for you to get an understand the key terminologies: Roles, Security principals and Role based access control (RBAC).\nRoles: Azure Event Hubs defines specific roles each of which allows you to take specific action on its resources - Data Owner, Data Sender, Data Receiver. They are quite self explanatory - Sender and Receiver roles only allow send and receive respectively, while the Owner role is like an admin privilege which allows you to complete access.\nFor details on these roles, please refer to their documentation links - Azure Event Hubs Data Owner, Azure Event Hubs Data Sender, Azure Event Hubs Data Receiver\nRBAC, Service Principals: Service Principals are entities to who, these roles are granted - this is “Role Based Access Control” since the role you grant to the Service Principal defines which actions they can perform - in this case, the actions are: send, receive or everything!\n… the scenario Here is the example we will use to learn this. There are two Event Hubs client apps - a Java producer and a Go consumer. We will configure fine grained rules (i.e. enforce RBAC) such that:\nJava app can only send messages to an Event Hubs topic, and, Go app can only receive messages from an Event Hubs topic Here are relevant code snippets:\nIn the Java producer client, the DefaultAzureCredentialBuilder is used.\nString eventhubsNamespace = System.getenv(\"EVENTHUBS_NAMESPACE\"); String eventhubName = System.getenv(\"EVENTHUB_NAME\"); EventHubProducerClient producer = new EventHubClientBuilder().credential(eventhubsNamespace, eventhubName, new DefaultAzureCredentialBuilder().build()).buildProducerClient(); By convention (default), the Java SDK tries to read pre-defined environment variables for Service Principal info and authenticate based on that. If this does not work, it falls back to try the SAS auth mechanism and looks for another set of environment variables\nSimilarly, in the Go consumer client, the Event Hubs client creation process is greatly simplified as well\nehNamespace := os.Getenv(\"EVENTHUBS_NAMESPACE\") ehName := os.Getenv(\"EVENTHUB_NAME\") hub, err := eventhub.NewHubWithNamespaceNameAndEnvironment(ehNamespace, ehName) The developer experience is uniform across SDKs and NewHubWithNamespaceNameAndEnvironment does the same thing as the DefaultAzureCredentialBuilder in terms of following a fixed convention attempting to authenticate using Azure Active Directory (AAD) backed Service Principal first, followed by SAS mechanism\nWith that said, you do need a few things to get through this tutorial:\nPre-requisites You will need a Microsoft Azure account. Go ahead and sign up for a free one!\nAzure CLI or Azure Cloud Shell - you can either choose to install the Azure CLI if you don’t have it already (should be quick!) or just use the Azure Cloud Shell from your browser.\nLet’s get going.. start off by setting up Azure Event Hubs\nSetup Azure Event Hubs This is all done via Azure CLI, but you can also use the Azure Portal if you like. The end goal is to have an Azure Event Hubs namespace along with an Event Hub (aka topic)\nCreate an Azure resource group if you don’t have one already\nAZURE_SUBSCRIPTION=[to be filled] AZURE_RESOURCE_GROUP=[to be filled] AZURE_LOCATION=[to be filled] az account set --subscription $AZURE_SUBSCRIPTION az group create --name $AZURE_RESOURCE_GROUP --location $AZURE_LOCATION Use az eventhubs namespace create to create an Event Hubs namespace\nEVENT_HUBS_NAMESPACE=[to be filled] az eventhubs namespace create --name $EVENT_HUBS_NAMESPACE --resource-group $AZURE_RESOURCE_GROUP --location $AZURE_LOCATION --enable-auto-inflate false And then create an Event Hub using az eventhubs eventhub create\nEVENT_HUB_NAME=[to be filled] az eventhubs eventhub create --name $EVENT_HUB_NAME --resource-group $AZURE_RESOURCE_GROUP --namespace-name $EVENT_HUBS_NAMESPACE --partition-count 3 .. now for the the key parts…\n… security configuration We will use Azure CLI to create Service Principals using az ad sp create-for-rbac\nFor the sender application (we name it eh-sender-sp)\naz ad sp create-for-rbac -n \"eh-sender-sp\" You will get a JSON response as such - please note down the appId, password and tenant\n{ \"appId\": \"fe7280c7-5705-4789-b17f-71a472340429\", \"displayName\": \"eh-sender-sp\", \"name\": \"http://eh-sender-sp\", \"password\": \"29c719dd-f2b3-46de-b71c-4004fb6116ee\", \"tenant\": \"42f988bf-86f1-42af-91ab-2d7cd011db42\" } For the receiver application (we name it eh-receiver-sp)\naz ad sp create-for-rbac -n \"eh-receiver-sp\" You will get a JSON response - please note down the appId, password and tenant\nYou can also use the Azure Portal to create a Service Principal and generate a Client Secret\nEnforce RBAC Going through this process using CLI involves a few steps, but it’s beneficial in the long term e.g. for automation. It can also be carried out using the Azure Portal\nGet the IDs for both the roles using az role definition list\nEH_SENDER_ROLE_ID=$(az role definition list -n \"Azure Event Hubs Data Sender\" -o tsv --query '[0].id') EH_RECEIVER_ROLE_ID=$(az role definition list -n \"Azure Event Hubs Data Receiver\" -o tsv --query '[0].id') Azure Event Hubs Data Sender and Azure Event Hubs Data Receiver are the role names\nGet the subscription ID for Azure Event Hubs namespace using az eventhubs namespace show\nexport RESOURCE_GROUP=[replace with resource group name] export EVENTHUBS_NAMESPACE=[replace with namespace] EVENTHUBS_ID=$(az eventhubs namespace show --resource-group $RESOURCE_GROUP --name $EVENTHUBS_NAMESPACE -o tsv --query 'id') Assign roles using az role assignment create\nexport SENDER_SP_ID=[replace with Service Principal \"appId\" for the sender SP] export RECEIVER_SP_ID=[replace with Service Principal \"appId\" for the receiver SP] az role assignment create --assignee $SENDER_SP_ID --role $EH_SENDER_ROLE_ID --scope $EVENTHUBS_ID az role assignment create --assignee $RECEIVER_SP_ID --role $EH_RECEIVER_ROLE_ID --scope $EVENTHUBS_ID Alright, you’re all set!\nLet’s try it out Clone the sample apps from GitHub\ngit clone https://github.com/abhirockzz/azure-eventhubs-rbac-example.git Start the Go consumer application:\nexport EVENTHUBS_NAMESPACE=[replace with namespace] export EVENTHUB_NAME=[replace with name of the Event Hub] export AZURE_TENANT_ID=[replace with Service Principal \"tenant\" for the receiver SP] export AZURE_CLIENT_ID=[replace with Service Principal \"appId\" for the receiver SP] export AZURE_CLIENT_SECRET=[replace with Service Principal \"password\" for the receiver SP] cd azure-eventhubs-rbac-example/consumer-go go run main.go The program will block, waiting for events….\nIn another terminal, start producer application - this will just send 10 events end exit (re-run if you want to send more events)\ncd azure-eventhubs-rbac-example/producer-java //build the Java app - it uses Maven mvn clean install export EVENTHUBS_NAMESPACE=[replace with namespace] export EVENTHUB_NAME=[replace with name of the Event Hub] export AZURE_TENANT_ID=[replace with Service Principal \"tenant\" for the producer SP] export AZURE_CLIENT_ID=[replace with Service Principal \"appId\" for the producer SP] export AZURE_CLIENT_SECRET=[replace with Service Principal \"password\" for the producer SP] java -jar target/eventhubs-java-producer-jar-with-dependencies.jar You should see the received events in the consumer app terminal!\nAs an exercise, to simulate an error scenario, you exchange can the client details for sender and consumer apps to see how they behave.","overview#Overview":"","pre-requisites#Pre-requisites":"","setup-azure-event-hubs#Setup Azure Event Hubs":""},"title":"Azure Event Hubs 'Role Based Access Control' in action"},"/blog/get-started-with-vector-search-in-azure-cosmos-db/":{"data":{"":"\nThis is a guide for folks who are looking for a way to quickly and easily try out the Vector Search feature in Azure Cosmos DB for NoSQL. This app uses a simple dataset of movies to find similar movies based on a given criteria. It’s implemented in four languages - Python, TypeScript, .NET and Java. There are instructions that walk you through the process of setting things up, loading data, and then executing similarity search queries.\nA vector database is designed to store and manage vector embeddings, which are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, and tens of thousands of dimensions might be used to represent data. A vector’s position in this space represents its characteristics. Words, phrases, or entire documents, and images, audio, and other types of data can all be vectorized. These vector embeddings are used in similarity search, multi-modal search, recommendations engines, large languages models (LLMs), etc.","closing-notes#Closing notes":"I hope you found this useful! Before wrapping up, here are a few things to keep in mind:\nThere are different vector index types you should experiment with (flat, quantizedFlat) Consider the metric your are using to compute distance/similarity (I used cosine, but you can also use euclidean, or dot product.) Which embedding model you use is also an important consideration - I used text-embedding-ada-002 but there are other options such as text-embedding-3-large, text-embedding-3-small. You can use also Azure Cosmos DB for MongoDB vCore for vector search. ","configure-integrated-vector-database-in-azure-cosmos-db-for-nosql#Configure Integrated Vector Database in Azure Cosmos DB for NoSQL":"Before you start loading data, make sure to configure the vector database in Azure Cosmos DB.\nEnable the feature This is a one-time operation - you will need to explicitly enable the vector indexing and search feature.\nCreate database and container Once you have done that, go ahead and create a database and collection. I created a database named movies_db and a container named movies with the partition key set to /id.\nCreate policies You will need to configure a vector embedding policy as well as an indexing policy for the container. For now, you can do it manually via the Azure portal (it’s possible to do it programmatically as well) as part of the collection creation process. Use the same policy information as per the above, at least for this sample app:\nChoice of index type: Note that I have chosen the diskANN index type which and a dimension of 1536 for the vector embeddings. The embedding model I chose was text-embedding-ada-002 model and it supports dimension size of 1536. I would recommend that you stick to these values for running this sample app. But know that you can change the index type but will need to change the embedding model to match the new dimension of the specified index type.\nAlright, lets move on.","load-data-in-azure-cosmos-db#Load data in Azure Cosmos DB":"To keep things simple, I have a small dataset of movies in JSON format (in movies.json file). The process is straightforward:\nRead movie info data from json file, Generate vector embeddings (of the movie description), and Insert the complete data (title, description and embeddings) into Azure Cosmos DB container. As promised, here are the language specific instructions - refer to the one thats relevant to you. Irrespective of the language, you need to set the following environment variables:\nexport COSMOS_DB_CONNECTION_STRING=\"\" export DATABASE_NAME=\"\" export CONTAINER_NAME=\"\" export AZURE_OPENAI_ENDPOINT=\"\" export AZURE_OPENAI_KEY=\"\" export AZURE_OPENAI_VERSION=\"2024-10-21\" export EMBEDDINGS_MODEL=\"text-embedding-ada-002\" Before moving on, don’t forget to clone this repository:\ngit clone https://github.com/abhirockzz/cosmosdb-vector-search-python-typescript-java-dotnet cd cosmosdb-vector-search-python-typescript-java-dotnet Load vector data using Python SDK for Azure Cosmos DB Setup the Python environment and install the required dependencies:\ncd python python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt To load the data, run the following command:\npython load.py Load vector data using Typescript SDK for Azure Cosmos DB Install the required dependencies:\ncd typescript npm install Build the program and then load the data:\nnpm run build npm run load Load vector data using Java SDK for Azure Cosmos DB Install dependencies, build the application:\ncd java mvn clean install Load the data:\njava -jar target/cosmosdb-java-vector-search-1.0-SNAPSHOT.jar load Load vector data using dotnet SDK for Azure Cosmos DB Install dependencies and load the data:\ncd dotnet dotnet restore dotnet run load Irrespective of the language, you should see the output similar to this (with slight differences):\ndatabase and container ready.... Generated description embedding for movie: The Matrix Added data to Cosmos DB for movie: The Matrix .... ","prerequisites#Prerequisites":"You will need:\nAn Azure subscription. If you don’t have one, you can create a free Azure account. If for some reason you cannot create an Azure subscription, try Azure Cosmos DB for NoSQL free. Once that’s done, go ahead and create an Azure Cosmos DB for NoSQL account Create an Azure OpenAI Service resource. Azure OpenAI Service provides access to OpenAI’s models including the GPT-4o, GPT-4o mini (and more), as well as embedding models. In this example, we will use the text-embedding-ada-002 embedding model. Deploy this model using the Azure AI Foundry portal. I am assuming you have the required programming language already setup. To run the Java example, you need to have Maven installed (most likely you do, but I wanted to call it out).","vectorsimilarity-search#Vector/Similarity search":"The search component queries Azure Cosmos DB collection to find similar movies based on a given search criteria - for example, you can search for comedy movies. This is done using the VectorDistance function to get the similarity score between two vectors.\nAgain, the process is quite simple:\nGenerate a vector embedding for the search criteria, and Use the VectorDistance function to compare it. This is what the query looks like:\nSELECT TOP @num_results c.id, c.description, VectorDistance(c.embeddings, @embedding) AS similarityScore FROM c ORDER BY VectorDistance(c.embeddings, @embedding) Just like data loading, the search is also language specific. Here are the instructions for each language.\nI am assuming you have already set the environment variables and loaded the data.\nInvoke the respective program with your search criteria (e.g. inspiring, comedy, etc.) and the number of results (top N) you want to see.\nPython\npython search.py \"inspiring\" 3 Typescript\nnpm run search \"inspiring\" 3 Java\njava -jar target/cosmosdb-java-vector-search-1.0-SNAPSHOT.jar search \"inspiring\" 3 dotnet\ndotnet run search \"inspiring\" 3 Irrespective of the language, you should get the results similar to this. For example, my search query was “inspiring” and I got the following results:\nSearch results for query: inspiring Similarity score: 0.7809536662138555 Title: Forrest Gump Description: The story of a man with a low IQ who achieves incredible feats in his life, meeting historical figures and finding love along the way. ===================================== Similarity score: 0.771059411474658 Title: The Shawshank Redemption Description: Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency. ===================================== Similarity score: 0.768073216615931 Title: Avatar Description: A paraplegic Marine dispatched to the moon Pandora on a unique mission becomes torn between following his orders and protecting the world he feels is his home. ===================================== ","verify-data-in-azure-cosmos-db#Verify data in Azure Cosmos DB":"Check the data in the Azure portal. You can also use the Visual Studio Code extension as well, that’s pretty handy!\nLets move on to the search part!"},"title":"Getting started with Vector Search in Azure Cosmos DB"},"/blog/getting-started-msk-serverless-lamda-go/":{"data":{"":"In this blog post you will learn how to deploy a Go Lambda function and trigger it in response to events sent to a topic in a MSK Serverless cluster.\nPrerequisites Infrastructure setup Send data to MSK Serverless using producer application Configure and deploy the Lambda function Verify the integration Conclusion The following topics have been covered:\nHow to use the franz-go Go Kafka client to connect to MSK Serverless using IAM authentication. Write a Go Lambda function to process data in MSK topic. Create the infrastructure - VPC, subnets, MSK cluster, Cloud9 etc. Configure Lambda and Cloud9 to access MSK using IAM roles and fine-grained permissions. ","conclusion#Conclusion":"You were able to setup, configure and deploy a Go Lambda function and trigger it in response to events sent to a topic in a MSK Serverless cluster!","configure-and-deploy-the-lambda-function#Configure and deploy the Lambda function":"Create Lambda execution IAM role and attach the policy\naws iam create-role --role-name LambdaMSKRole --assume-role-policy-document file://lambda-trust-policy.json aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaMSKExecutionRole --role-name LambdaMSKRole Before creating the policy, update the msk-consumer-policy.json file to reflect the required details including MSK cluster ARN etc.\naws iam put-role-policy --role-name LambdaMSKRole --policy-name MSKConsumerPolicy --policy-document file://msk-consumer-policy.json Build and deploy the Go function and create a zip file\nBuild and zip the function code:\nGOOS=linux go build -o app zip func.zip app Deploy to Lambda:\nexport LAMBDA_ROLE_ARN=\u003center the ARN of the LambdaMSKRole created above e.g. arn:aws:iam::\u003cyour AWS account ID\u003e:role/LambdaMSKRole\u003e aws lambda create-function \\ --function-name msk-consumer-function \\ --runtime go1.x \\ --zip-file fileb://func.zip \\ --handler app \\ --role $LAMBDA_ROLE_ARN Lambda VPC configuration\nMake sure you choose the same VPC and private subnets as the MSK cluster. Also, select the same security group ID as MSK (for convenience) - if you select a different one, make sure to update MSK security group to add an inbound rule (for port 9098), just like you did for the Cloud9 instance in an earlier step.\nConfigure the MSK trigger for the function\nMake sure to choose the right MSK Serverless cluster and enter the correct topic name.","infrastructure-setup#Infrastructure setup":"Create VPC and other resources\nUse a CloudFormation template for this.\naws cloudformation create-stack --stack-name msk-vpc-stack --template-body file://template.yaml Wait for the stack creation to complete before proceeding to other steps.\nCreate MSK Serverless cluster\nUse AWS Console to create the cluster.\nConfigure the VPC and private subnets created in the previous step.\nCreate AWS Cloud9 instance\nMake sure its in the same VPC as the MSK Serverless cluster and choose the public subnet that you created earlier.\nConfigure MSK cluster security group\nAfter the Cloud9 instance is created, edit the MSK cluster security group to allow access from the Cloud9 instance.\nConfigure Cloud9 to send data to MSK Serverless cluster\nThe code that we run from Cloud9 is going to produce data to the MSK Serverless cluster. So we need to ensure that it has the right privileges. For this, we need to create an IAM role and attach required permissions policy.\naws iam create-role --role-name Cloud9MSKRole --assume-role-policy-document file://ec2-trust-policy.json Before creating the policy, update the msk-producer-policy.json file to reflect the required details including MSK cluster ARN etc.\naws iam put-role-policy --role-name Cloud9MSKRole --policy-name MSKProducerPolicy --policy-document file://msk-producer-policy.json Attach the IAM role to the Cloud9 EC2 instance:","prerequisites#Prerequisites":"You will need an AWS account, install AWS CLI as well a recent version of Go (1.18 or above).\nClone this GitHub repository and change to the right directory:\ngit clone https://github.com/abhirockzz/lambda-msk-serverless-trigger-golang cd lambda-msk-serverless-trigger-golang ","send-data-to-msk-serverless-using-producer-application#Send data to MSK Serverless using producer application":"Log into the Cloud9 instance and run the producer application (its a Docker image) from a terminal.\nexport MSK_BROKER=\u003center the MSK Serverless endpoint\u003e export MSK_TOPIC=test-topic docker run -p 8080:8080 -e MSK_BROKER=$MSK_BROKER -e MSK_TOPIC=$MSK_TOPIC public.ecr.aws/l0r2y6t0/msk-producer-app The application exposes a REST API endpoint using which you can send data to MSK.\ncurl -i -X POST -d 'test event 1' http://localhost:8080 This will create the specified topic (since it was missing to begin with) and also send the data to MSK.\nNow that the cluster and producer application are ready, we can move on to the consumer. Instead of creating a traditional consumer, we will deploy a Lambda function that will be automatically invoked in response to data being sent to the topic in MSK.","verify-the-integration#Verify the integration":"Go back to the Cloud9 terminal and send more data using the producer application\nI used a handy json utility called jo (sudo yum install jo)\nAPP_URL=http://localhost:8080 for i in {1..5}; do jo email=user${i}@foo.com name=user${i} | curl -i -X POST -d @- $APP_URL; done In the Lambda function logs, you should see the messages that you sent."},"title":"Getting started with MSK Serverless and AWS Lambda using Go"},"/blog/golang-azure-data-explorer-getting-started/":{"data":{"":"With the help of an example, this blog post will walk you through how to use the Azure Data explorer Go SDK to ingest data from a Azure Blob storage container and query it programmatically using the SDK. After a quick overview of how to setup Azure Data Explorer cluster (and a database), we will explore the code to understand what’s going on (and how) and finally test the application using a simple CLI interface\nThe sample data is a CSV file that can be downloaded from here\nThe code is available on GitHub https://github.com/abhirockzz/azure-dataexplorer-go","code-walk-through#Code walk through":"At a high level, this is what the sample code does:\nConnect to an Azure Data Explorer cluster (of course!) Create a table (and list them just to be sure) Create data mapping Ingest/load existing data from a CSV file in Azure Blob storage Run a query on the data you just ingested Let’s look at each of these steps\nConnect to an Azure Data Explorer cluster We use Service Principal to authenticate to Azure Data Explorer and provide the Azure tenant ID, client ID and client secret (which were obtained after creating the principal using az ad sp create-for-rbac)\nauth := kusto.Authorization{Config: auth.NewClientCredentialsConfig(clientID, clientSecret, tenantID)} kc, err := kusto.New(kustoEndpoint, auth) if err != nil { log.Fatal(\"failed to create kusto client\", err) } You can check out the code here\nCreate table and data mappings To create a table, we simply execute create table\nfunc CreateTable(kc *kusto.Client, kustoDB string) { _, err := kc.Mgmt(context.Background(), kustoDB, kusto.NewStmt(createTableCommand)) if err != nil { log.Fatal(\"failed to create table\", err) } log.Printf(\"table %s created\\n\", kustoTable) } You can check out the code here\nNotice how we use client.Mgmt to execute this operation since this is a management query. Later, you will see how to execute query to read data from Azure Data Explorer.\nTo confirm, we run a query to check the tables in database i.e. show tables\nfunc FindTable(kc *kusto.Client, kustoDB string) []TableInfo { var tables []TableInfo ri, err := kc.Mgmt(context.Background(), kustoDB, kusto.NewStmt(testQuery)) if err != nil { log.Fatalf(\"failed to execute query %s - %s\", testQuery, err) } var t TableInfo for { row, err := ri.Next() if err != nil { if err == io.EOF { break } else { log.Println(\"error\", err) } } row.ToStruct(\u0026t) tables = append(tables, t) } return tables } ... type TableInfo struct { Name string `kusto:\"TableName\"` DB string `kusto:\"DatabaseName\"` } You can check out the code here\nAfter executing the query, ToStruct is used to save the result to an instance of a user-defined TableInfo struct\nOnce the table is created, we can configure data mappings that are used during ingestion to map incoming data to columns inside Kusto tables\nfunc CreateMapping(kc *kusto.Client, kustoDB string) { _, err := kc.Mgmt(context.Background(), kustoDB, kusto.NewStmt(createMappingCommand)) if err != nil { log.Fatal(\"failed to create mapping\", err) } log.Printf(\"mapping %s created\\n\", kustoMappingRefName) } You can check out the code here\nIngest data from Azure Blob storage To ingest data we use the Ingestion client\nconst blobStorePathFormat = \"https://%s.blob.core.windows.net/%s/%s%s\" func CSVFromBlob(kc *kusto.Client, blobStoreAccountName, blobStoreContainer, blobStoreToken, blobStoreFileName, kustoMappingRefName, kustoDB, kustoTable string) { kIngest, err := ingest.New(kc, kustoDB, kustoTable) if err != nil { log.Fatal(\"failed to create ingestion client\", err) } blobStorePath := fmt.Sprintf(blobStorePathFormat, blobStoreAccountName, blobStoreContainer, blobStoreFileName, blobStoreToken) err = kIngest.FromFile(context.Background(), blobStorePath, ingest.FileFormat(ingest.CSV), ingest.IngestionMappingRef(kustoMappingRefName, ingest.CSV)) if err != nil { log.Fatal(\"failed to ingest file\", err) } log.Println(\"Ingested file from -\", blobStorePath) } You can check out the code here\nWe have the path to the file in Azure Blob storage and we refer to it in FromFile function along with file type (CSV in this case) as well as data mapping we just created (StormEvents_CSV_Mapping)\nQuery data We fetch some data from the StormEvents table using the following query:\nStormEvents | where EventType == 'Flood' and State == 'WASHINGTON' | sort by DamageProperty desc | project StartTime, EndTime, Source, DamageProperty This time, we use client.Query (not Mgmt) to read data from the table.\nfunc Get(kc *kusto.Client, kustoDB string) []StormDetails { var events []StormDetail ri, err := kc.Query(context.Background(), kustoDB, kusto.NewStmt(query)) if err != nil { log.Fatalf(\"failed to execute query %s - %s\", query, err) } for { row, err := ri.Next() if err != nil { if err == io.EOF { break } else { log.Println(\"error\", err) } } var event StormDetail row.ToStruct(\u0026event) events = append(events, event) } return events ... type StormDetail struct { Start time.Time `kusto:\"StartTime\"` End time.Time `kusto:\"EndTime\"` From string `kusto:\"Source\"` Damage int32 `kusto:\"DamageProperty\"` } Each row in the result in converted into a StormDetail struct using ToStruct\nYou can check out the code here\nThe list of StormDetails is then displayed to stdout in the form a user-friendly tabular format\n... data := [][]string{} for _, detail := range details { data = append(data, []string{detail.Start.String(), detail.End.String(), detail.From, strconv.Itoa(int(detail.Damage))}) } log.Println(\"StormEvents data....\") table := tablewriter.NewWriter(os.Stdout) table.SetHeader([]string{\"Start Time\", \"End Time\", \"From\", \"Damage\"}) for _, v := range data { table.Append(v) } table.Render() ... You can check out the code here\nFinally, to drop the table, we use .drop table StormEvents\nconst dropTableQ = \".drop table StormEvents\" func dropTable(kc *kusto.Client) { _, err := kc.Mgmt(context.Background(), kustoDB, kusto.NewStmt(dropTableQ)) if err != nil { log.Fatal(\"failed to drop table - \", err) } } You can check out the code here","conclusion#Conclusion":"Hopefully this was helpful in demonstrating how to use the Go SDK to interact with Azure Data Explorer. This obviously just one of the ways! For more, dig into the documentation and happy exploring!","pre-requisites#Pre-requisites":"Install Go 1.13 or above\nYou will need a Microsoft Azure account. Go ahead and sign up for a free one!\nInstall the Azure CLI if you don’t have it already (should be quick!)","run-the-example#Run the example":"Now that you have an idea of what’s going on, let’s try it out using a CLI\nSet the required environment variables\nexport AZURE_SP_CLIENT_ID=\"service principal client id\" export AZURE_SP_CLIENT_SECRET=\"\u003cservice principal client secret\u003e\" export AZURE_SP_TENANT_ID=\"\u003ctenant ID\u003e\" #e.g. https://mykusto.southeastasia.kusto.windows.net export KUSTO_ENDPOINT=\"https://\u003ccluster name\u003e.\u003cazure region\u003e.kusto.windows.net\" Get the code and build it\ngit clone https://github.com/abhirockzz/azure-dataexplorer-go cd azure-dataexplorer-go go build -o azdatax //to confirm chmod a+x azdatax \u0026\u0026 ./azdatax //output CLI to test sample program for Azure Data Explorer Usage: azdatax [command] Available Commands: create-mapping creates a mapping named StormEvents_CSV_Mapping create-table creates a table named StormEvents drop-table drops the StormEvents table get queries data from StormEvents help Help about any command ingest ingests CSV file from blob store list-tables lists tables Flags: -h, --help help for azdatax Use \"azdatax [command] --help\" for more information about a command. Let’s start by creating a table:\n./azdatax create-table --dbname \u003cname of the database you created initially\u003e //output Connected to Azure Data Explorer table StormEvents created To list tables:\n./azdatax list-tables --dbname \u003cname of the database you created initially\u003e //output Connected to Azure Data Explorer Table name: StormEvents, Database name: testkustodb To create the data mapping\n./azdatax create-mapping --dbname \u003cname of the database you created initially\u003e //output Connected to Azure Data Explorer mapping StormEvents_CSV_Mapping created To ingest data:\n./azdatax ingest --dbname \u003cname of the database you created initially\u003e //output Connected to Azure Data Explorer Ingested file from - https://kustosamplefiles.blob.core.windows.net/samplefiles/StormEvents.csv?st=2018-08-31T22%3A02%3A25Z\u0026se=2020-09-01T22%3A02%3A00Z\u0026sp=r\u0026sv=2018-03-28\u0026sr=b\u0026sig=LQIbomcKI8Ooz425hWtjeq6d61uEaq21UVX7YrM61N4%3D Wait for a while for the ingestion to complete before you try to query the data (next step)\nTo query data\n./azdatax get --dbname \u003cname of the database you created initially\u003e //output Connected to Azure Data Explorer StormEvents data.... +-------------------------------+-------------------------------+---------------------------+----------+ | START TIME | END TIME | FROM | DAMAGE | +-------------------------------+-------------------------------+---------------------------+----------+ | 2007-12-02 23:00:00 +0000 UTC | 2007-12-05 23:00:00 +0000 UTC | Official NWS Observations | 50000000 | | 2007-01-03 00:00:00 +0000 UTC | 2007-01-03 22:00:00 +0000 UTC | Newspaper | 20000 | | 2007-12-03 03:00:00 +0000 UTC | 2007-12-05 19:00:00 +0000 UTC | Official NWS Observations | 12000 | | 2007-01-03 00:00:00 +0000 UTC | 2007-01-03 22:00:00 +0000 UTC | Newspaper | 5000 | | 2007-03-12 00:00:00 +0000 UTC | 2007-03-13 23:00:00 +0000 UTC | Public | 0 | | 2007-03-12 00:00:00 +0000 UTC | 2007-03-14 23:00:00 +0000 UTC | Other Federal | 0 | +-------------------------------+-------------------------------+---------------------------+----------+ Finally, to drop the StormEvents table:\n./azdatax drop-table --dbname \u003cname of the database you created initially\u003e //output Connected to Azure Data Explorer Table StormEvents dropped ","setup-azure-data-explorer-cluster-create-a-database-and-configure-security#Setup Azure Data Explorer cluster, create a Database and configure security":"Start by creating a cluster using az kusto cluster create. Once that’s done, create a database with az kusto database create, e.g.\naz kusto cluster create -l \"Central US\" -n MyADXCluster -g MyADXResGrp --sku Standard_D11_v2 --capacity 2 az kusto database create --cluster-name MyADXCluster -g MyADXResGrp -n MyADXdb az kusto database show --cluster-name MyADXCluster --name MyADXdb --resource-group MyADXResGrp Create a Service Principal using az ad sp create-for-rbac\naz ad sp create-for-rbac -n \"test-datax-sp\" You will get a JSON response as such - please note down the appId, password and tenant as you will be using them in subsequent steps\n{ \"appId\": \"fe7280c7-5705-4789-b17f-71a472340429\", \"displayName\": \"test-datax-sp\", \"name\": \"http://test-datax-sp\", \"password\": \"29c719dd-f2b3-46de-b71c-4004fb6116ee\", \"tenant\": \"42f988bf-86f1-42af-91ab-2d7cd011db42\" } You will need to assign roles to the Service Principal so that it can access the database you just created. To do so using the Azure Portal, open the Azure Data Explorer cluster, navigate to Data \u003e Databases and select the database. Choose Permissions form the left menu and and click Add to proceed.\nFor more information, please refer to Secure Azure Data Explorer clusters in Azure","what-is-azure-data-explorer-#What is Azure Data Explorer ?":"Azure Data Explorer (also known as Kusto) is a fast and scalable data exploration service for analyzing large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. This data can then be used for diagnostics, monitoring, reporting, machine learning, and additional analytics capabilities.\nIt supports several ingestion methods, including connectors to common services like Event Hub, programmatic ingestion using SDKs, such as .NET and Python, and direct access to the engine for exploration purposes. It also integrates with analytics and modeling services for additional analysis and visualization of data using tools such as Power BI\nGo SDK for Azure Data Explorer The Go client SDK allows you to query, control and ingest into Azure Data Explorer clusters using Go. Please note that this is for interacting with the Azure Data Explorer cluster (and related components such as tables etc.). To create Azure Data Explorer clusters, databases etc. you should the use the admin component (control plane) SDK which is a part of the larger Azure SDK for Go\nAPI docs - https://godoc.org/github.com/Azure/azure-kusto-go\nBefore getting started, here is what you would need to try out the sample application"},"title":"Tutorial: Getting started with Azure Data Explorer using the Go SDK"},"/blog/golang-azure-data-explorer-manage/":{"data":{"":"Getting started with Azure Data Explorer using the Go SDK covered how to use the Azure Data Explorer Go SDK to ingest and query data from azure data explorer to ingest and query data. In this blog you will the Azure Go SDK to manage Azure Data Explorer clusters and databases.\nAzure Data Explorer (also known as Kusto) is a fast and scalable data exploration service for analyzing large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. This data can then be used for diagnostics, monitoring, reporting, machine learning, and additional analytics capabilities.\nIn case you’re wondering, we are talking about two different SDKs here. The one covered in this blog is for resource administration (also known as the control plane SDK) and the the one I used in the other post is data plane SDK for interacting with the Azure Data Explorer service itself (ingestion, query etc.)\nWhat’s covered ?\nA simple CLI application is used as an example to demonstrate how to use the Go SDK. We’ll try out the application first and go through how to:\nCreate and list Azure Data Explorer clusters Create and list databases in that cluster Delete the database and cluster $ ./goadx --help CLI to test sample program for Azure Data Explorer Usage: goadx [command] Available Commands: cluster create, list and delete Azure Data Explorer clusters db create, list and delete databases in an Azure Data Explorer cluster help Help about any command Flags: -h, --help help for goadx Use \"goadx [command] --help\" for more information about a command. Once that’s done, we’ll walk through the sample code to understand what’s going on\nThe code is available on GitHub https://github.com/abhirockzz/azure-go-sdk-for-dataexplorer\nPlease note that this CLI based example is just meant to showcase how to use the Azure Go SDK (in the context of Azure Data Explorer) as a part of a larger application. It is not supposed to replace/substitute the Azure CLI which can be used to manage Azure Data Explorer resources","code-walk-through#Code walk through":"At a high level, the application consists of two parts:\nAzure Data Explorer operations (create, read, delete) - part of the ops package CLI commands - part of the cli package To be honest, the CLI bit is not important (at-least in the context of the blog). It uses the (ubiquitous) Cobra package to implement the cluster and cluster (top-level) commands and the sub-commands - create, list and delete. If you’re interested, take a look at the following:\nroot.go - defines the root command for CLI entry point cluster.go - defines and implements sub-commands for cluster operations db.go - defines and implements sub-commands for database operations The CLI part simply invokes CRUD oeprations on Azure Data Explorer resources which is the important part. So, lets go through them\nAuthentication This is handled in ops/client.go. To get a handle for the client components to execute the required operations, we need to authenticate. For e.g. to get the kusto.ClustersClient:\nfunc getClustersClient(subscription string) kusto.ClustersClient { client := kusto.NewClustersClient(subscription) authR, err := auth.NewAuthorizerFromEnvironment() if err != nil { log.Fatal(err) } client.Authorizer = authR return client } The part to note is the auth.NewAuthorizerFromEnvironment() which looks for a set of pre-defined environment variables - this is User environment-based authentication\nCluster operations ops/cluster.go is where the cluster create, list and delete is handled. Here is the snippet for create:\nfunc CreateCluster(sub, clusterName, location, rgName string) { ... result, err := client.CreateOrUpdate(ctx, rgName, clusterName, kusto.Cluster{Location: \u0026location, Sku: \u0026kusto.AzureSku{Name: kusto.DevNoSLAStandardD11V2, Capacity: \u0026numInstances, Tier: kusto.Basic}}) err = result.WaitForCompletionRef(context.Background(), client.Client) r, err := result.Result(client) ... } CreateOrUpdate is used to initiate the cluster creation. Since this is a long running op, it returns a ClustersCreateOrUpdateFuture which we them wait on (using WaitForCompletionRef)\nNote that I have used context.Background(), you can use a different context e,g one with timeout or one which can be cancelled.\nOnce that’s one, we check the future for the Result\nListing all the clusters in a resource group is quite simple and can be done using ListByResourceGroup\nfunc ListClusters(sub, rgName string) kusto.ClusterListResult { ctx := context.Background() result, err := getClustersClient(sub).ListByResourceGroup(ctx, rgName) if err != nil { log.Fatal(err.Error()) } return result } Delete is similar to Create in the sense that it is a long running operation as well. Its the same drill - initiate the deletion (using Delete), wait for it to finish and then confirm the result\nfunc DeleteCluster(sub, clusterName, rgName string) { ... result, err := client.Delete(ctx, rgName, clusterName) err = result.WaitForCompletionRef(context.Background(), client.Client) r, err := result.Result(client) ... } Database operations ops/db.go is where the database create, list and delete is handled. Here is the snippet for create:\nfunc CreateDatabase(sub, rgName, clusterName, location, dbName string) { ... future, err := client.CreateOrUpdate(ctx, rgName, clusterName, dbName, kusto.ReadWriteDatabase{Kind: kusto.KindReadWrite, Location: \u0026location}) err = future.WaitForCompletionRef(context.Background(), client.Client) r, err := future.Result(client) kdb, _ := r.Value.AsReadWriteDatabase() ... } Listing is straightforward and taken care of by ListByCluster\nfunc ListDBs(sub, rgName, clusterName string) kusto.DatabaseListResult { ctx := context.Background() result, err := getDBClient(sub).ListByCluster(ctx, rgName, clusterName) if err != nil { log.Fatal(\"failed to get databases in cluster\", err) } return result } And finally, the deletion which is handled using Delete\nfunc DeleteDB(sub, rgName, clusterName, dbName string) { ... future, err := getDBClient(sub).Delete(ctx, rgName, clusterName, dbName) err = future.WaitForCompletionRef(context.Background(), client.Client) r, err := future.Result(client) ... } That’s it for the code walkthrough!","conclusion#Conclusion":"Hopefully this was helpful in demonstrating how to use the Azure Go SDK to work with Azure Data Explorer resources. You can check the documentation for Python, .NET and other examples. Happy exploring!","pre-requisites#Pre-requisites":"Install Go 1.13 or above\nYou will need a Microsoft Azure account. Go ahead and sign up for a free one!\nInstall the Azure CLI if you don’t have it already (should be quick!)","run-the-cli-application#Run the CLI application":"Get the code and build it:\ngit clone https://github.com/abhirockzz/azure-go-sdk-for-dataexplorer cd azure-go-sdk-for-dataexplorer go build -o goadx //to confirm chmod a+x goadx \u0026\u0026 ./goadx To see the details for individual commands, e.g. for cluster creation:\n./goadx cluster create --help //output Creates 1 instance of compute type DevNoSLAStandardD11V2 in Basic tier Usage: goadx cluster create [flags] Flags: -h, --help help for create --loc string ADX cluster location --name string ADX cluster name Global Flags: --rg string Azure resource group --sub string Azure subscription Few more steps before creating the cluster… Create a resource group ….\n… using az group create CLI command\naz group create -l \u003cregion\u003e -n \u003cname\u003e e.g. az group create -l southeastasia -n my-adx-rg We need a Service Principal for the Go SDK to authenticate with Azure Data Explorer service to execute cluster and database operations.\nCreate a Service Principal…\n…using az ad sp create-for-rbac\naz ad sp create-for-rbac -n \"test-datax-sp\" You will get a JSON response as such\n{ \"appId\": \"fe7280c7-5705-4789-b17f-71a472340429\", \"displayName\": \"test-datax-sp\", \"name\": \"http://test-datax-sp\", \"password\": \"29c719dd-f2b3-46de-b71c-4004fb6116ee\", \"tenant\": \"42f988bf-86f1-42af-91ab-2d7cd011db42\" } Set the service principal details as environment variables. During the code walk-through, you will see why these specific variables are required\nexport AZURE_CLIENT_ID=\"\u003cappId\u003e\" export AZURE_CLIENT_SECRET=\"\u003cpassword\u003e\" export AZURE_TENANT_ID=\"\u003ctenant\u003e\" You are all set!\nCreate an Azure Data Explorer cluster ./goadx cluster create --name \u003cname of the cluster you want to create\u003e --loc \u003cazure region for the cluster\u003e --rg \u003cname of the resource group\u003e --sub \u003cazure subscription id\u003e For example:\n./goadx cluster create --name MyAdxCluster --loc \"Southeast Asia\" --rg my-adx-rg --sub 9a42a42f-ae42-4242-b6a7-eea0ea42d342 This will create a single instance cluster with the DevNoSLAStandardD11V2 VM\nthis is hard coded for sake of simplicity/ease-of-use\nThe cluster creation will take some time, and the code blocks for that time period. Please be patient ;) Grab some coffee, check Twitter or do whatever keeps you busy for about ~10-15 mins. Once the cluster is created, you will see this message:\ncreated cluster MyAdxCluster with ID /subscriptions/9a42a42f-ae42-4242-b6a7-eea0ea42d342/resourceGroups/my-adx-rg/providers/Microsoft.Kusto/Clusters/MyAdxCluster and type Microsoft.Kusto/Clusters List down all the clusters You just created a cluster, let’s make sure you can get its info (including any other cluster you might already have in your resource group)\n./goadx cluster list --rg \u003cname of the resource group\u003e --sub \u003cazure subscription id\u003e for example\n./goadx cluster list --rg my-adx-rg --sub 9a42a42f-ae42-4242-b6a7-eea0ea42d342 You will get a tabular output as such:\n+---------------+---------+----------------+-----------+-------------------------------------------------------+ | NAME | STATE | LOCATION | INSTANCES | URI | +---------------+---------+----------------+-----------+-------------------------------------------------------+ | MyAdxCluster | Running | Southeast Asia | 1 | https://MyAdxCluster.southeastasia.kusto.windows.net | +---------------+---------+----------------+-----------+-------------------------------------------------------+ You have the cluster, its time to create a database To create a database in the cluster\n./goadx db create --name \u003cname of the database you want to create\u003e --cluster \u003cnname of the adx cluster\u003e --loc \u003cazure region\u003e --rg \u003cresource group name\u003e --sub \u003cazure subcription id\u003e for example\n./goadx db create --name testadxdb --cluster MyAdxCluster --loc \"Southeast Asia\" --rg my-adx-rg --sub 9a42a42f-ae42-4242-b6a7-eea0ea42d342 It shouldnt take a too long. Once the DB is created, you will see this message:\ncreated DB MyAdxCluster/testadxdb with ID /subscriptions/9a42a42f-ae42-4242-b6a7-eea0ea42d342/resourceGroups/my-adx-rg/providers/Microsoft.Kusto/Clusters/MyAdxCluster/Databases/testadxdb and type Microsoft.Kusto/Clusters/Databases To check the DB you just created … ./goadx db list --cluster \u003cname of the adx cluster\u003e --rg \u003cresource group name\u003e --sub \u003cazure subscription id\u003e for example\n./goadx db list --cluster MyAdxCluster --rg my-adx-rg --sub 9a42a42f-ae42-4242-b6a7-eea0ea42d342 You will get a tabular output as such:\n+---------------------------+-----------+----------------+------------------------------------+ | NAME | STATE | LOCATION | TYPE | +---------------------------+-----------+----------------+------------------------------------+ | MyAdxCluster/testadxdb | Succeeded | Southeast Asia | Microsoft.Kusto/Clusters/Databases | +---------------------------+-----------+----------------+------------------------------------+ You can continue experimenting further and create more clusters and/or databases\nTime to clean up … You can delete the cluster directly as such:\n./goadx cluster delete --name \u003cname of adx cluster\u003e --rg \u003cresource group name\u003e --sub \u003cazure subcription id\u003e example:\n./goadx cluster delete --name MyAdxCluster --rg my-adx-rg --sub 9a42a42f-ae42-4242-b6a7-eea0ea42d342 You should see a confirmation message indicating that the cluster has been deleted:\ndeleted ADX cluster MyAdxCluster from resource group my-adx-rg Or you could just delete the database itself\n./goadx db delete --name \u003cdatabase name\u003e --cluster \u003cadx cluster name\u003e --rg \u003cresource group name\u003e --sub \u003cazure subscription id\u003e For example\n./goadx db delete --name testadxdb --cluster MyAdxCluster --rg my-adx-rg --sub 9a42a42f-ae42-4242-b6a7-eea0ea42d342 You should see a confirmation message indicating that the database has been deleted:\ndeleted DB testadxdb from cluster MyAdxCluster Alright, now that you what can be done, it’s time to see how its done!"},"title":"How to use Azure Go SDK to manage Azure Data Explorer clusters"},"/blog/golang-serverless-backend/":{"data":{"":"Webhook backend is a popular use case for FaaS (Functions-as-a-service) platforms. They could be used for many use cases such as sending customer notifications to responding with funny GIFs! Using a Serverless function, it’s quite convenient to encapsulate the webhook functionality and expose it in the form of an HTTP endpoint. In this tutorial you will learn how to implement a Slack app as a Serverless backend using Azure Functions and Go. You can extend the Slack platform and integrate services by implementing custom apps or workflows that have access to the full scope of the platform allowing you to build powerful experiences in Slack.\nThis is a simpler version of the Giphy for Slack. The original Giphy Slack app works by responding with multiple GIFs in response to a search request. For the sake of simplicity, the function app demonstrated in this post just returns a single (random) image corresponding to a search keyword using the Giphy Random API. This post provides a step-by-step guide to getting the application deployed to Azure Functions and integrating it with your Slack workspace.\nIn this post, you will:\nGet an overview of Custom Handlers in Azure Functions Understand what’s going on behind the scenes with a brief code walk through Learn how to setup the solution using configure Azure Functions and Slack and of course, run your Slack app in the workspace! The backend function logic is written in Go (the code isavailable on GitHub. Those who have worked with Azure Functions might recall that Go is not one of the language handlers that is supported by default. That’s where Custom Handlers come to the rescue!","azure-functions-setup#Azure Functions setup":"Start by creating a Resource Group to host all the components of the solution.\nCreate a Function App Start by searching for Function App in the Azure Portal and click Add\nEnter the required details: you should select Custom Handler as the Runtime stack\nIn the Hosting section, choose Linux and Consumption (Serverless) for Operating system and Plan type respectively.\nEnable Application Insights (if you need to)\nReview the final settings and click Create to proceed\nOnce the process is complete, the following resource will also be created along with the Function App:\nApp Service plan (a Consumption/Serverless plan in this case) An Azure Storage account An Azure Application Insights function) Deploy the function Clone the GitHub repo and build the function\ngit clone https://github.com/abhirockzz/serverless-go-slack-app cd serverless-go-slack-app GOOS=linux go build -o go_funcy cmd/main.go GOOS=linux is used to build a Linux executable since we chose a Linux OS for our Function App\nTo deploy, use the Azure Functions core tools CLI\nfunc azure functionapp publish \u003center name of the function app\u003e Once you’ve deployed, copy the function URL that’s returned by the command - you will use it in subsequent steps","configure-slack#Configure Slack":"This section will cover the steps you need to execute to setup the Slack application (Slash command) in your workspace:\nCreate a Slack app Create a Slash Command Install the app to your workspace Create a Slack App and Slash command Sign into your Slack Workspace and start by creating a new Slack App\nClick on Create New Command to define your new Slash Command with the required information. Please note that the Request URL field is the one where you will enter the HTTP endpoint of function which is nothing but the URL you obtained after deploying the function in the previous section. Once you’re done, hit Save to finish.\nInstall the app to your workspace Once you’re done creating the Slash Command, head to your app’s settings page, click the Basic Information feature in the navigation menu, choose Install your app to your workspace and click Install App to Workspace - this will install the app to your Slack workspace to test your app and generate the tokens you need to interact with the Slack API. As soon as you finish installing the app, the App Credentials will show up on the same page.\nMake a note of your app Signing Secret as you’ll be using it later\nBefore moving on to the fun part … … make sure to update the Function App configuration to add the Slack Signing Secret (SLACK_SIGNING_SECRET) and Giphy API key (GIPHY_API_KEY) - they will be available as environment variables inside the function.","funcy-time#fun(cy) time!":"From your Slack workspace, invoke the command /funcy \u003csearch term\u003e. For e.g. try /funcy dog. You should get back a random GIF in return!\nJust a recap of what’s going on: When you invoke the /funcy command in Slack, it calls the function, which then interacts Giphy API and finally returning the GIF to the user (if all goes well!)\nYou may see timeout error from Slack after the first invocation. This is most likely due to the cold start where the function takes a few seconds to bootstrap when you invoke it for the very first time. This is combined with the fact that Slack expects a response in 3 seconds - hence the error message.\nThere is nothing to worry about. All you need is to retry again and things should be fine!\nClean up: Once you’re done, don’t forget to delete the resource group which in turn will delete all the resources created before (Function app, App Service Plan etc.)\nThere is nothing stopping you from using Go for your serverless functions on Azure! I hope this turns out to be a fun way to try out Custom Handlers. Let us know what you think!","overview#Overview":"Before we dive into the other areas, it might help to understand the nitty gritty by exploring the code (which is relatively simple by the way)\nApplication structure Let’s look at the how the app is setup. this is as defined in the doc\n. ├── cmd │ └── main.go ├── funcy │ └── function.json ├── go.mod ├── host.json └── pkg └── function ├── function.go ├── giphy.go └── slack.go The function.json file is a located in a folder whose name is used the function name (this is by convention) { \"bindings\": [ { \"type\": \"httpTrigger\", \"direction\": \"in\", \"name\": \"req\", \"methods\": [ \"get\", \"post\" ] }, { \"type\": \"http\", \"direction\": \"out\", \"name\": \"res\" } ] } host.json tells the Functions host where to send requests by pointing to a web server capable of processing HTTP events. Notice the customHandler.description.defaultExecutablePath which defines that go_funcy is the name of the executable that’ll be used to run the web server. \"enableForwardingHttpRequest\": true ensures that the raw HTTP data is sent to the custom handlers without any modifications { \"version\": \"2.0\", \"extensionBundle\": { \"id\": \"Microsoft.Azure.Functions.ExtensionBundle\", \"version\": \"[1.*, 2.0.0)\" }, \"customHandler\": { \"description\": { \"defaultExecutablePath\": \"go_funcy\" }, \"enableForwardingHttpRequest\": true }, \"logging\": { \"logLevel\": { \"default\": \"Trace\" } } } The cmd and pkg directories contain the Go source code. Let’s explore this in the next sub-section Code walk through cmd/main.go sets up and starts the HTTP server. Notice that the /api/funcy endpoint is the one which the Function host sends the request to the custom handler HTTP server.\nfunc main() { port, exists := os.LookupEnv(\"FUNCTIONS_CUSTOMHANDLER_PORT\") if !exists { port = \"8080\" } http.HandleFunc(\"/api/funcy\", function.Funcy) log.Fatal(http.ListenAndServe(\":\"+port, nil)) } All the heavy lifting is done in function/function.go.\nThe first part is to read the request body (from Slack) and ensure its integrity via a signature validation process based on this recipe defined by Slack.\nsigningSecret := os.Getenv(\"SLACK_SIGNING_SECRET\") apiKey := os.Getenv(\"GIPHY_API_KEY\") if signingSecret == \"\" || apiKey == \"\" { http.Error(w, \"Failed to process request. Please contact the admin\", http.StatusUnauthorized) return } slackTimestamp := r.Header.Get(\"X-Slack-Request-Timestamp\") b, err := ioutil.ReadAll(r.Body) if err != nil { http.Error(w, \"Failed to process request\", http.StatusBadRequest) return } slackSigningBaseString := \"v0:\" + slackTimestamp + \":\" + string(b) slackSignature := r.Header.Get(\"X-Slack-Signature\") if !matchSignature(slackSignature, signingSecret, slackSigningBaseString) { http.Error(w, \"Function was not invoked by Slack\", http.StatusForbidden) return } Once we’ve confirmed that the function has indeed being invoked via Slack, the next part is to extract the search term entered by the (Slack) user\nvals, err := parse(b) if err != nil { http.Error(w, \"Failed to process request\", http.StatusBadRequest) return } giphyTag := vals.Get(\"text\") Look up for GIFs with the search term by invoking the GIPHY REST API\ngiphyResp, err := http.Get(\"http://api.giphy.com/v1/gifs/random?tag=\" + giphyTag + \"\u0026api_key=\" + apiKey) if err != nil { http.Error(w, \"Failed to process request\", http.StatusFailedDependency) return } resp, err := ioutil.ReadAll(giphyResp.Body) if err != nil { http.Error(w, \"Failed to process request\", http.StatusInternalServerError) return } Un-marshal the response sent back by the GIPHY API, convert it into a form which Slack can make sense of and return it. That’s it !\nvar gr GiphyResponse json.Unmarshal(resp, \u0026gr) title := gr.Data.Title url := gr.Data.Images.Downsized.URL slackResponse := SlackResponse{Text: slackResponseStaticText, Attachments: []Attachment{{Text: title, ImageURL: url}}} w.Header().Set(\"Content-Type\", \"application/json\") json.NewEncoder(w).Encode(slackResponse) fmt.Println(\"Sent response to Slack\") Check the matchSignature function if you’re interested in checking the signature validation process and look at slack.go, giphy.go (in the function directory) to see the Go structs used represent information (JSON) being exchanged between various components. These have not been included here to keep this post concise.\nAlright! So far, we have covered lots of theory and background info. It’s time to get things done! Before you proceed, ensure that you take care of the below mentioned pre-requisites.","pre-requisites#Pre-requisites":" Download and install Go if you don’t have it already Install Azure functions Core Tools - this will allow you to deploy the function using a CLI (and also run it test and debug it locally) Create a Slack workspace if you don’t have one. Get a GIPHY API key - You need to create a GIHPY account (it’s free!) and create an app. Each application you create will have its own API Key. Please note down your GIPHY API key as you will be using it later\nThe upcoming sections will guide you through the process of deploying the Azure Function and configuring the Slack for the Slash command.","what-are-custom-handlers#What are Custom Handlers?":"In a nutshell, a Custom Handler is a lightweight web server that receive events from the Functions host. The only thing you need to implement a Custom Handler in your favorite runtime/language is: HTTP support! This does not mean that Custom handlers are restricted to HTTP triggers only - you are free to use other triggers along with input and output bindings via extension bundles.\nHere is a summary of how Custom Handlers work at a high level (the diagram below has been picked from the documentation)\nAn event trigger (via HTTP, Storage, Event Hubs etc.) invokes the Functions host. The way Custom Handlers differ from traditional functions is that the Functions host acts as a middle man: it issues a request payload to the web server of the Custom Handler (the function) along with a payload that contains trigger, input binding data and other metadata for the function. The function returns a response back to the Functions host which passes data from the response to the function’s output bindings for processing."},"title":"Build a Serverless app using Go and Azure Functions"},"/blog/how-to-configure-and-customize-the-go-sdk-for-azure-cosmos-db/":{"data":{"":"\nWhen building applications that interact with databases, developers often need to customize the SDK behavior to address real-world challenges like network instability, performance bottlenecks, debugging complexity, monitoring requirements, and more. This is especially true when working with a massively scalable, cloud-native, distributed database like Azure Cosmos DB.","conclusion#Conclusion":"The Go SDK for Azure Cosmos DB is designed to be flexible and customizable, allowing you to tailor it to your specific needs.In this blog, we covered how to configure and customize the Go SDK for Azure Cosmos DB. We looked at retry policies, HTTP-level customizations, OpenTelemetry support, and how to access metrics.\nFor more information, refer to the package documentation and the GitHub repository. I hope you find this useful!","http-level-customizations#HTTP-level customizations":"There are scenarios where you may need to customize the HTTP client used by the SDK. For example, when using the Cosmos DB emulator locally, you want to skip certificate verification to connect without SSL errors during development or testing.\nTLSClientConfig allows you to customize TLS settings for the HTTP client and setting InsecureSkipVerify: true disables certificate verification – useful for local testing but insecure for production.\nfunc customHTTP1() { // Create a custom HTTP client with a timeout client := \u0026http.Client{ Transport: \u0026http.Transport{ TLSClientConfig: \u0026tls.Config{InsecureSkipVerify: true}, }, } clientOptions := \u0026azcosmos.ClientOptions{ ClientOptions: azcore.ClientOptions{ Transport: client, }, } c, err := auth.GetEmulatorClientWithAzureADAuth(\"http://localhost:8081\", clientOptions) if err != nil { log.Fatal(err) } _, err = c.CreateDatabase(context.Background(), azcosmos.DatabaseProperties{ID: \"test\"}, nil) if err != nil { log.Fatal(err) } } All you need to do is pass the custom HTTP client to the ClientOptions struct when creating the Cosmos DB client. The SDK will use this for all requests.\nAnother scenario is when you want to set a custom header for all requests to track requests or add metadata. All you need to do is implement the Do method of the policy.Policy interface and set the header in the request:\ntype CustomHeaderPolicy struct{} func (c *CustomHeaderPolicy) Do(req *policy.Request) (*http.Response, error) { correlationID := uuid.New().String() req.Raw().Header.Set(\"X-Correlation-ID\", correlationID) return req.Next() } Looking at the logs, notice the custom header X-Correlation-ID is added to each request:\n//... Request Event: ==\u003e OUTGOING REQUEST (Try=1) GET https://ACCOUNT_NAME.documents.azure.com:443/ Authorization: REDACTED User-Agent: azsdk-go-azcosmos/v1.3.0 (go1.23.6; darwin) X-Correlation-Id: REDACTED X-Ms-Cosmos-Sdk-Supportedcapabilities: 1 X-Ms-Date: Tue, 06 May 2025 04:27:37 GMT X-Ms-Version: 2020-11-05 Request Event: ==\u003e OUTGOING REQUEST (Try=1) POST https://ACCOUNT_NAME-region.documents.azure.com:443/dbs Authorization: REDACTED Content-Length: 27 Content-Type: application/query+json User-Agent: azsdk-go-azcosmos/v1.3.0 (go1.23.6; darwin) X-Correlation-Id: REDACTED X-Ms-Cosmos-Sdk-Supportedcapabilities: 1 X-Ms-Date: Tue, 06 May 2025 04:27:37 GMT X-Ms-Documentdb-Query: True X-Ms-Version: 2020-11-05 //.... ","introduction#Introduction":"When building applications that interact with databases, developers frequently encounter scenarios where default SDK configurations don’t align with their specific operational requirements. They need to customize SDK behavior to address real-world challenges like network instability, performance bottlenecks, debugging complexity, monitoring requirements, and more. These factors become even more pronounced when working with a massively scalable, cloud-native, distributed database like Azure Cosmos DB.\nThis blog post explores how to customize and configure the Go SDK for Azure Cosmos DB beyond its default settings, covering techniques for modifying client behavior, implementing custom policies, accessing operational metrics, etc. These enable developers to build more resilient applications, troubleshoot issues effectively, and gain deeper insights into their database interactions.\nThe Go SDK for Azure Cosmos DB is built on top of the core Azure Go SDK package, which implements several patterns that are applied throughout the SDK. The core SDK is designed to be quite customizable, and its configurations can be applied with the ClientOptions struct when creating a new Cosmos DB client object using NewClient (and other similar functions). If you peek inside the azcore.ClientOptions struct, you will notice that it has many options for configuring the HTTP client, retry policies, timeouts, and other settings.\nLets dive into how to make use of (and extend) these common options when building Go applications with Azure Cosmos DB.\nI have provided code snippets throughout this blog. Refer to this GitHub repository for runnable examples.","opentelemetry-support#OpenTelemetry support":"The Azure Go SDK supports distributed tracing via OpenTelemetry. This allows you to collect, export, and analyze traces for requests made to Azure services, including Cosmos DB.\nThe azotel package is used to connect an instance of OpenTelemetry’s TracerProvider to an Azure SDK client (in this case Cosmos DB). You can then configure the TracingProvider in azcore.ClientOptions to enable automatic propagation of trace context and emission of spans for SDK operations.\nfunc getClientOptionsWithTracing() (*azcosmos.ClientOptions, *trace.TracerProvider) { exporter, err := stdouttrace.New(stdouttrace.WithPrettyPrint()) if err != nil { log.Fatalf(\"failed to initialize stdouttrace exporter: %v\", err) } tp := trace.NewTracerProvider(trace.WithBatcher(exporter)) otel.SetTracerProvider(tp) op := azcosmos.ClientOptions{ ClientOptions: policy.ClientOptions{ TracingProvider: azotel.NewTracingProvider(tp, nil), }, } return \u0026op, tp } The above function creates a stdout exporter for OpenTelemetry (prints traces to the console). It sets up a TracerProvider, registers this as the global tracer, and returns a ClientOptions struct with the TracingProvider set, ready to be used with the Cosmos DB client.\nfunc tracing() { op, tp := getClientOptionsWithTracing() defer func() { _ = tp.Shutdown(context.Background()) }() c, err := auth.GetClientWithDefaultAzureCredential(\"https://ACCOUNT_NAME.documents.azure.com:443/\", op) //.... container, err := c.NewContainer(\"existing_db\", \"existing_container\") if err != nil { log.Fatal(err) } //ctx := context.Background() tracer := otel.Tracer(\"tracer_app1\") ctx, span := tracer.Start(context.Background(), \"query-items-operation\") defer span.End() query := \"SELECT * FROM c\" pager := container.NewQueryItemsPager(query, azcosmos.NewPartitionKey(), nil) for pager.More() { queryResp, err := pager.NextPage(ctx) if err != nil { log.Fatal(\"query items failed:\", err) } for _, item := range queryResp.Items { log.Printf(\"Queried item: %+v\\n\", string(item)) } } } The above function calls getClientOptionsWithTracing to get tracing-enabled options and a tracer provider and ensures the tracer provider is shut down at the end (flushes traces). It creates a Cosmos DB client with tracing enabled, executes an operation to query items in a container. The SDK call is traced automatically, and exported to stdout in this case.\nYou can plug in any OpenTelemetry-compatible tracer provider and traces can be exported to various backend. Here is a snippet for Jaeger exporter.\nThe traces are quite large, so here is a small snippet of the trace output. Check the query_items_trace.txt file in the repo for the full trace output:\n//... { \"Name\": \"query_items democontainer\", \"SpanContext\": { \"TraceID\": \"39a650bcd34ff70d48bbee467d728211\", \"SpanID\": \"f2c892bec75dbf5d\", \"TraceFlags\": \"01\", \"TraceState\": \"\", \"Remote\": false }, \"Parent\": { \"TraceID\": \"39a650bcd34ff70d48bbee467d728211\", \"SpanID\": \"b833d109450b779b\", \"TraceFlags\": \"01\", \"TraceState\": \"\", \"Remote\": false }, \"SpanKind\": 3, \"StartTime\": \"2025-05-06T17:59:30.90146+05:30\", \"EndTime\": \"2025-05-06T17:59:36.665605042+05:30\", \"Attributes\": [ { \"Key\": \"db.system\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"cosmosdb\" } }, { \"Key\": \"db.cosmosdb.connection_mode\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"gateway\" } }, { \"Key\": \"db.namespace\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"demodb-gosdk3\" } }, { \"Key\": \"db.collection.name\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"democontainer\" } }, { \"Key\": \"db.operation.name\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"query_items\" } }, { \"Key\": \"server.address\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"ACCOUNT_NAME.documents.azure.com\" } }, { \"Key\": \"az.namespace\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"Microsoft.DocumentDB\" } }, { \"Key\": \"db.cosmosdb.request_charge\", \"Value\": { \"Type\": \"STRING\", \"Value\": \"2.37\" } }, { \"Key\": \"db.cosmosdb.status_code\", \"Value\": { \"Type\": \"INT64\", \"Value\": 200 } } ], //.... Refer to Semantic Conventions for Microsoft Cosmos DB","query-and-index-metrics#Query and Index Metrics":"The Go SDK provides a way to access query and index metrics, which can help you optimize your queries and understand their performance characteristics.\nQuery Metrics When executing queries, you can get basic metrics about the query execution. The Go SDK provides a way to access these metrics through the QueryResponse struct in the QueryItemsResponse object. This includes information about the query execution, including the number of documents retrieved, etc.\nfunc queryMetrics() { //.... container, err := c.NewContainer(\"existing_db\", \"existing_container\") if err != nil { log.Fatal(err) } query := \"SELECT * FROM c\" pager := container.NewQueryItemsPager(query, azcosmos.NewPartitionKey(), nil) for pager.More() { queryResp, err := pager.NextPage(context.Background()) if err != nil { log.Fatal(\"query items failed:\", err) } log.Println(\"query metrics:\\n\", *queryResp.QueryMetrics) //.... } } The query metrics are provided as a simple raw string in a key-value format (semicolon-separated), which is very easy to parse. Here is an example:\ntotalExecutionTimeInMs=0.34;queryCompileTimeInMs=0.04;queryLogicalPlanBuildTimeInMs=0.00;queryPhysicalPlanBuildTimeInMs=0.02;queryOptimizationTimeInMs=0.00;VMExecutionTimeInMs=0.07;indexLookupTimeInMs=0.00;instructionCount=41;documentLoadTimeInMs=0.04;systemFunctionExecuteTimeInMs=0.00;userFunctionExecuteTimeInMs=0.00;retrievedDocumentCount=9;retrievedDocumentSize=1251;outputDocumentCount=9;outputDocumentSize=2217;writeOutputTimeInMs=0.02;indexUtilizationRatio=1.00 Here is a breakdown of the metrics you can obtain from the query response:\n| Metric | Unit | Description | | ------------------------------ | ----- | ------------------------------------------------------------ | | totalExecutionTimeInMs | ms | Total time taken to execute the query, including all phases. | | queryCompileTimeInMs | ms | Time spent compiling the query. | | queryLogicalPlanBuildTimeInMs | ms | Time spent building the logical plan for the query. | | queryPhysicalPlanBuildTimeInMs | ms | Time spent building the physical plan for the query. | | queryOptimizationTimeInMs | ms | Time spent optimizing the query. | | VMExecutionTimeInMs | ms | Time spent executing the query in the Cosmos DB VM. | | indexLookupTimeInMs | ms | Time spent looking up indexes. | | instructionCount | count | Number of instructions executed for the query. | | documentLoadTimeInMs | ms | Time spent loading documents from storage. | | systemFunctionExecuteTimeInMs | ms | Time spent executing system functions in the query. | | userFunctionExecuteTimeInMs | ms | Time spent executing user-defined functions in the query. | | retrievedDocumentCount | count | Number of documents retrieved by the query. | | retrievedDocumentSize | bytes | Total size of documents retrieved. | | outputDocumentCount | count | Number of documents returned as output. | | outputDocumentSize | bytes | Total size of output documents. | | writeOutputTimeInMs | ms | Time spent writing the output. | | indexUtilizationRatio | ratio | Ratio of index utilization (1.0 means fully utilized). | Index Metrics Indexing metrics shows both utilized indexed paths and recommended indexed paths. You can use the indexing metrics to optimize query performance, especially in cases where you aren’t sure how to modify the indexing policy.\nTo enable indexing metrics in Go SDK, set PopulateIndexMetrics to true in the QueryOptions. Index metrics data in the QueryItemsResponse is base64 encoded and needs to be decoded before they can be used.\nEnabling indexing metrics incurs overhead, so it should be done only when debugging slow queries and not recommended in production.\npager := container.NewQueryItemsPager(\"SELECT c.id FROM c WHERE CONTAINS(LOWER(c.description), @word)\", azcosmos.NewPartitionKey(), \u0026azcosmos.QueryOptions{ PopulateIndexMetrics: true, QueryParameters: []azcosmos.QueryParameter{ { Name: \"@word\", Value: \"happy\", }, }, }) if pager.More() { page, _ := pager.NextPage(context.Background()) // process results decoded, _ := base64.StdEncoding.DecodeString(*page.IndexMetrics) log.Println(\"Index metrics\", string(decoded)) } Once decoded, the index metrics are available in JSON format. For example:\n{ \"UtilizedSingleIndexes\": [ { \"FilterExpression\": \"\", \"IndexSpec\": \"/description/?\", \"FilterPreciseSet\": true, \"IndexPreciseSet\": true, \"IndexImpactScore\": \"High\" } ], \"PotentialSingleIndexes\": [], \"UtilizedCompositeIndexes\": [], \"PotentialCompositeIndexes\": [] } ","retry-policies#Retry policies":"Common retry scenarios are handled in the SDK. Here is a summary of errors for which retries are attempted:\nError Type / Status Code Retry Logic Network Connection Errors Retry after marking endpoint unavailable and waiting for defaultBackoff. 403 Forbidden (with specific substatuses) Retry after marking endpoint unavailable and updating the endpoint manager. 404 Not Found (specific substatus) Retry by switching to another session or endpoint. 503 Service Unavailable Retry by switching to another preferred location. You can explore the source code in cosmos_client_retry_policy.go if you want to see the details of how the retry policy is implemented.\nLet’s see some of these in action.\nNon-Retriable Errors When a request fails with a non-retriable error, the SDK does not retry the operation. This is useful for scenarios where the error indicates that the operation cannot succeed.\nFor example, here is a function that tries to read a database that does not exist.\nfunc retryPolicy1() { c, err := auth.GetClientWithDefaultAzureCredential(\"https://demodb.documents.azure.com:443/\", nil) if err != nil { log.Fatal(err) } azlog.SetListener(func(cls azlog.Event, msg string) { // Log retry-related events switch cls { case azlog.EventRetryPolicy: fmt.Printf(\"Retry Policy Event: %s\\n\", msg) } }) // Set logging level to include retries azlog.SetEvents(azlog.EventRetryPolicy) db, err := c.NewDatabase(\"i_dont_exist\") if err != nil { log.Fatal(\"NewDatabase call failed\", err) } _, err = db.Read(context.Background(), nil) if err != nil { log.Fatal(\"Read call failed: \", err) } } The azcore logging implementation is configured using SetListener and SetEvents to write retry policy event logs to standard output.\nSee Logging section in azcosmos package README for details.\nLet’s look at the logs generated when this code is run:\n//.... Retry Policy Event: exit due to non-retriable status code Retry Policy Event: =====\u003e Try=1 for GET https://demodb.documents.azure.com:443/dbs/i_dont_exist Retry Policy Event: response 404 Retry Policy Event: exit due to non-retriable status code Read call failed: GET https://demodb-region.documents.azure.com:443/dbs/i_dont_exist -------------------------------------------------------------------------------- RESPONSE 404: 404 Not Found ERROR CODE: 404 Not Found //... When a request is made to read a non-existent database, the SDK gets a 404 (not found) response for the database. This is recognized as a non-retriable error and the SDK stops retrying. Retries are only performed for retriable errors (like network issues or certain status codes). The operation failed because the database does not exist.\nRetriable Errors When a request fails with a retriable error, the SDK automatically retries the operation based on the retry policy. This is useful for transient errors that may resolve themselves after a few attempts.\nThis function tries to create a Cosmos DB client using an invalid account endpoint. It sets up logging for retry policy events and attempts to create a database.\nfunc retryPolicy2() { c, err := auth.GetClientWithDefaultAzureCredential(\"https://iamnothere.documents.azure.com:443/\", nil) if err != nil { log.Fatal(err) } azlog.SetListener(func(cls azlog.Event, msg string) { // Log retry-related events switch cls { case azlog.EventRetryPolicy: fmt.Printf(\"Retry Policy Event: %s\\n\", msg) } }) // Set logging level to include retries azlog.SetEvents(azlog.EventRetryPolicy) _, err = c.CreateDatabase(context.Background(), azcosmos.DatabaseProperties{ID: \"test\"}, nil) if err != nil { log.Fatal(err) } } Let’s look at the logs generated when this code is run, and see show how the SDK handles retries when the endpoint is unreachable:\n//.... Retry Policy Event: error Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host Retry Policy Event: End Try #1, Delay=682.644105ms Retry Policy Event: =====\u003e Try=2 for GET https://iamnothere.documents.azure.com:443/ Retry Policy Event: error Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host Retry Policy Event: End Try #2, Delay=2.343322179s Retry Policy Event: =====\u003e Try=3 for GET https://iamnothere.documents.azure.com:443/ Retry Policy Event: error Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host Retry Policy Event: End Try #3, Delay=7.177314269s Retry Policy Event: =====\u003e Try=4 for GET https://iamnothere.documents.azure.com:443/ Retry Policy Event: error Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host Retry Policy Event: MaxRetries 3 exceeded failed to retrieve account properties: Get \"https://iamnothere.docume Each failed attempt is logged, and the SDK retries the operation several times (three times to be specific), with increasing delays between attempts. After exceeding the maximum number of retries, the operation fails with an error indicating the host could not be found - the SDK automatically retries transient network errors before giving up.\nBut you don’t have to stick to the default retry policy. You can customize the retry policy by setting the azcore.ClientOptions when creating the Cosmos DB client.\nConfigurable Retries Let’s say you want to set a custom retry policy with a maximum of two retries and a delay of one second between retries. You can do this by creating a policy.RetryOptions struct and passing it to the azcosmos.ClientOptions when creating the client.\nfunc retryPolicy3() { retryPolicy := policy.RetryOptions{ MaxRetries: 2, RetryDelay: 1 * time.Second, } opts := azcosmos.ClientOptions{ ClientOptions: policy.ClientOptions{ Retry: retryPolicy, }, } c, err := auth.GetClientWithDefaultAzureCredential(\"https://iamnothere.documents.azure.com:443/\", \u0026opts) if err != nil { log.Fatal(err) } log.Println(c.Endpoint()) azlog.SetListener(func(cls azlog.Event, msg string) { // Log retry-related events switch cls { case azlog.EventRetryPolicy: fmt.Printf(\"Retry Policy Event: %s\\n\", msg) } }) azlog.SetEvents(azlog.EventRetryPolicy) _, err = c.CreateDatabase(context.Background(), azcosmos.DatabaseProperties{ID: \"test\"}, nil) if err != nil { log.Fatal(err) } } Each failed attempt is logged, and the SDK retries the operation according to the custom policy — only two retries, with a 1-second delay after the first attempt and a longer delay after the second. After reaching the maximum number of retries, the operation fails with an error indicating the host could not be found.\nRetry Policy Event: =====\u003e Try=1 for GET https://iamnothere.documents.azure.com:443/ //.... Retry Policy Event: error Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host Retry Policy Event: End Try #1, Delay=1.211970493s Retry Policy Event: =====\u003e Try=2 for GET https://iamnothere.documents.azure.com:443/ Retry Policy Event: error Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host Retry Policy Event: End Try #2, Delay=3.300739653s Retry Policy Event: =====\u003e Try=3 for GET https://iamnothere.documents.azure.com:443/ Retry Policy Event: error Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host Retry Policy Event: MaxRetries 2 exceeded failed to retrieve account properties: Get \"https://iamnothere.documents.azure.com:443/\": dial tcp: lookup iamnothere.documents.azure.com: no such host exit status 1 Note: The first attempt is not counted as a retry, so the total number of attempts is three (1 initial + 2 retries).\nYou can customize this further by implementing fault injection policies. This allows you to simulate various error scenarios for testing purposes.\nFault Injection You can create custom policies to inject faults into the request pipeline. This is useful for testing how your application handles various error scenarios without needing to rely on actual network failures or service outages.\nFor example, you can create a custom policy that injects a fault into the request pipeline. Here, we use a custom policy (FaultInjectionPolicy) that simulates a network error on every request.\ntype FaultInjectionPolicy struct { failureProbability float64 // e.g., 0.3 for 30% chance to fail } // Implement the Policy interface func (f *FaultInjectionPolicy) Do(req *policy.Request) (*http.Response, error) { if rand.Float64() \u003c f.failureProbability { // Simulate a network error return nil, \u0026net.OpError{ Op: \"read\", Net: \"tcp\", Err: errors.New(\"simulated network failure\"), } } // no failure - continue with the request return req.Next() } This function configures the Cosmos DB client to use this policy, sets up logging for retry events, and attempts to create a database.\nfunc retryPolicy4() { opts := azcosmos.ClientOptions{ ClientOptions: policy.ClientOptions{ PerRetryPolicies: []policy.Policy{\u0026FaultInjectionPolicy{failureProbability: 0.6}}, }, } c, err := auth.GetClientWithDefaultAzureCredential(\"https://ACCOUNT_NAME.documents.azure.com:443/\", \u0026opts) // Updated to use opts if err != nil { log.Fatal(err) } azlog.SetListener(func(cls azlog.Event, msg string) { // Log retry-related events switch cls { case azlog.EventRetryPolicy: fmt.Printf(\"Retry Policy Event: %s\\n\", msg) } }) // Set logging level to include retries azlog.SetEvents(azlog.EventRetryPolicy) _, err = c.CreateDatabase(context.Background(), azcosmos.DatabaseProperties{ID: \"test_1\"}, nil) if err != nil { log.Fatal(err) } } Take a look at the logs generated when this code is run - each request attempt fails due to the simulated network error. The SDK logs each retry, with increasing delays between attempts. After reaching the maximum number of retries (default = 3), the operation fails with an error indicating a simulated network failure.\nNote: This can change depending on the failure probability you set in the FaultInjectionPolicy. In this case, we set it to 0.6 (60% chance to fail), so you may see different results each time you run the code.\nRetry Policy Event: =====\u003e Try=1 for GET https://ACCOUNT_NAME.documents.azure.com:443/ //.... Retry Policy Event: MaxRetries 0 exceeded Retry Policy Event: error read tcp: simulated network failure Retry Policy Event: End Try #1, Delay=794.018648ms Retry Policy Event: =====\u003e Try=2 for GET https://ACCOUNT_NAME.documents.azure.com:443/ Retry Policy Event: error read tcp: simulated network failure Retry Policy Event: End Try #2, Delay=2.374693498s Retry Policy Event: =====\u003e Try=3 for GET https://ACCOUNT_NAME.documents.azure.com:443/ Retry Policy Event: error read tcp: simulated network failure Retry Policy Event: End Try #3, Delay=7.275038434s Retry Policy Event: =====\u003e Try=4 for GET https://ACCOUNT_NAME.documents.azure.com:443/ Retry Policy Event: error read tcp: simulated network failure Retry Policy Event: MaxRetries 3 exceeded Retry Policy Event: =====\u003e Try=1 for GET https://ACCOUNT_NAME.documents.azure.com:443/ Retry Policy Event: error read tcp: simulated network failure Retry Policy Event: End Try #1, Delay=968.457331ms 2025/05/05 19:53:50 failed to retrieve account properties: read tcp: simulated network failure exit status 1 Do take a look at Custom HTTP pipeline policies in the Azure SDK for Go documentation for more information on how to implement custom policies."},"title":"Build reliable Go applications: Configuring Azure Cosmos DB Go SDK for real-world scenarios"},"/blog/implementing-chat-history-for-ai-applications-using-azure-cosmos-db-go-sdk/":{"data":{"":"\nWhen building AI applications, especially those involving conversational interfaces, maintaining chat history is crucial. This blog post explores how to implement chat history using the Azure Cosmos DB Go SDK and the langchaingo library, which provides a framework for LLM-powered applications.\nThis blog post covers how to build a chat history implementation using Azure Cosmos DB for NoSQL Go SDK and langchaingo. If you are new to the Go SDK, the sample chatbot application presented in the blog serves as a practical introduction, covering basic operations like read, upsert, etc. It also demonstrates using the Azure Cosmos DB Linux-based emulator (in preview at the time of writing) for integration tests with Testcontainers for Go.\nGo developers looking to build AI applications can use langchaingo, which is a framework for LLM-powered (Large Language Model) applications. It provides pluggable APIs for components like vector store, embedding, loading documents, chains (for composing multiple operations), chat history and more.\nBefore diving in, let’s take a step back to understand the basics.","application-overview#Application overview":"The chat application follows a straightforward domain model: users can initiate multiple conversations, and each conversation can contain multiple messages. Built in Go, the application includes both backend and frontend components.\nBackend: It has multiple sub-parts: The Azure Cosmos DB chat history implementation. Core operations like starting a chat, sending/receiving messages, and retrieving conversation history are exposed via a REST API. The REST API leverages a langchaingo chain to handle user messages. The chain automatically incorporates chat history to ensure past conversations are sent to the LLM. langchaingo handles all orchestration – LLM invocation, chat history inclusion, and more without requiring manual implementation. Frontend: It is built using JavaScript, HTML, and CSS. It is packaged as part of the Go web server (using the embed package), and invokes the backend REST APIs in response to user interactions ","chat-history-implementation-using-azure-cosmos-db#Chat history implementation using Azure Cosmos DB":"langchaingo is a pluggable framework, including its chat history (or memory) component. To integrate Azure Cosmos DB, you need to implement the schema.ChatMessageHistory interface, which provides methods to manage the chat history:\nAddMessage to add messages to a conversation (or start a new one). Messages to retrieve all messages for a conversation. Clear to delete all messages in a conversation. While you can directly instantiate a CosmosDBChatMessageHistory instance and use these methods, the recommended approach is to integrate it into the langchaingo application. Below is an example of using Azure Cosmos DB chat history with a LLMChain:\n// Create a chat history instance cosmosChatHistory, err := cosmosdb.NewCosmosDBChatMessageHistory(cosmosClient, databaseName, containerName, req.SessionID, req.UserID) if err != nil { log.Printf(\"Error creating chat history: %v\", err) sendErrorResponse(w, \"Failed to create chat session\", http.StatusInternalServerError) return } // Create a memory with the chat history chatMemory := memory.NewConversationBuffer( memory.WithMemoryKey(\"chat_history\"), memory.WithChatHistory(cosmosChatHistory), ) // Create an LLM chain chain := chains.LLMChain{ Prompt: promptsTemplate, LLM: llm, Memory: chatMemory, OutputParser: outputparser.NewSimple(), OutputKey: \"text\", } From an Azure Cosmos DB point of view, note that the implementation in this example is just one of many possible options. The one shown here is based on a combination of userid as the partition key and conversation ID (also referred to as the session ID sometimes) being the unique key (id of an Azure Cosmos DB item).\nThis allows an application to:\nGet all the messages for a conversation – This is a point read using the unique id (conversation ID) and partition key (user ID). Add a new message to a conversation – It uses an upsert operation (instead of create) to avoid the need for a read before write. Delete a specific conversation – It uses the delete operation to remove a conversation (and all its messages). Although the langchaingo interface does not expose it, when integrating this as a part of an application, you can also issue a separate query to get all the conversations for a user. This is also efficient since its scoped to a single partition.","how-to-run-the-chatbot#How to run the chatbot":"Like I mentioned earlier, the sample application is a useful way for you to explore langchaingo, the Azure Cosmos DB chat history implementation, as well as the Go SDK.\nBefore exploring the implementation details, it’s a good idea to see the application in action. Refer to the README section of the GitHub repository that provides instructions on how to configure, run and start conversing with the chatbot.","simplify-testing-with-azure-cosmos-db-emulator-and-testcontainers#Simplify testing with Azure Cosmos DB emulator and testcontainers":"The sample application includes basic test cases for both Azure Cosmos DB chat history and the main application. It is worth highlighting the use of testcontainers-go to integrate the Azure Cosmos DB Linux-based emulator docker container.\nThis is great for integration tests since the database is available locally and the tests run much faster (let’s not forget the cost savings as well!). An icing on top is that you do not need to manage the Docker container lifecycle manually. This is taken care as part of the test suite, thanks to the testcontainers-go API which makes it convenient to start the container before the tests run and terminate it once they complete.\nYou can refer to the test cases in the sample application for more details. Here is a snippet of how testcontainers-go is used:\nfunc setupCosmosEmulator(ctx context.Context) (testcontainers.Container, error) { req := testcontainers.ContainerRequest{ Image: emulatorImage, ExposedPorts: []string{emulatorPort + \":8081\", \"1234:1234\"}, WaitingFor: wait.ForListeningPort(nat.Port(emulatorPort)), } container, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{ ContainerRequest: req, Started: true, }) if err != nil { return nil, fmt.Errorf(\"failed to start container: %w\", err) } // Give the emulator a bit more time to fully initialize time.Sleep(5 * time.Second) return container, nil } If you’re interested in using the Azure Cosmos DB Emulator in CI pipelines, check out the blog post – Using the Azure Cosmos DB Emulator in CI/CD Pipelines.","what-is-chat-history-and-why-its-important-for-modern-ai-applications#What is chat history and why it’s important for modern AI applications?":"A common requirement for conversational AI applications is to be able to store and retrieve messages exchanged as part of conversations. This is often referred to as “chat history”. If you have used applications like ChatGPT (which also uses Azure Cosmos DB by the way!), you may be familiar with this concept. When a user logs in, they can start chatting and the messages exchanged as part of the conversation are saved. When they log in again, they can see their previous conversations and can continue from where they left off.\nChat history is obviously important for application end users, but let’s not forget about LLMs! As smart as LLMs might seem, they cannot recall past interactions due to lack of built-in memory (at least for now). Using chat history bridges this gap by providing previous conversations as additional context, enabling LLMs to generate more relevant and high-quality responses. This enhances the natural flow of conversations and significantly improves the user experience.\nA simple example illustrates this: Suppose you ask an LLM via an API, \"Tell me about Azure Cosmos DB\" and it responds with a lengthy paragraph. If you then make another API call saying, \"Break this down into bullet points for easier reading\", the LLM might get confused because it lacks context from the previous interaction. However, if you include the earlier message as part of the context in the second API call, the LLM is more likely to provide an accurate response (though not guaranteed, as LLM outputs are inherently non-deterministic).","wrapping-up#Wrapping Up":"Being able to store chat history is an important part of conversational AI apps. They can serve as a great add-on to existing techniques such as RAG (Retrieval Augmented Generation). Do try out the chatbot application and let us know what you think!\nWhile the implementation in the sample application is relatively simple, how you model the chat history data depends on the requirements. One such scenario has been presented is this excellent blog post on How Microsoft Copilot scales to millions of users with Azure Cosmos DB.\nSome of your requirements might include:\nStoring metadata, such as reactions (in addition to messages) Showing top N recent messages Considering chat history data retention period (using TTL) Incorporating additional analytics (on user interactions) based on the chat history data, and more. Irrespective of the implementation, always make sure to incorporate best practices for data modeling. Refer How to model and partition data on Azure Cosmos DB using a real-world example for guidelines.\nAre you already using or planning to leverage Azure Cosmos DB for your Go applications? We would love to hear from you! Send us your questions and feedback."},"title":"Implementing Chat History for AI Applications Using Azure Cosmos DB Go SDK"},"/blog/integration-testing-for-go-applications-using-testcontainers-and-containerized-databases/":{"data":{"":"","conclusion#Conclusion":"With the right tools, the complexity of integration testing can be tamed. Testcontainers bridges this gap by making it easier to test against containerised services while maintaining isolation, repeatability, and eliminating the need for shared infrastructure.\nWhile this example focused on Azure Cosmos DB, the patterns apply broadly to any service that offers Docker containers, including PostgreSQL, Kafka, and more. Start with a single service and expand your test coverage gradually.\nAlthough the Cosmos DB emulator is a powerful tool for local development and testing, but it does not support every feature available in the full Azure Cosmos DB service. You can can read on the Differences between the emulator and cloud service, or consult the documentation to understand the subset of features that are supported in the emulator.\nThe full example code is available in the GitHub repository, and the testcontainers-go documentation provides comprehensive guidance for exploring more. Happy building!","deep-dive#Deep dive":"Now let’s examine how this works by walking through the code and exploring the key components that make this work.\nOrchestrating the test environment The TestMain function serves as the entry point for our test suite, providing a way to perform setup and teardown operations that span multiple test functions. Unlike individual test functions that run in isolation, TestMain runs once per package and gives you control over the entire test execution lifecycle.\nfunc TestMain(m *testing.M) { // Set up the CosmosDB emulator container ctx := context.Background() var err error emulator, err = setupCosmosDBEmulator(ctx) if err != nil { fmt.Printf(\"Failed to set up CosmosDB emulator: %v\\n\", err) os.Exit(1) } // ... client setup and data seeding // Run the tests code := m.Run() // Cleanup if emulator != nil { _ = emulator.Terminate(ctx) } os.Exit(code) } This structure ensures that the expensive operation of starting a Docker container happens only once, while all individual tests can run against the same container instance. The pattern is particularly valuable when your setup involves external resources like databases or message brokers.\nSetting up the Cosmos DB emulator The setupCosmosDBEmulator function encapsulates the logic for creating and starting the Cosmos DB emulator:\nfunc setupCosmosDBEmulator(ctx context.Context) (testcontainers.Container, error) { req := testcontainers.ContainerRequest{ Image: emulatorImage, ExposedPorts: []string{emulatorPort + \":8081\"}, WaitingFor: wait.ForListeningPort(nat.Port(emulatorPort)), Env: map[string]string{ \"ENABLE_EXPLORER\": \"false\", \"PROTOCOL\": \"https\", }, } container, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{ ContainerRequest: req, Started: true, }) // Give the emulator a bit more time to fully initialize time.Sleep(5 * time.Second) return container, nil } The ContainerRequest struct defines everything testcontainers needs to know about our desired container. The ExposedPorts field maps the container’s internal port 8081 to the same port on the host, while WaitingFor ensures testcontainers doesn’t consider the container ready until the Cosmos DB service is actually listening on that port. The environment variables configure the emulator to run in HTTPS (PROTOCOL) mode without the web-based data explorer (ENABLE_EXPLORER is set to false). I encourage you to explore the documentation for more configuration options that might suit your testing needs.\nConnecting to the emulator The auth.GetEmulatorClientWithAzureADAuth function is part of the cosmosdb-go-sdk-helper package. It provides a convenient way to create a Cosmos DB client that uses Microsoft Entra AD authentication, which is the recommended approach for production applications. However, in this test setup, GetEmulatorClientWithAzureADAuth creates a Cosmos DB client configured specifically for the emulator environment. By passing the custom transport through the ClientOptions, we ensure that the SDK can successfully connect to the emulator despite its self-signed certificate.\ntransport := \u0026http.Client{Transport: \u0026http.Transport{ TLSClientConfig: \u0026tls.Config{InsecureSkipVerify: true}, }} options := \u0026azcosmos.ClientOptions{ClientOptions: azcore.ClientOptions{ Transport: transport, }} client, err = auth.GetEmulatorClientWithAzureADAuth(emulatorEndpoint, options) Note that InsecureSkipVerify: true should only be used in test/dev environments.\nTest execution and cleanup After the container is ready and the client is configured, the code seeds test data through seedTestData() to provide known items for the tests to work with. m.Run() then executes all the actual test functions, returning an exit code that indicates whether the tests passed or failed.\nFinally, the emulator.Terminate(ctx) call in the cleanup section ensures that testcontainers properly stops and removes the Docker container, regardless of the test results. This cleanup is essential for preventing resource leaks and ensuring that subsequent test runs start with a clean slate.","hello-testcontainers#Hello Testcontainers!":"","testcontainers-and-cosmos-db-emulator-in-action#Testcontainers and Cosmos DB emulator in action":"Let’s see testcontainers in action with a practical example. The tests demonstrate how testcontainers automatically spins up a Cosmos DB emulator container, runs our integration tests against it, and cleans up afterward. This gives us the confidence of testing against real database behavior without the complexity of managing external services.\nRunning the tests Before we examine the code, let’s run the tests to see testcontainers in action. Make sure you have Go installed and Docker running on your machine. If you don’t have Docker installed, you can follow the instructions on the Docker website.\nPull the Cosmos DB emulator image from Docker Hub:\ndocker pull mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview Next, clone the repository and run the tests. The test suite will automatically start the Cosmos DB emulator in a Docker container, run the integration tests, and then clean up the container afterward.\ngit clone github.com/abhirockzz/cosmosdb-testcontainers-go cd cosmosdb-testcontainers-go go test -v ./... You should see the tests passing with output similar to this (some lines removed for brevity):\ngithub.com/testcontainers/testcontainers-go - Connected to docker: Server Version: 28.3.0 API Version: 1.48 Operating System: Docker Desktop Total Memory: 7836 MB //..... 🐳 Creating container for image mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview 🐳 Creating container for image testcontainers/ryuk:0.11.0 ✅ Container created: 5e43ec4bd8ea 🐳 Starting container: 5e43ec4bd8ea ✅ Container started: 5e43ec4bd8ea ⏳ Waiting for container id 5e43ec4bd8ea image: testcontainers/ryuk:0.11.0. Waiting for: \u0026{Port:8080/tcp timeout:\u003cnil\u003e PollInterval:100ms skipInternalCheck:false} 🔔 Container is ready: 5e43ec4bd8ea ✅ Container created: cb1d2abb7255 🐳 Starting container: cb1d2abb7255 ✅ Container started: cb1d2abb7255 ⏳ Waiting for container id cb1d2abb7255 image: mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview. Waiting for: \u0026{Port:8081 timeout:\u003cnil\u003e PollInterval:100ms skipInternalCheck:false} 🔔 Container is ready: cb1d2abb7255 === RUN TestCreateItem --- PASS: TestCreateItem (0.00s) === RUN TestCreateItem_FailureScenarios //..... --- PASS: TestCreateItem_FailureScenarios (0.00s) --- PASS: TestCreateItem_FailureScenarios/missing_name (0.00s) --- PASS: TestCreateItem_FailureScenarios/missing_category (0.00s) --- PASS: TestCreateItem_FailureScenarios/invalid_JSON (0.00s) --- PASS: TestCreateItem_FailureScenarios/wrong_HTTP_method (0.00s) === RUN TestGetItem --- PASS: TestGetItem (0.00s) === RUN TestGetItem_FailureScenarios //.... --- PASS: TestGetItem_FailureScenarios (0.00s) --- PASS: TestGetItem_FailureScenarios/missing_item_ID (0.00s) --- PASS: TestGetItem_FailureScenarios/missing_category_parameter (0.00s) --- PASS: TestGetItem_FailureScenarios/empty_category_parameter (0.00s) --- PASS: TestGetItem_FailureScenarios/non-existent_item_ID (0.00s) --- PASS: TestGetItem_FailureScenarios/valid_item_ID_but_wrong_category (0.00s) PASS 🐳 Stopping container: cb1d2abb7255 ✅ Container stopped: cb1d2abb7255 🐳 Terminating container: cb1d2abb7255 🚫 Container terminated: cb1d2abb7255 ok demo 9.194s Understanding the test flow The output shows how testcontainers orchestrates the entire process in three distinct phases:\nContainer Setup Phase: testcontainers connects to Docker, starts the Cosmos DB emulator and Ryuk containers, and waits until the emulator is ready for use. Test Execution Phase: Integration tests run against the live Cosmos DB container, covering both successful operations and various error scenarios to ensure robust handling. Cleanup Phase: All containers are automatically stopped and removed, ensuring the system is left clean with no lingering resources. The combination of testcontainers and the Cosmos DB emulator gives you the reliability of testing against services without the operational overhead of managing test infrastructure. Every test run is isolated, repeatable, and starts from a known clean state. It gives you fast feedback while maintaining the confidence that comes from testing against actual database behavior.","what-youll-learn#What you\u0026rsquo;ll learn":"Integration testing has always presented a fundamental challenge: how do you test your application against real dependencies without the complexity of managing external services? Traditional approaches often involve either mocking dependencies (which can miss integration issues) or maintaining separate test environments (which can be expensive and difficult to manage consistently).\nHello Testcontainers! Testcontainers solves this problem elegantly by providing a way to run lightweight, throwaway instances of databases, message brokers, web servers, and other services directly within your test suite. Instead of complex setup scripts or shared test environments, you can spin up real services in Docker containers that exist only for the duration of your tests. The core value proposition is compelling: write tests that run against the actual technologies your application uses in production, while maintaining the isolation and repeatability that good tests require. When your tests complete, the containers are automatically cleaned up, leaving no trace behind.\nTestcontainers for Go (testcontainers-go package) brings this powerful testing approach to the Go ecosystem. It provides a clean, idiomatic Go API for creating and managing Docker containers within your test suite, handling the lifecycle management, port mapping, and health checks automatically.\nWhat you’ll learn This post walks through a practical example of using testcontainers-go with the Azure Cosmos DB emulator. Cosmos DB offers fully-featured Docker containers that mimic the behavior of the cloud service, making it an excellent candidate for integration testing with Testcontainers. Whether you’re building applications that will eventually run against Azure Cosmos DB, or you’re simply exploring document database patterns, the emulator provides a realistic testing environment without requiring cloud resources.\nThere are two flavors of the Cosmos DB emulator. In this example, I have used the Linux-based emulator (in preview) (which runs, almost anywhere, including ARM64 architecture). You are welcome to try out the other version as well.\nWe’ll examine a simple application with a focus on how testcontainers handles the container setup, test execution, and cleanup. The app uses the Go SDK for Azure Cosmos DB (azcosmos package) and exposes a simple REST API that manages items in a Cosmos DB container:\nPOST /items - Creates a new item with name, description, and category GET /items/{id}?category={category} - Retrieves an item by ID and category "},"title":"Integration testing for Go applications using Testcontainers and containerized databases"},"/blog/iot-cosmosdb-confluent/":{"data":{"":"It’s possible to build seamless device-to-cloud experience with the integrations between Azure and Confluent Cloud. The example provided in this blog post showcases how the following capabilities of Azure and Confluent Cloud are leveraged:\nA fully managed platform for data in motion and self-managed connectors from Confluent A secure connection from Confluent to Azure A highly distributed and scalable data management solution from Azure A limitless analytics service offered by Azure that brings together data integration, enterprise data warehousing, and big data analytics A fully managed microservice development platform on Azure The following diagram provides high-level architecture of the end-to-end solution:\nYou can check out the blog post in its entirety here - https://www.confluent.io/blog/real-time-iot-data-integration-and-analytics-with-confluent-cloud-and-azure/"},"title":"Extracting Value from IoT Using Azure Cosmos DB, Azure Synapse Analytics, and Confluent Cloud"},"/blog/joined-microsoft/":{"data":{"":"","#":"Hello, folks! Delighted to announce that I recently joined the Cloud Developer Advocacy team at Microsoft 😃 😃\nI was fortunate to have worked on developer platforms (PaaS and Serverless) during my stint as a Product Manager at Oracle with a focus on developer experience and advocacy — so I decided to convert this into a full-time role and glad to have joined a company which lays a huge emphasis on developer outreach.\nhttps://makeameme.org/meme/i-should-be-5b5bb0\nI am humbled and a little intimidated at the same time, to have joined this amazing team at Microsoft. You will realize that this feeling of being “a little intimidated” is not without reason, once you take a look at the list below! There are some rock stars in there, but you know what — sometimes being the “dumbest person in the room” can be good for you 😉 Microsoft Cloud Advocates *Our team’s charter is to help every technologist on the planet succeed, be they students or those working in…*developer.microsoft.com\nWhat will I be doing and what have I been up to so far? TL;DR: a whole bunch of things, with a focus on developers and technical communities.\nThis includes (but not limited to) producing content in the form of code, blogs, demos, tutorials; run workshops and hackathon; engaging with you in-person at events, conferences, meetups, and (hopefully) much more. Also, I’ll be on the lookout for your feedback/inputs/pain-points and funnel them back to respective teams at Microsoft, such as engineering, PM, etc. (yes, this time I’ll be on the other side 😉). And, in case you were curious, some of the specific areas I’ll be focusing on include Azure, Kubernetes, Serverless, Go and Kafka — yeah I know, all the 🔥 🔥 stuff!\nIt’s been a couple of weeks since I joined and things seem to be moving at a rapid pace. I have been busy with onboarding (obviously!), getting to know Microsoft (yeah it’s huge!), chatting with colleagues, teams and getting up to speed with things in general. In between all this, I already have a few talks lined up, got to be part of an interesting little project and kick-started a new blog series — stay tuned!\nInterested in joining the team? Maybe you’re already working in Developer Relations, new to Developer Advocacy or just curious about it — doesn’t matter, let’s talk 🙌 Please DM me on Twitter or ping me on LinkedIn\n“the ☁️ is a small place” Hope to see you at conferences, meetups or even GitHub!\nCheers! 😄"},"title":"Joined the Microsoft Developer Advocacy team 🎉🎊 🍻"},"/blog/kafka-azure-data-explorer-kubernetes/":{"data":{"":"In this blog, we will go over how to ingest data into Azure Data Explorer using the open source Kafka Connect Sink connector for Azure Data Explorer running on Kubernetes using Strimzi. Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems using source and sink connectors and Strimzi provides a “Kubernetes-native” way of running Kafka clusters as well as Kafka Connect workers.\nAzure Data Explorer is a fast and scalable data exploration service that lets you collect, store, and analyze large volumes of data from any diverse sources, such as websites, applications, IoT devices, and more. It has a rich connector ecosystem that supports ingestion into Azure Data Explorer as detailed here. One of the supported sources is Apache Kafka and the sink connector allows you to move data from Kafka topics into Azure Data Explorer tables which you can later query and analyse. The best part is that you can do so in a scalable and fault tolerant way using just configuration!\nHere is an overview of the scenario depicted in this blog post:\nThe Azure Data Explorer Kafka Connector picks up data from the configured Kafka topic and queues up ingestion processes (in batches) which eventually write data to a table in Azure Data Explorer. Behind the scenes, the connector leverages the Java SDK for Azure Data Explorer.\nResources for this blog post are available on GitHub","base-installation#Base installation":"Start by installing the Strimzi Operator and use it to spin up a single-node Kafka Cluster on Kubernetes. Installing Strimzi using Helm is pretty easy:\nhelm repo add strimzi https://strimzi.io/charts/ helm install strimzi-kafka strimzi/strimzi-kafka-operator To confirm successful installation:\nkubectl get pods -l=name=strimzi-cluster-operator You should see the cluster operator Pod in Running status\nNAME READY STATUS RESTARTS AGE strimzi-cluster-operator-5c66f679d5-69rgk 1/1 Running 0 43s To deploy a single-node kafka cluster (along with Zookeeper):\nkubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/kafka.yaml Wait for the cluster to start:\nkubectl get pod my-kafka-cluster-kafka-0 -w The Kafka Pod should transition to Running status and both the containers should be in READY state\nNAME READY STATUS RESTARTS AGE my-kafka-cluster-kafka-0 2/2 Running 0 1m ","clean-up-resources#Clean up resources":"To delete the connector and/or Kafka cluster:\nkubectl delete kafkaconnect/my-connect-cluster kubectl delete kafka/my-kafka-cluster To delete the AKS and Azure Data Explorer clusters, simply delete the resource group:\naz group delete --name \u003cAZURE_RESOURCE_GROUP\u003e --yes --no-wait ","conclusion#Conclusion":"That’s all for this blog post and I hope you found it useful! Please note that, this is not the only way to ingest data into Azure Data Explorer. You’re welcome to refer to the documentation and explore other techniques such as One-click Ingestion, using Event Grid, IoT Hub etc.\nPlease consider exploring the following topics as additional learning resources:\nResources Configuring Kafka Connect cluster using Strimzi Strimzi KafkaConnect schema reference Strimzi KafkaConnector schema reference Just Enough Azure Data Explorer for Cloud Architects What’s new in Azure Data Explorer connector 1.x Kusto Query Language ","data-ingestion-in-action#Data ingestion in action":"So, we have everything setup. All we need is events to be sent to the Kafka topic, so that we can see the connector in action and ingest data into Azure Data Explorer.\nYou can use this handy event generator application (available in Docker Hub) and deploy it to your Kubernetes cluster - the Dockerfile is available in the GitHub repo in case you want to reference it.\nKubernetes Deployment snippet:\napiVersion: apps/v1 kind: Deployment metadata: name: adx-event-producer spec: replicas: 1 .... spec: containers: - name: adx-event-producer image: abhirockzz/adx-event-producer imagePullPolicy: Always env: - name: KAFKA_BOOTSTRAP_SERVER value: my-kafka-cluster-kafka-bootstrap:9092 - name: KAFKA_TOPIC value: storm-events - name: SOURCE_FILE value: StormEvents.csv To deploy the producer application:\nkubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/producer.yaml The application picks up records from the StormEvents.csv file and sends them to a Kafka topic. Each event is a CSV record that represent data for a Storm occurence (start and end time, state, type etc.), for example: 2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23357,WISCONSIN,Winter Storm,COOP Observer.\nThe producer application waits for 3 seconds between subsequent produce operations to Kafka. This is intentional so that you can monitor the Kafka Connect logs and make sense of what’s going on. The StormEvents.csv file contains more than 50,000 records, so it might take a while for all of them to be batched and ingested to Azure Data Explorer\nYou can track the application logs using: kubectl logs -f -l app=adx-event-producer. If all is well, you should see something similar to this:\n... sent message to partition 0 offset 0 event 2007-01-01 00:00:00.0000000,2007-01-01 00:00:00.0000000,13208,NORTH CAROLINA,Thunderstorm Wind,Public sent message to partition 0 offset 1 event 2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23358,WISCONSIN,Winter Storm,COOP Observer sent message to partition 0 offset 2 event 2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23357,WISCONSIN,Winter Storm,COOP Observer The storm-events topic will now start getting events and these will be picked up by the sink connector. If you were to track the connector logs:\nkubectl logs -f -l strimzi.io/cluster=my-connect-cluster … you should see logs similar to this:\n.... INFO Kusto ingestion: file (/tmp/kusto-sink-connector-17d03941-f8ca-498e-bc52-68ced036dc69/kafka_storm-events_0_0.csv.gz) of size (1722) at current offset (16) (com.microsoft.azure.kusto.kafka.connect.sink.TopicPartitionWriter) [Timer-6] INFO WorkerSinkTask{id=adx-sink-connector-0} Committing offsets asynchronously using sequence number 17: {storm-events-0=OffsetAndMetadata{offset=17, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-adx-sink-connector-0] INFO Kusto ingestion: file (/tmp/kusto-sink-connector-17d03941-f8ca-498e-bc52-68ced036dc69/kafka_storm-events_0_17.csv.gz) of size (1666) at current offset (33) (com.microsoft.azure.kusto.kafka.connect.sink.TopicPartitionWriter) [Timer-7] .... Query Azure Data Explorer Wait for sometime before data ends up in the Storms table. To confirm, check the row count and confirm that there are no failures in the ingestion process:\nStorms | count . show ingestion failures Once there is some data, try out a few queries. To see all the records:\nStorms Use where and project to filter specific data\nStorms | where EventType == 'Drought' and State == 'TEXAS' | project StartTime, EndTime, Source, EventId Use the summarize operator\nStorms | summarize event_count=count() by State | where event_count \u003e 10 | project State, event_count | render columnchart These are just few examples. Please take a look at the Kusto Query Language documentation or explore tutorials about how to ingest JSON formatted sample data into Azure Data Explorer, using scalar operators, timecharts etc.","kafka-connect-cluster-setup#Kafka Connect cluster setup":"The Strimzi container images for Kafka Connect include two built-in file connectors - FileStreamSourceConnector and FileStreamSinkConnector. For the purposes of this blog, a custom Docker image seeded with Azure Data Explorer connector (version 1.0.1) is available on Docker Hub and it is referenced in the KafkaConnect resource definition (image: abhirockzz/adx-connector-strimzi:1.0.1):\napiVersion: kafka.strimzi.io/v1beta1 kind: KafkaConnect metadata: name: my-connect-cluster spec: image: abhirockzz/adx-connector-strimzi:1.0.1 version: 2.4.0 .... If you want to build your own Docker image, use the Strimzi Kafka Docker image as a base and add the Azure Data Explorer connector JAR top to the plugin path. Start by downloading the connector JAR file:\nexport KUSTO_KAFKA_SINK_VERSION=1.0.1 mkdir connector \u0026\u0026 cd connector curl -L -O https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v$KUSTO_KAFKA_SINK_VERSION/kafka-sink-azure-kusto-$KUSTO_KAFKA_SINK_VERSION-jar-with-dependencies.jar Then, you can use this Dockerfile to build the Docker image:\nFROM strimzi/kafka:0.19.0-kafka-2.4.0 USER root:root COPY ./connector/ /opt/kafka/plugins/ RUN ls -lrt /opt/kafka/plugins/ USER 1001 This technique has been illustrated in the Strimzi documentation\nAuthentication Before installing the connector, we need to create an Azure Service Principal in order for the connector to authenticate and connect to Azure Data Explorer service. You can use the az ad sp create-for-rbac command:\naz ad sp create-for-rbac -n \"kusto-sp\" You will get a JSON response as below - please note down the appId, password and tenant as you will be using them in subsequent steps\n{ \"appId\": \"fe7280c7-5705-4789-b17f-71a472340429\", \"displayName\": \"kusto-sp\", \"name\": \"http://kusto-sp\", \"password\": \"29c719dd-f2b3-46de-b71c-4004fb6116ee\", \"tenant\": \"42f988bf-86f1-42af-91ab-2d7cd011db42\" } Add permissions to your database\nProvide an appropriate role to the Service principal you just created. To assign the admin role, follow this guide to use the Azure portal or use the following command in your Data Explorer cluster\n.add database \u003cdatabase name\u003e admins ('aadapp=\u003cservice principal AppID\u003e;\u003cservice principal TenantID\u003e') 'AAD App' We will seed the auth related config as a Kubernetes Secret - later on you will see where this Secret is referenced.\nCreate a file called adx-auth.yaml with the below contents.\napiVersion: v1 kind: Secret metadata: name: adx-auth type: Opaque stringData: adx-auth.properties: |- kustoURL: \u003creplace ADX Ingest URL\u003e tenantID: \u003center service principal tenant ID\u003e appID: \u003center service principal app ID\u003e password: \u003center service principal tenant password\u003e Replace values for the following:\nkustoURL: Azure Data Explorer ingestion URL e.g. https://ingest-[cluster name].[region].kusto.windows.net tenantID - service principal tenant ID appID - service principal application ID password - service principal password Install Kafka Connect Create the Secret and initiate the Kafka Cluster creation:\nkubectl apply -f adx-auth.yaml kubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/kafka-connect.yaml While you wait for the Kafka Connect cluster to start, take a look at this snippet of the KafkaConnect cluster resource definition. Notice the externalConfiguration attribute that points to the secret we had just created. It is loaded into the Kafka Connect Pod as a Volume and the Kafka FileConfigProvider is used to access them.\napiVersion: kafka.strimzi.io/v1beta1 kind: KafkaConnect metadata: name: my-connect-cluster spec: image: abhirockzz/adx-connector-strimzi:1.0.1 config: ... config.providers: file config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider externalConfiguration: volumes: - name: adx-auth-config secret: secretName: adx-auth To check Kafka Connect cluster status:\nkubectl get pod -l=strimzi.io/cluster=my-connect-cluster -w Wait for the Kafka Connect Pod to transition into Running state.\nNAME READY STATUS RESTARTS AGE my-connect-cluster-connect-5bf9db5d9f-9ttg4 1/1 Running 0 1m Create the topic and install connector You can use the Strimzi Entity Operator to create the storm-events topic. Here is the Topic definition:\napiVersion: kafka.strimzi.io/v1beta1 kind: KafkaTopic metadata: name: storm-events labels: strimzi.io/cluster: my-kafka-cluster spec: partitions: 3 replicas: 1 To create:\nkubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/topic.yaml Use kubectl get kafkatopic to see the topic you just created as well as internal Kafka topics\nNAME PARTITIONS REPLICATION FACTOR consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a 50 1 storm-events 3 1 strimzi-connect-cluster-configs 1 1 strimzi-connect-cluster-offsets 25 1 strimzi-connect-cluster-status 5 1 Here is snippet of the connector (KafkaConnector) definition - it’s just a way to capture configuration and metadata for the connector you want to install.\napiVersion: kafka.strimzi.io/v1alpha1 kind: KafkaConnector metadata: name: adx-sink-connector labels: strimzi.io/cluster: my-connect-cluster spec: class: com.microsoft.azure.kusto.kafka.connect.sink.KustoSinkConnector tasksMax: 3 config: topics: storm-events flush.size.bytes: 10000 flush.interval.ms: 50000 kusto.tables.topics.mapping: \"[{'topic': 'storm-events','db': '[REPLACE DATABASE NAME]', 'table': 'Storms','format': 'csv', 'mapping':'Storms_CSV_Mapping'}]\" kusto.url: ${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:kustoURL} aad.auth.authority: ${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:tenantID} aad.auth.appid: ${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:appID} aad.auth.appkey: ${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:password} key.converter: \"org.apache.kafka.connect.storage.StringConverter\" value.converter: \"org.apache.kafka.connect.storage.StringConverter\" The flush.size.bytes and flush.interval.ms attributes work in tandem with each other and serve as a performance knob for batching. Please refer to the connector GitHub repo for details on these and other configuration parameters\nNotice how the individual properties (from the Secret) are actually referenced. For example to reference the Service Principal application ID, we used this:\naad.auth.appid: ${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:appID} /opt/kafka/external-configuration is a fixed path inside the container adx-auth-config is the name of the volume in the KafkaConnect definition adx-auth.properties is the name of the file as defined in the Secret appID is the name of key The direct attribute name has been used to define non-sensitive connector configs (e.g. topics: storm-events). Alternatively, can encapsulate these in a ConfigMap, load them as a Volume and reference them (just like the sensitive attributes using a Secret).\nCopy the above definition for the KafkaConnector to local file adx-connect-config.yaml. Make sure you replace the correct database name in the kusto.tables.topics.mapping attribute. To create:\nkubectl apply -f adx-connect-config.yaml Check the kafka connect logs kubectl logs -l=strimzi.io/cluster=my-connect-cluster. If everything is working fine, you should see logs similar to this:\n.... INFO [Consumer clientId=connector-consumer-adx-sink-connector-1, groupId=connect-adx-sink-connector] Resetting offset for partition storm-events-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [task-thread-adx-sink-connector-1] INFO [Consumer clientId=connector-consumer-adx-sink-connector-2, groupId=connect-adx-sink-connector] Resetting offset for partition storm-events-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [task-thread-adx-sink-connector-2] ","prerequisites#Prerequisites":"You will need an Azure account along with Azure CLI or Azure Cloud Shell.\nHere are some quick pointers to setting up a Azure Data Explorer cluster and a managed Kubernetes service on Azure. I recommend installing the below services as a part of a single Azure Resource Group which makes it easy to manage these services\nAzure Data Explorer You can setup an Azure Data Explorer cluster and database using Azure Portal, Azure CLI or any of the client SDKs such as Python. Once that’s done, create a table (named Storms) and respective mapping (named Storms_CSV_Mapping) using below queries:\n.create table Storms (StartTime: datetime, EndTime: datetime, EventId: int, State: string, EventType: string, Source: string) .create table Storms ingestion csv mapping 'Storms_CSV_Mapping' '[{\"Name\":\"StartTime\",\"datatype\":\"datetime\",\"Ordinal\":0}, {\"Name\":\"EndTime\",\"datatype\":\"datetime\",\"Ordinal\":1},{\"Name\":\"EventId\",\"datatype\":\"int\",\"Ordinal\":2},{\"Name\":\"State\",\"datatype\":\"string\",\"Ordinal\":3},{\"Name\":\"EventType\",\"datatype\":\"string\",\"Ordinal\":4},{\"Name\":\"Source\",\"datatype\":\"string\",\"Ordinal\":5}]' Azure Kubernetes Service (optional) I have used Azure Kubernetes Service (AKS) but the instructions in this blog post should work for other options as well (e.g. with a local minikube cluster on your laptop). You can setup an AKS cluster using Azure CLI, Azure portal or ARM template"},"title":"Data Ingestion into Azure Data Explorer using Kafka Connect on Kubernetes"},"/blog/kafka-azure-data-explorer/":{"data":{"":"","-lets-try-it-out#.. let\u0026rsquo;s try it out!":"","overview#Overview":"","pre-requisites#Pre-requisites":"","thats-a-wrap#That\u0026rsquo;s a wrap!":"This blog will cover data ingestion from Kafka to Azure Data Explorer (Kusto) using Kafka Connect.\nAzure Data Explorer is a fast and scalable data exploration service that lets you collect, store, and analyze large volumes of data from any diverse sources, such as websites, applications, IoT devices, and more. Kafka Connect platform allows you to stream data between Apache Kafka and external systems in a scalable and reliable manner. The Kafka Connect Sink connector for Azure Data Explorer allows you to move data in Kafka topics to Azure Data Explorer tables which you can later query and analyze.\nHere is the GitHub repo for this blog - https://github.com/abhirockzz/kafka-kusto-ingestion-tutorial\nThe goal is to get started quickly, so we will keep things simple and Docker-ize everything! This includes Kafka, Zookeeper, Kafka Connect worker and the event generator application - defined in docker-compose.yaml\nOver the course of this tutorial, you will:\nGet an overview of the individual components Configure and setup Azure Data Explorer and install the connector Run the end to end demo If you’re looking for a comprehensive coverage of data ingestion with Azure Data Explorer, Kafka and Kubernetes and like a hands-on learning experience, please check out this workshop! https://github.com/Azure/azure-kusto-labs\nPre-requisites You will need a Microsoft Azure account. Maybe try a free one?\nInstall Azure CLI if you don’t have it already (should be quick!) or just use the Azure Cloud Shell from your browser.\nDocker and Docker Compose installed\nOverview As previously mentioned, all the components are defined inside docker-compose.yaml file. Let’s go over it bit by bit:\nThe Kafka and Zookeeper part is pretty straightforward - using the debezium images\nzookeeper: image: debezium/zookeeper:1.2 ports: - 2181:2181 kafka: image: debezium/kafka:1.2 ports: - 9092:9092 links: - zookeeper depends_on: - zookeeper environment: - ZOOKEEPER_CONNECT=zookeeper:2181 - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 The events-producer service is a simple application that sends Storm Events data to a Kafka topic. Storm Events data is a canonical example used throughout the Azure Data Explorer documentation (for example, check this Quickstart and the complete CSV file). The producer app uses the original CSV, but only includes selected fields (such as start and end time, state, source etc.) rather than the entire row (which has more than 20 columns). Here is the sample data:\n2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23357,WISCONSIN,Winter Storm,COOP Observer 2007-01-01 00:00:00.0000000,2007-01-01 06:00:00.0000000,9488,NEW YORK,Winter Weather,Department of Highways 2007-01-01 00:00:00.0000000,2007-01-01 06:00:00.0000000,9487,NEW YORK,Winter Weather,Department of Highways ... The service component in Docker Compose is defined as such:\nevents-producer: build: context: ./storm-events-producer links: - kafka depends_on: - kafka environment: - KAFKA_BOOTSTRAP_SERVER=kafka:9092 - KAFKA_TOPIC=storm-events - SOURCE_FILE=StormEvents.csv The sink connector is where a lot of the magic happens! Let’s explore it:\nKafka Sink Connector for Azure Data Explorer Here is the kusto-connect service in docker compose file:\nkusto-connect: build: context: ./connector ports: - 8083:8083 links: - kafka depends_on: - kafka environment: - BOOTSTRAP_SERVERS=kafka:9092 - GROUP_ID=adx - CONFIG_STORAGE_TOPIC=my_connect_configs - OFFSET_STORAGE_TOPIC=my_connect_offsets - STATUS_STORAGE_TOPIC=my_connect_statuses The container is built from a Dockerfile - this makes it easier for you to run it locally as opposed to pulling it from an external Docker registry\nFROM debezium/connect:1.2 WORKDIR $KAFKA_HOME/connect ARG KUSTO_KAFKA_SINK_VERSION RUN curl -L -O https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v$KUSTO_KAFKA_SINK_VERSION/kafka-sink-azure-kusto-$KUSTO_KAFKA_SINK_VERSION-jar-with-dependencies.jar It’s based on top of the Debezium Kafka Connect image. Simply download the Kusto Connector JAR (version 1.0.1 at the time of writing) and place it in the Kafka Connect plugins directory. That’s it!\nHere is what the sink connector configuration file looks like:\n{ \"name\": \"storm\", \"config\": { \"connector.class\": \"com.microsoft.azure.kusto.kafka.connect.sink.KustoSinkConnector\", \"flush.size.bytes\": 10000, \"flush.interval.ms\": 50000, \"tasks.max\": 1, \"topics\": \"storm-events\", \"kusto.tables.topics.mapping\": \"[{'topic': 'storm-events','db': '\u003center database name\u003e', 'table': 'Storms','format': 'csv', 'mapping':'Storms_CSV_Mapping'}]\", \"aad.auth.authority\": \"\u003center tenant ID\u003e\", \"aad.auth.appid\": \"\u003center application ID\u003e\", \"aad.auth.appkey\": \"\u003center client secret\u003e\", \"kusto.url\": \"https://ingest-\u003cname of cluster\u003e.\u003cregion\u003e.kusto.windows.net\", \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\", \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\" } } The process of loading/importing data into a table in Azure Data Explorer is known as Ingestion. This is how the the connector operates as well.\nBehind the scenes, it uses the following modules in the Java SDK for Azure Data Explorer\ndata: to connect, issue (control) commands and query data ingest: to ingest data At the time of writing, the data formats supported by the connector are: csv, json, txt, avro, apacheAvro, tsv, scsv, sohsv and psv. Data in the Kafka topics is written to files on disk. These are then sent to Azure Data Explorer based on the following connector configurations - when file has reached flush.size.bytes or the flush.interval.ms interval has passed.\nThe only exception to the above mechanism is the avro and apacheAvro data types which are handled as byte arrays\nBy “sent to Azure Data Explorer”, what I really mean that the file is queued for Ingestion (using IngestClient.ingestFromFile)\nAlright, lots of theory so far…\n.. let’s try it out! Clone this repo:\ngit clone https://github.com/abhirockzz/kafka-kusto-ingestion-tutorial cd kafka-kusto-ingestion-tutorial Start off creating an Azure Data Explorer cluster and database using Azure Portal, Azure CLI or any of the client SDKs such as Python.\nOnce that’s done, create a table (Storms) and respective mapping (Storms_CSV_Mapping):\n.create table Storms (StartTime: datetime, EndTime: datetime, EventId: int, State: string, EventType: string, Source: string) .create table Storms ingestion csv mapping 'Storms_CSV_Mapping' '[{\"Name\":\"StartTime\",\"datatype\":\"datetime\",\"Ordinal\":0}, {\"Name\":\"EndTime\",\"datatype\":\"datetime\",\"Ordinal\":1},{\"Name\":\"EventId\",\"datatype\":\"int\",\"Ordinal\":2},{\"Name\":\"State\",\"datatype\":\"string\",\"Ordinal\":3},{\"Name\":\"EventType\",\"datatype\":\"string\",\"Ordinal\":4},{\"Name\":\"Source\",\"datatype\":\"string\",\"Ordinal\":5}]' Start containers and install the connector Before installing the connector, we need to create a Service Principal in order for the connector to authenticate and connect to Azure Data Explorer service.\nUse az ad sp create-for-rbac command:\naz ad sp create-for-rbac -n \"kusto-sp\" You will get a JSON response as such - please note down the appId, password and tenant as you will be using them in subsequent steps\n{ \"appId\": \"fe7280c7-5705-4789-b17f-71a472340429\", \"displayName\": \"kusto-sp\", \"name\": \"http://kusto-sp\", \"password\": \"29c719dd-f2b3-46de-b71c-4004fb6116ee\", \"tenant\": \"42f988bf-86f1-42af-91ab-2d7cd011db42\" } Add permissions to your database\nProvide appropriate role to the Service principal you just created. To assign the admin role, follow this guide to use the Azure portal or use the following command in your Data Explorer cluster\n.add database \u003cdatabase name\u003e admins ('aadapp=\u003cservice principal AppID\u003e;\u003cservice principal TenantID\u003e') 'AAD App' Start the containers:\ndocker-compose up The producer application will start sending events to the storm-events topic. You should see logs similar to:\n.... events-producer_1 | sent message to partition 0 offset 0 events-producer_1 | event 2007-01-01 00:00:00.0000000,2007-01-01 00:00:00.0000000,13208,NORTH CAROLINA,Thunderstorm Wind,Public events-producer_1 | events-producer_1 | sent message to partition 0 offset 1 events-producer_1 | event 2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23358,WISCONSIN,Winter Storm,COOP Observer events-producer_1 | events-producer_1 | sent message to partition 0 offset 2 events-producer_1 | event 2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23357,WISCONSIN,Winter Storm,COOP Observer events-producer_1 | events-producer_1 | sent message to partition 0 offset 3 events-producer_1 | event 2007-01-01 00:00:00.0000000,2007-01-01 06:00:00.0000000,9494,NEW YORK,Winter Weather,Department of Highways events-producer_1 | events-producer_1 | sent message to partition 0 offset 4 events-producer_1 | 2020/08/20 16:51:35 event 2007-01-01 00:00:00.0000000,2007-01-01 06:00:00.0000000,9488,NEW YORK,Winter Weather,Department of Highways .... We can now install the sink connector to consume these events and ingest them into Azure Data Explorer\nReplace the values for following attributes in adx-sink-config.json: aad.auth.authority, aad.auth.appid, aad.auth.appkey, kusto.tables.topics.mapping (the database name) and kusto.url\n{ \"name\": \"storm\", \"config\": { \"connector.class\": \"com.microsoft.azure.kusto.kafka.connect.sink.KustoSinkConnector\", \"flush.size.bytes\": 10000, \"flush.interval.ms\": 50000, \"tasks.max\": 1, \"topics\": \"storm-events\", \"kusto.tables.topics.mapping\": \"[{'topic': 'storm-events','db': '\u003center database name\u003e', 'table': 'Storms','format': 'csv', 'mapping':'Storms_CSV_Mapping'}]\", \"aad.auth.authority\": \"\u003center tenant ID\u003e\", \"aad.auth.appid\": \"\u003center application ID\u003e\", \"aad.auth.appkey\": \"\u003center client secret\u003e\", \"kusto.url\": \"https://ingest-\u003cname of cluster\u003e.\u003cregion\u003e.kusto.windows.net\", \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\", \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\" } } In a different terminnal, keep a track of the connector service logs:\ndocker-compose logs -f | grep kusto-connect Install the connector:\ncurl -X POST -H \"Content-Type: application/json\" --data @adx-sink-config.json http://localhost:8083/connectors //check status curl http://localhost:8083/connectors/storm/status The connector should spring into action. Meanwhile in the other terminal, you should see logs similar to:\nkusto-connect_1 | INFO || Refreshing Ingestion Resources [com.microsoft.azure.kusto.ingest.ResourceManager] kusto-connect_1 | INFO || Kusto ingestion: file (/tmp/kusto-sink-connector-0a8a9fa2-9e4b-414d-bae1-5d01f3969522/kafka_storm-events_0_0.csv.gz) of size (9192) at current offset (93) [com.microsoft.azure.kusto.kafka.connect.sink.TopicPartitionWriter] kusto-connect_1 | INFO || WorkerSinkTask{id=storm-0} Committing offsets asynchronously using sequence number 1: {storm-events-0=OffsetAndMetadata{offset=94, leaderEpoch=null, metadata=''}} [org.apache.kafka.connect.runtime.WorkerSinkTask] ct.runtime.WorkerSinkTask] kusto-connect_1 | INFO || Kusto ingestion: file (/tmp/kusto-sink-connector-0a8a9fa2-9e4b-414d-bae1-5d01f3969522/kafka_storm-events_0_94.csv.gz) of size (1864) at current offset (111) [com.microsoft.azure.kusto.kafka.connect.sink.TopicPartitionWriter] kusto-connect_1 | INFO || WorkerSinkTask{id=storm-0} Committing offsets asynchronously using sequence number 2: {storm-events-0=OffsetAndMetadata{offset=112, leaderEpoch=null, metadata=''}} [org.apache.kafka.connect.runtime.WorkerSinkTask] .... Wait for sometime before data ends up in the Storms table. To confirm, check the row count and confirm that there are no failures in the ingestion process:\nStorms | count . show ingestion failures Once there is some data, try out a few queries. To see all the records:\nStorms Use where and project to filter specific data\nStorms | where EventType == 'Drought' and State == 'TEXAS' | project StartTime, EndTime, Source, EventId Use the summarize operator\nStorms | summarize event_count=count() by State | where event_count \u003e 10 | project State, event_count | render columnchart These are just few examples. Dig into the Kusto Query Language documentation or explore tutorials about how to ingest JSON formatted sample data into Azure Data Explorer, using scalar operators, timecharts etc.\nIf you want to re-start from scratch, simply stop the containers (docker-compose down -v), delete (.drop table Storms) and re-create the Storms table (along with the mapping) and re-start containers (docker-compose up)\nClean up To delete the Azure Data Explorer cluster/database, use az cluster delete or az kusto database delete\naz kusto cluster delete -n \u003ccluster name\u003e -g \u003cresource group name\u003e az kusto database delete -n \u003cdatabase name\u003e --cluster-name \u003ccluster name\u003e -g \u003cresource group name\u003e That’s a wrap! I hope this helps you get started building data ingestion pipelines from Kafka to Azure Data Explorer using the Kafka Connect sink connector. This is not the only way to ingest data into Azure Data Explorer (of course!). You’re welcome to explore the documentation and explore other techniques such as One-click Ingestion, using Event Grid, IoT Hub and much more!\nUntil next time, Happy Exploring!"},"title":"How to Ingest data from Kafka into Azure Data Explorer"},"/blog/kafka-cassandra-ingestion/":{"data":{"":"This blog post demonstrates how you can use an open source solution (connector based) to ingest data from Kafka into Azure Cosmos DB Cassandra API. It uses a simple yet practical scenario along with a re-usable setup using Docker Compose to help with iterative development and testing. You will learn about:\nOverview of Kafka Connect along with the details of the integration How to configure and use the connector to work with Azure Cosmos DB Use the connector to write data to multiple tables from a single Kafka topic By the end of the article, you should have a working end to end integration and be able to validate it.\nAzure Cosmos DB Cassandra API is a fully managed cloud service that is compatible with Cassandra Query Language (CQL) v3.11 API. It has no operational overhead and you can benefit from all the underlying Azure Cosmos DB capabilities such as global distribution, automatic scale out partitioning, availability and latency guarantees, encryption at rest, backups etc.\nYour existing Cassandra applications can work with the Azure Cosmos DB Cassandra API since it works with CQLv4 compliant drivers (see examples for Java, .Net Core, Node.js, Python etc.) But, you also need to think about integrating with other systems with existing data and bringing that into Azure Cosmos DB. One such system is Apache Kafka, which is a distributed streaming platform. It is used in industries and organizations to solve a wide variety of problems ranging from traditional asynchronous messaging, website activity tracking, log aggregation, real-time fraud detection and much more! It has a rich ecosystem of technologies such as Kafka Streams for stream processing and Kafka Connect for real-time data integration.\nThanks to its scalable design, Apache Kafka often serves as a central component in the overall data architecture with other systems pumping data into it. These could be click stream events, logs, sensor data, orders, database change-events etc. You name it! So, as you can imagine, there is a lot of data in Apache Kafka (topics) but it’s only useful when consumed or ingested into other systems. You could achieve this by writing good old plumbing code using the Kafka Producer/Consumer APIs using a language and client SDK of your choice. But you can do better!\nThe code and configuration associated with this blog post is available in this GitHub repository - https://github.com/abhirockzz/kafka-cosmosdb-cassandra","additional-resources#Additional resources":"If you want to explore further, I would recommend:\nIntegrating Azure Cosmos DB Cassandra API and Apache Spark Migrate data from Cassandra to Azure Cosmos DB Cassandra API account using Azure Databricks How to write new connectors for Kafka Connect Understanding the architecture for DataStax Apache Kafka Connector Explore Debezium connectors ","cassandra-sink-connector-setup#Cassandra Sink connector setup":"Copy the JSON contents below to a file (you can name it cassandra-sink-config.json). You will need to update it as per your setup and the rest of this section will provide guidance around this topic.\n{ \"name\": \"kafka-cosmosdb-sink\", \"config\": { \"connector.class\": \"com.datastax.oss.kafka.sink.CassandraSinkConnector\", \"tasks.max\": \"1\", \"topics\": \"weather-data\", \"contactPoints\": \"\u003ccosmos db account name\u003e.cassandra.cosmos.azure.com\", \"port\": 10350, \"loadBalancing.localDc\": \"\u003ccosmos db region e.g. Southeast Asia\u003e\", \"auth.username\": \"\u003center username for cosmosdb account\u003e\", \"auth.password\": \"\u003center password for cosmosdb account\u003e\", \"ssl.hostnameValidation\": true, \"ssl.provider\": \"JDK\", \"ssl.keystore.path\": \"/etc/alternatives/jre/lib/security/cacerts/\", \"ssl.keystore.password\": \"changeit\", \"datastax-java-driver.advanced.connection.init-query-timeout\": 5000, \"maxConcurrentRequests\": 500, \"maxNumberOfRecordsInBatch\": 32, \"queryExecutionTimeout\": 30, \"connectionPoolLocalSize\": 4, \"topic.weather-data.weather.data_by_state.mapping\": \"station_id=value.stationid, temp=value.temp, state=value.state, ts=value.created\", \"topic.weather-data.weather.data_by_station.mapping\": \"station_id=value.stationid, temp=value.temp, state=value.state, ts=value.created\", \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\", \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"value.converter.schemas.enable\": false, \"offset.flush.interval.ms\": 10000 } } Here is a summary of the attributes:\nBasic connectivity\ncontactPoints: enter the contact point for Cosmos DB Cassandra loadBalancing.localDc: enter the region for Cosmos DB account e.g. Southeast Asia auth.username: enter the username auth.password: enter the password port: enter the port value (this is 10350, not 9042. leave it as is) You can access this info the Azure Portal:\nSSL configuration\nAzure Cosmos DB enforces secure connectivity over SSL and Kafka Connect connector supports SSL as well.\nssl.keystore.path: path to the JDK keystore (inside the container it is /etc/alternatives/jre/lib/security/cacerts/) ssl.keystore.password: JDK keystore (default) password ssl.hostnameValidation: We turn onn node hostname validation ssl.provider: JDK is used as the SSL provider The value for ssl.keystore.path should not be updated since it points to a path inside the Docker container of the Kafka Connect worker. It goes without saying that, this would be different for a production grade deployment where you would have to update the Docker container to add your certificate etc.\nKafka to Cassandra mapping\nTo push data from Kafka topics to Cassandra, the connector must be configured by providing mapping between records in Kafka topics and the columns in the Cassandra table(s). One of the nice capabilities of the connector is that it allows you to write to multiple Cassandra tables using data from a single Kafka topic. This is really helpful in scenarios where you need derived representations (tables) of events in your Kafka topic(s).\nTake a look at the following mapping attributes:\n\"topic.weather-data.weather.data_by_state.mapping\": \"station_id=value.stationid, temp=value.temp, state=value.state, ts=value.created\" \"topic.weather-data.weather.data_by_station.mapping\": \"station_id=value.stationid, temp=value.temp, state=value.state, ts=value.created\" Let’s break it down:\nThe key (e.g. topic.weather-data.weather.data_by_state.mapping) is nothing but a combination of the Kafka topic name and the Cassandra table (including the keyspace). Note that define mappings for two tables (data_by_state and data_by_station) using separate config parameters. The value is comma-separated entries of the Cassandra column name and the corresponding JSON attribute of the event in the Kafka topic e.g. station_id=value.stationid refers to station_id which is a column in the data_by_station table and value.stationid refers to stationid which is an attribute in the JSON payload (which looks like this - {\"stationid\":\"station-7\",\"temp\":\"64\",\"state\":\"state-17\",\"created\":\"2020-11-28T04:51:06Z\"}) Check out https://docs.datastax.com/en/kafka/doc/kafka/kafkaMapTopicTable.html for details\nGeneric parameters\nkey.converter: We use the string converter org.apache.kafka.connect.storage.StringConverter value.converter: since the data in Kafka topics is JSON, we make use of org.apache.kafka.connect.json.JsonConverter value.converter.schemas.enable: This is important - our JSON payload does not have a schema associated with it (for the purposes of the demo app). We need to instruct Kafka Connect to not look for a schema by setting this to false. Not doing so will result in failures. Passing Java driver level configs\ndatastax-java-driver.advanced.connection.init-query-timeout: 5000\nAlthough the connector provides sane defaults, you can pass in the Java driver properties as connector configuration parameters. The Java driver uses 500 ms as the default value for the init-query-timeout parameter (which is quite low in my opinion), given the fact that it is used as “timeout to use for internal queries that run as part of the initialization process” (read more here https://docs.datastax.com/en/developer/java-driver/4.2/manual/core/configuration/reference/)\nI did face some issues due to this and was glad to see that it was tunable! Setting it to 5000 ms worked for me but it can probably be set slightly lower and it would still work just fine e.g. 2000 ms\nPlease leave the other attributes unchanged\nFor more details around the configuration, please refer to the documentation - https://docs.datastax.com/en/kafka/doc/kafka/kafkaConfigTasksTOC.html\nInstall the connector using the Kafka Connect REST endpoint:\ncurl -X POST -H \"Content-Type: application/json\" --data @cassandra-sink-config.json http://localhost:8083/connectors # check status curl http://localhost:8080/connectors/kafka-cosmosdb-sink/status If all goes well, the connector should start weaving its magic. It should authenticate to Azure Cosmos DB and start ingesting data from the Kafka topic (weather-data) into Cassandra tables - weather.data_by_state and weather.data_by_station\nYou can now query data in the tables. Head over to the Azure portal, bring up the Hosted CQL Shell for your Azure Cosmos DB account.","conclusion#Conclusion":"To summarize, you learnt how to use Kafka Connect for real-time data integration between Apache Kafka and Azure Cosmos DB. Since the sample adopts a Docker container based approach, you can easily customize this as per your own unique requirements, rinse and repeat!\nThe use case and data-flow demonstrated in this article was relatively simple, but the rich Kafka Connect ecosystem of connectors allows you to integrate disparate systems and stitch together complex data pipelines without having to write custom integration code. For example, to migrate/integrate with another RDBMS (via Kafka), you could potentially use the Kafka Connect JDBC Source connector to pull database records into Kafka, transform or enrich them in a streaming fashion using Kafka Streams, re-write them back to a Kafka topic and then bring that data into Azure Cosmos DB using the approach outlined in this article. The are lots of possibilities and the solution will depend on the use case and requirements.\nYou would obviously need to setup, configure and operate these connectors. At the very core, Kafka Connect cluster instances are just JVM processes and inherently stateless (all the state handling is offloaded to Kafka). Hence, you’ve a lot of flexibility in terms of your overall architecture as well as orchestration: for example, running them in Kubernetes for fault-tolerance and scalability!","hello-kafka-connect#Hello, Kafka Connect":"Kafka Connect is a platform to stream data between Apache Kafka and other systems in a scalable and reliable manner. Besides the fact that it only depends on Kafka, the great thing about it is the fact that it provides a suite of ready-to-use connectors. This means that you do not need to write custom integration code to glue systems together; no code, just configuration! In case an existing connector is not available, you can leverage the powerful Kafka Connect framework to build your own connectors.\nThere are two broad categories of connectors offered by Kafka Connect:\nSource connector: It is used to to extract data “from” an external system and send it to Apache Kafka. Sink connector: It is used to send existing data in Apache Kafka “to” an external system. In this blog post, we will be using the open source DataStax Apache Kafka connector which is a Sink connector that works on top of Kafka Connect framework to ingest records from a Kafka topic into rows of one or more Cassandra table(s).","pre-requisites#Pre-requisites":" You will need a Microsoft Azure account. Don’t worry, you can get it for free if you don’t have one already! Install Docker and Docker Compose Finally, clone this GitHub repo:\ngit clone https://github.com/abhirockzz/kafka-cosmosdb-cassandra cd kafka-cosmos-cassandra The next sections will guide you through:\nSet up an Azure Cosmos DB account, Cassandra keyspace and tables Bootstrap the integration pipeline Understand the configuration and start a connector instance Test the end to end result and run queries on data in Azure Cosmos DB tables ","query-azure-cosmos-db#Query Azure Cosmos DB":"Check the data_by_state and data_by_station tables. Here are some sample queries to get you started:\nselect * from weather.data_by_state where state = 'state-1'; select * from weather.data_by_state where state IN ('state-1', 'state-2'); select * from weather.data_by_state where state = 'state-3' and ts \u003e toTimeStamp('2020-11-26'); select * from weather.data_by_station where station_id = 'station-1'; select * from weather.data_by_station where station_id IN ('station-1', 'station-2'); select * from weather.data_by_station where station_id IN ('station-2', 'station-3') and ts \u003e toTimeStamp('2020-11-26'); Clean up To stop the containers, you can:\ndocker-compose -p kafka-cosmos-cassandra down -v You can either delete the keyspace/table or the Azure Cosmos DB account.","setup-and-configure-azure-cosmos-db#Setup and configure Azure Cosmos DB":"Start by creating an Azure Cosmos DB account with the Cassandra API option selected\nUsing the Azure portal, create the Cassandra Keyspace and the tables required for the demo application.\nCREATE KEYSPACE weather WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'datacenter1' : 1}; CREATE TABLE weather.data_by_state (station_id text, temp int, state text, ts timestamp, PRIMARY KEY (state, ts)) WITH CLUSTERING ORDER BY (ts DESC) AND cosmosdb_cell_level_timestamp=true AND cosmosdb_cell_level_timestamp_tombstones=true AND cosmosdb_cell_level_timetolive=true; CREATE TABLE weather.data_by_station (station_id text, temp int, state text, ts timestamp, PRIMARY KEY (station_id, ts)) WITH CLUSTERING ORDER BY (ts DESC) AND cosmosdb_cell_level_timestamp=true AND cosmosdb_cell_level_timestamp_tombstones=true AND cosmosdb_cell_level_timetolive=true; That’s it for the database part! It’s time to start up other components.","solution-overview#Solution overview":"At a high level, the solution is quite simple! But a diagram should be helpful nonetheless.\nSample weather data continuously generated into a Kafka topic. This is picked up by the connector and sent to Azure Cosmos DB and can be queried using any Cassandra client driver.\nExcept Azure Cosmos DB, the rest of the components of the solution run as Docker containers (using Docker Compose). This includes Kafka (and Zookeeper), Kafka Connect worker (the Cassandra connector) along with the sample data generator (Go) application. Having said that, the instructions would work with any Kafka cluster and Kafka Connect workers, provided all the components are configured to access and communicate with each other as required. For example, you could have a Kafka cluster on Azure HD Insight or Confluent Cloud on Azure Marketplace.\nDocker Compose services Here is a breakdown of the components and their service definitions - you can refer to the complete docker-compose file in the GitHub repo.\nThe debezium images are used to run Kafka and Zookeeper. They just work and are great for iterative development with quick feedback loop, demos etc.\nzookeeper: image: debezium/zookeeper:1.2 ports: - 2181:2181 kafka: image: debezium/kafka:1.2 ports: - 9092:9092 links: - zookeeper depends_on: - zookeeper environment: - ZOOKEEPER_CONNECT=zookeeper:2181 - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 To run as a Docker container, the DataStax Apache Kafka Connector is baked on top of an existing Docker image - debezium/connect-base. This image includes an installation of Kafka and its Kafka Connect libraries, thus making it really convenient to add custom connectors.\ncassandra-connector: build: context: ./connector ports: - 8083:8083 links: - kafka depends_on: - kafka environment: - BOOTSTRAP_SERVERS=kafka:9092 - GROUP_ID=cass - CONFIG_STORAGE_TOPIC=cass_connect_configs - OFFSET_STORAGE_TOPIC=cass_connect_offsets - STATUS_STORAGE_TOPIC=cass_connect_statuses The Dockerfile is quite compact. It downloads the connectors and unzips it to appropriate directory in the filesystem (plugin path) for the Kafka Connect framework to detect it.\nFROM debezium/connect-base:1.2 WORKDIR $KAFKA_HOME/connect RUN curl -L -O https://downloads.datastax.com/kafka/kafka-connect-cassandra-sink.tar.gz RUN tar zxf kafka-connect-cassandra-sink.tar.gz RUN rm kafka-connect-cassandra-sink.tar.gz Finally, the data-generator service seeds randomly generated (JSON) data into the weather-data Kafka topic. You can refer to the code and Dockerfile in the GitHub repo\ndata-generator: build: context: ./data-generator ports: - 8080:8080 links: - kafka depends_on: - kafka environment: - KAFKA_BROKER=kafka:9092 - KAFKA_TOPIC=weather-data Let’s move on to the practical aspects! Make sure you have the following ready before you proceed.","start-integration-pipeline#Start integration pipeline":"Since everything is Docker-ized, all you need is a single command to bootstrap services locally - Kafka, Zookeeper, Kafka Connect worker and the sample data generator application.\ndocker-compose --project-name kafka-cosmos-cassandra up --build It might take a while to download and start the containers: this is just a one time process.\nTo confirm whether all the containers have started:\ndocker-compose -p kafka-cosmos-cassandra ps #output Name Command State Ports -------------------------------------------------------------------------------------------------------------------------- kafka-cosmos-cassandra_cassandra- /docker-entrypoint.sh start Up 0.0.0.0:8083-\u003e8083/tcp, 8778/tcp, connector_1 9092/tcp, 9779/tcp kafka-cosmos-cassandra_datagen_1 /app/orders-gen Up 0.0.0.0:8080-\u003e8080/tcp kafka-cosmos-cassandra_kafka_1 /docker-entrypoint.sh start Up 8778/tcp, 0.0.0.0:9092-\u003e9092/tcp, 9779/tcp kafka-cosmos-cassandra_zookeeper_1 /docker-entrypoint.sh start Up 0.0.0.0:2181-\u003e2181/tcp, 2888/tcp, 3888/tcp, 8778/tcp, 9779/tcp The data generator application will start pumping data into the weather-data topic in Kafka. You can also do quick sanity check to confirm. Peek into the Docker container running the Kafka connect worker:\ndocker exec -it kafka-cosmos-cassandra_cassandra-connector_1 bash Once you drop into the container shell, just start the usual Kafka console consumer process and you should see weather data (in JSON format) flowing in.\ncd ../bin ./kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic weather-data #output {\"stationid\":\"station-7\",\"temp\":\"64\",\"state\":\"state-17\",\"created\":\"2020-11-28T04:51:06Z\"} {\"stationid\":\"station-9\",\"temp\":\"65\",\"state\":\"state-1\",\"created\":\"2020-11-28T04:51:09Z\"} {\"stationid\":\"station-3\",\"temp\":\"60\",\"state\":\"state-9\",\"created\":\"2020-11-28T04:51:12Z\"} {\"stationid\":\"station-8\",\"temp\":\"60\",\"state\":\"state-3\",\"created\":\"2020-11-28T04:51:15Z\"} {\"stationid\":\"station-5\",\"temp\":\"65\",\"state\":\"state-7\",\"created\":\"2020-11-28T04:51:18Z\"} {\"stationid\":\"station-6\",\"temp\":\"60\",\"state\":\"state-4\",\"created\":\"2020-11-28T04:51:21Z\"} .... "},"title":"Integrate Kafka and Cassandra using Kafka Connect"},"/blog/kafka-kubernetes-strimzi-1/":{"data":{"":"","install-strimzi#Install Strimzi":"","ok-but-does-it-work#Ok, but does it work?":"","time-to-create-a-kafka-cluster#Time to create a Kafka cluster!":"","wait-what-is-strimzi#Wait, what is \u003ccode\u003eStrimzi\u003c/code\u003e?":"","were-just-getting-started#We\u0026rsquo;re just getting started\u0026hellip;":"Some of my previous blog posts (such as Kafka Connect on Kubernetes, the easy way!), demonstrate how to use Kafka Connect in a Kubernetes-native way. This is the first in a series of blog posts which will cover Apache Kafka on Kubernetes using the Strimzi Operator. In this post, we will start off with the simplest possible setup i.e. a single node Kafka (and Zookeeper) cluster and learn:\nStrimzi overview and setup Kafka cluster installation Kubernetes resources used/created behind the scenes Test the Kafka setup using clients within the Kubernetes cluster The code is available on GitHub - https://github.com/abhirockzz/kafka-kubernetes-strimzi\nWhat do I need to try this out? kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl/\nI will be using Azure Kubernetes Service (AKS) to demonstrate the concepts, but by and large it is independent of the Kubernetes provider (e.g. feel free to use a local setup such as minikube). If you want to use AKS, all you need is a Microsoft Azure account which you can get for FREE if you don’t have one already.\nInstall Helm I will be using Helm to install Strimzi. Here is the documentation to install Helm itself - https://helm.sh/docs/intro/install/\nYou can also use the YAML files directly to install Strimzi. Check out the quick start guide here - https://strimzi.io/docs/quickstart/latest/#proc-install-product-str\n(optional) Setup Azure Kubernetes Service Azure Kubernetes Service (AKS) makes it simple to deploy a managed Kubernetes cluster in Azure. It reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. Here are examples of how you can setup an AKS cluster using\nAzure CLI, - Azure portal or ARM template Once you setup the cluster, you can easily configure kubectl to point to it\naz aks get-credentials --resource-group \u003cCLUSTER_RESOURCE_GROUP\u003e --name \u003cCLUSTER_NAME\u003e Wait, what is Strimzi? from the Strimzi documentation\nStrimzi simplifies the process of running Apache Kafka in a Kubernetes cluster. Strimzi provides container images and Operators for running Kafka on Kubernetes. It is a part of the Cloud Native Computing Foundation as a Sandbox project (at the time of writing)\nStrimzi Operators are fundamental to the project. These Operators are purpose-built with specialist operational knowledge to effectively manage Kafka. Operators simplify the process of: Deploying and running Kafka clusters and components, Configuring and securing access to Kafka, Upgrading and managing Kafka and even taking care of managing topics and users.\nHere is a diagram which shows a 10,000 feet overview of the Operator roles:\nInstall Strimzi Installing Strimzi using Helm is pretty easy:\n//add helm chart repo for Strimzi helm repo add strimzi https://strimzi.io/charts/ //install it! (I have used strimzi-kafka as the release name) helm install strimzi-kafka strimzi/strimzi-kafka-operator This will install the Strimzi Operator (which is nothing but a Deployment), Custom Resource Definitions and other Kubernetes components such as Cluster Roles, Cluster Role Bindings and Service Accounts\nFor more details, check out this link\nTo delete, simply helm uninstall strimzi-kafka\nTo confirm that the Strimzi Operator had been deployed, check it’s Pod (it should transition to Running status after a while)\nkubectl get pods -l=name=strimzi-cluster-operator NAME READY STATUS RESTARTS AGE strimzi-cluster-operator-5c66f679d5-69rgk 1/1 Running 0 43s Check the Custom Resource Definitions as well:\nkubectl get crd | grep strimzi kafkabridges.kafka.strimzi.io 2020-04-13T16:49:36Z kafkaconnectors.kafka.strimzi.io 2020-04-13T16:49:33Z kafkaconnects.kafka.strimzi.io 2020-04-13T16:49:36Z kafkaconnects2is.kafka.strimzi.io 2020-04-13T16:49:38Z kafkamirrormaker2s.kafka.strimzi.io 2020-04-13T16:49:37Z kafkamirrormakers.kafka.strimzi.io 2020-04-13T16:49:39Z kafkas.kafka.strimzi.io 2020-04-13T16:49:40Z kafkatopics.kafka.strimzi.io 2020-04-13T16:49:34Z kafkausers.kafka.strimzi.io 2020-04-13T16:49:33Z kafkas.kafka.strimzi.io CRD represents Kafka clusters in Kubernetes\nNow that we have the “brain” (the Strimzi Operator) wired up, let’s use it!\nTime to create a Kafka cluster! As mentioned, we will keep things simple and start off with the following setup (which we will incrementally update as a part of subsequent posts in this series):\nA single node Kafka cluster (and Zookeeper) Available internally to clients in the same Kubernetes cluster No encryption, authentication or authorization No persistence (uses emptyDir volume) To deploy a Kafka cluster all we need to do is create a Strimzi Kafka resource. This is what it looks like:\napiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: my-kafka-cluster spec: kafka: version: 2.4.0 replicas: 1 listeners: plain: {} config: offsets.topic.replication.factor: 1 transaction.state.log.replication.factor: 1 transaction.state.log.min.isr: 1 log.message.format.version: \"2.4\" storage: type: ephemeral zookeeper: replicas: 1 storage: type: ephemeral For a detailed Kafka CRD reference, please check out the documentation - https://strimzi.io/docs/operators/master/using.html#type-Kafka-reference\nWe define the name (my-kafka-cluster) of cluster in metadata.name. Here is a summary of attributes in spec.kafka:\nversion - The Kafka broker version (defaults to 2.5.0 at the time of writing, but we’re using 2.4.0) replicas - Kafka cluster size i.e. the number of Kafka nodes (Pods in the cluster) listeners - Configures listeners of Kafka brokers. In this example we are using the plain listener which means that the cluster will be accessible to internal clients (in the same Kubernetes cluster) on port 9092 (no encryption, authentication or authorization involved). Supported types are plain, tls, external (See https://strimzi.io/docs/operators/master/using.html#type-KafkaListeners-reference). It is possible to configure multiple listeners (we will cover this in subsequent blogs posts) config - These are key-value pairs used as Kafka broker config properties storage - Storage for Kafka cluster. Supported types are ephemeral, persistent-claim and jbod. We are using ephemeral in this example which means that the emptyDir volume is used and the data is only associated with the lifetime of the Kafka broker Pod (a future blog post will cover persistent-claim storage) Zookeeper cluster details (spec.zookeeper) are similar to that of Kafka. In this case we just configuring the no. of replicas and storage type. Refer to https://strimzi.io/docs/operators/master/using.html#type-ZookeeperClusterSpec-reference for details\nTo create the Kafka cluster:\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-1/kafka.yaml What’s next? The Strimzi operator spins into action and creates many Kubernetes resources in response to the Kafka CRD instance we just created.\nThe following resources are created:\nStatefulSet - Kafka and Zookeeper clusters are exist in the form of StatefulSets which is used to manage stateful workloads in Kubernetes. Please refer https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/ and related material for details Service - Kubernetes ClusterIP Service for internal access ConfigMap - Kafka and Zookeeper configuration is stored in Kubernetes ConfigMaps Secret - Kubernetes Secrets to store private keys and certificates for Kafka cluster components and clients. These are used for TLS encryption and authentication (covered in subsequent blog posts) Kafka Custom Resource kubectl get kafka NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS my-kafka-cluster 1 1 StatefulSet and Pod Check Kafka and Zookeeper StatefulSets using:\nkubectl get statefulset/my-kafka-cluster-zookeeper kubectl get statefulset/my-kafka-cluster-kafka Kafka and Zookeeper Pods\nkubectl get pod/my-kafka-cluster-zookeeper-0 kubectl get pod/my-kafka-cluster-kafka-0 ConfigMap Individual ConfigMaps are created to store Kafka and Zookeeper configurations\nkubectl get configmap my-kafka-cluster-kafka-config 4 19m my-kafka-cluster-zookeeper-config 2 20m Let’s peek into the Kafka configuration\nkubectl get configmap/my-kafka-cluster-kafka-config -o yaml The output is quite lengthy but I will highlight the important bits. As part of the data section, there are two config properties for the Kafka broker - log4j.properties and server.config.\nHere is a snippet of the server.config. Notice the advertised.listeners (highlights the internal access over port 9092) and User provided configuration (the one we specified in the yaml manifest)\n############################## ############################## # This file is automatically generated by the Strimzi Cluster Operator # Any changes to this file will be ignored and overwritten! ############################## ############################## broker.id=${STRIMZI_BROKER_ID} log.dirs=/var/lib/kafka/data/kafka-log${STRIMZI_BROKER_ID} ########## # Plain listener ########## ########## # Common listener configuration ########## listeners=REPLICATION-9091://0.0.0.0:9091,PLAIN-9092://0.0.0.0:9092 advertised.listeners=REPLICATION-9091://my-kafka-cluster-kafka-${STRIMZI_BROKER_ID}.my-kafka-cluster-kafka-brokers.default.svc:9091,PLAIN-9092://my-kafka-cluster-kafka-${STRIMZI_BROKER_ID}.my-kafka-cluster-kafka-brokers.default.svc:9092 listener.security.protocol.map=REPLICATION-9091:SSL,PLAIN-9092:PLAINTEXT inter.broker.listener.name=REPLICATION-9091 sasl.enabled.mechanisms= ssl.secure.random.implementation=SHA1PRNG ssl.endpoint.identification.algorithm=HTTPS ########## # User provided configuration ########## log.message.format.version=2.4 offsets.topic.replication.factor=1 transaction.state.log.min.isr=1 transaction.state.log.replication.factor=1 Service If you query for Services, you should see something similar to this:\nkubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) my-kafka-cluster-kafka-bootstrap ClusterIP 10.0.240.137 \u003cnone\u003e 9091/TCP,9092/TCP my-kafka-cluster-kafka-brokers ClusterIP None \u003cnone\u003e 9091/TCP,9092/TCP my-kafka-cluster-zookeeper-client ClusterIP 10.0.143.149 \u003cnone\u003e 2181/TCP my-kafka-cluster-zookeeper-nodes ClusterIP None \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP my-kafka-cluster-kafka-bootstrap makes it possible for internal Kubernetes clients to access the Kafka cluster and my-kafka-cluster-kafka-brokers is the Headless service corresponding to the StatefulSet\nSecret Although we’re not using them, it’s helpful to look at the Secrets created by Strimzi:\nkubectl get secret my-kafka-cluster-clients-ca Opaque my-kafka-cluster-clients-ca-cert Opaque my-kafka-cluster-cluster-ca Opaque my-kafka-cluster-cluster-ca-cert Opaque my-kafka-cluster-cluster-operator-certs Opaque my-kafka-cluster-kafka-brokers Opaque my-kafka-cluster-kafka-token-vb2qt kubernetes.io/service-account-token my-kafka-cluster-zookeeper-nodes Opaque my-kafka-cluster-zookeeper-token-xq8m2 kubernetes.io/service-account-token my-kafka-cluster-cluster-ca-cert - Cluster CA certificate to sign Kafka broker certificates, and is used by a connecting client to establish a TLS encrypted connection my-kafka-cluster-clients-ca-cert - Client CA certificate for a user to sign its own client certificate to allow mutual authentication against the Kafka cluster Ok, but does it work? Let’s take it for a spin!\nCreate a producer Pod:\nexport KAFKA_CLUSTER_NAME=my-kafka-cluster kubectl run kafka-producer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list $KAFKA_CLUSTER_NAME-kafka-bootstrap:9092 --topic my-topic In another terminal, create a consumer Pod:\nexport KAFKA_CLUSTER_NAME=my-kafka-cluster kubectl run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_CLUSTER_NAME-kafka-bootstrap:9092 --topic my-topic --from-beginning The above demonstration was taken from the Strimzi doc - https://strimzi.io/docs/operators/master/deploying.html#deploying-example-clients-str\nYou can use other clients as well\nWe’re just getting started… We started small, but have a Kafka cluster on Kubernetes, and it works (hopefully for you as well!). As I mentioned before, this is the beginning of a multi-part blog series. Stay tuned for upcoming posts where we will explore other aspects such as external client access, TLS access, authentication, persistence etc.","what-do-i-need-to-try-this-out#What do I need to try this out?":"","whats-next#What\u0026rsquo;s next?":""},"title":"Kafka on Kubernetes, the Strimzi way! (Part 1)"},"/blog/kafka-kubernetes-strimzi-2/":{"data":{"":"","lets-create-an-externally-accessible-kafka-cluster#Let\u0026rsquo;s create an externally accessible Kafka cluster":"","thats-all-for-now-but-there-is-more-to-come#That\u0026rsquo;s all for now, but there is more to come!":"We kicked off the the first part of the series by setting up a single node Kafka cluster which was accessible to only internal clients within the same Kubernetes cluster, had no encryption, authentication or authorization and used temporary persistence. We will keep iterating/improving on this during the course of this blog series.\nThis part will cover these topics:\nExpose Kafka cluster to external applications Apply TLS encryption Explore Kubernetes resources behind the scenes Use Kafka CLI and Go client applications to test our cluster setup The code is available on GitHub - https://github.com/abhirockzz/kafka-kubernetes-strimzi/\nWhat do I need to try this out? kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl/\nI will be using Azure Kubernetes Service (AKS) to demonstrate the concepts, but by and large it is independent of the Kubernetes provider (e.g. feel free to use a local setup such as minikube). If you want to use AKS, all you need is a Microsoft Azure account which you can get for FREE if you don’t have one already.\nI will not be repeating some of the common sections (such as Installation/Setup for Helm, Strimzi, Azure Kubernetes Service as well as Strimzi overview) in this or subsequent part of this series and would request you to refer to part one for those details\nLet’s create an externally accessible Kafka cluster To achieve this, we just need to tweak the Strimzi Kafka resource a little bit. I am highlighting the key part below - here is the original manifest from part 1\nspec: kafka: version: 2.4.0 replicas: 1 listeners: plain: {} external: type: loadbalancer tls: true What changed?\nTo make Kafka accessible to external client applications, we added an external listener of type loadbalancer. Since we will exposing our application to the public Internet, we need additional layers of protection such as transport level (TLS/SSL encryption) and application level security (authentication and authorization). In this part, we will just configure encryption and explore the other aspects in another blog. To configure end-to-end TLS encryption, we add tls: true\ntls: true config is actually used as a default, but I have added it explicitly for sake of clarity\nTo create the cluster:\nkubectl apply -f https://github.com/abhirockzz/kafka-kubernetes-strimzi/raw/master/part-2/kafka.yaml Kubernetes magic! The Strimzi Operator kicks into action and does all the heavy lifting for us:\nIt creates a Kubernetes LoadBalancer Service.. .. and seeding the appropriate Kafka server configuration in a ConfigMap I will be highlighting the resources created corresponding to the external listener and TLS encryption. For a walk through of ALL the resources which are created as part of the Kafka cluster, please refer to part 1\nIf you look for the Services, you will see something similar to this:\nkubectl get svc my-kafka-cluster-kafka-0 LoadBalancer 10.0.162.98 40.119.233.2 9094:31860/TCP 60s my-kafka-cluster-kafka-bootstrap ClusterIP 10.0.200.20 \u003cnone\u003e 9091/TCP,9092/TCP 60s my-kafka-cluster-kafka-brokers ClusterIP None \u003cnone\u003e 9091/TCP,9092/TCP 60s my-kafka-cluster-kafka-external-bootstrap LoadBalancer 10.0.122.211 20.44.239.202 9094:32267/TCP 60s my-kafka-cluster-zookeeper-client ClusterIP 10.0.137.33 \u003cnone\u003e 2181/TCP 82s my-kafka-cluster-zookeeper-nodes ClusterIP None \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 82s Notice the my-kafka-cluster-kafka-external-bootstrap Service of the type LoadBalancer? Since I am using Azure Kubernetes Service, this is powered by an Azure Load Balancer which has a public IP (20.44.239.202 in this example) and exposes Kafka to external clients over port 9094. You should be able to locate it using the Azure CLI (or the Azure portal if you prefer) by using the az network lb list command\nexport AKS_RESOURCE_GROUP=[replace with resource group name] export AKS_CLUSTER_NAME=[replace with AKS cluster name] export AKS_LOCATION=[replace with region e.g. southeastasia] az network lb list -g MC_${AKS_RESOURCE_GROUP}_${AKS_CLUSTER_NAME}_${AKS_LOCATION} What about the encryption part?\nTo figure that out, let’s introspect the Kafka server configuration:\nAs explained in the previous blog, this is stored in a ConfigMap\nexport CLUSTER_NAME=my-kafka-cluster kubectl get configmap/${CLUSTER_NAME}-kafka-config -o yaml This is what the Common listener configuration in server.config reveals:\nlisteners=REPLICATION-9091://0.0.0.0:9091,PLAIN-9092://0.0.0.0:9092,EXTERNAL-9094://0.0.0.0:9094 advertised.listeners=REPLICATION-9091://my-kafka-cluster-kafka-${STRIMZI_BROKER_ID}.my-kafka-cluster-kafka-brokers.default.svc:9091,PLAIN-9092://my-kafka-cluster-kafka-${STRIMZI_BROKER_ID}.my-kafka-cluster-kafka-brokers.default.svc:9092,EXTERNAL-9094://${STRIMZI_EXTERNAL_9094_ADVERTISED_HOSTNAME}:${STRIMZI_EXTERNAL_9094_ADVERTISED_PORT} listener.security.protocol.map=REPLICATION-9091:SSL,PLAIN-9092:PLAINTEXT,EXTERNAL-9094:SSL Notice that in addition to inter-broker replication (over port 9091) and un-encrypted internal (within Kubernetes cluster) client access over non TLS port 9092, appropriate listener config has been added for TLS encrypted access over port 9094\nThe moment of truth…. To confirm, let’s try out a couple of client applications which will communicate with our freshly minted Kafka cluster on Kubernetes! We will produce and consume messages using the following:\nKafka CLI (console) producer and consumer Go application (using the Confluent Kafka Go client) Communication to our Kafka cluster has to be encrypted (non TLS client connections will be rejected). TLS/SSL implicitly implies one way authentication, where the client validates the Kafka broker identity. In order to do this, client applications need to trust the cluster CA certificate. Remember that the cluster CA certificate is stored in a Kubernetes Secret (refer to details in part 1). By default, these are auto-generated by Strimzi, but you can provide your own certificates as well (refer https://strimzi.io/docs/operators/master/using.html#kafka-listener-certificates-str)\nStart by extracting the cluster CA certificate and password:\nexport CLUSTER_NAME=my-kafka-cluster kubectl get secret $CLUSTER_NAME-cluster-ca-cert -o jsonpath='{.data.ca\\.crt}' | base64 --decode \u003e ca.crt kubectl get secret $CLUSTER_NAME-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' | base64 --decode \u003e ca.password You should have two files: ca.crt and ca.password. Feel free to check out their contents\nWhile some Kafka clients (e.g. Confluent Go client) use the CA certificate directly, others (e.g. Java client, Kafka CLI etc.) require access to the CA certificate via a truststore. I am using the built-in truststore which comes in with a JDK (Java) installation - but this is just for convenience and you’re free to use other options (such as creating your own)\nexport CERT_FILE_PATH=ca.crt export CERT_PASSWORD_FILE_PATH=ca.password # replace this with the path to your truststore export KEYSTORE_LOCATION=/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home/jre/lib/security/cacerts export PASSWORD=`cat $CERT_PASSWORD_FILE_PATH` export CA_CERT_ALIAS=strimzi-kafka-cert # you will prompted for the truststore password. for JDK truststore, the default password is \"changeit\" # Type yes in response to the 'Trust this certificate? [no]:' prompt sudo keytool -importcert -alias $CA_CERT_ALIAS -file $CERT_FILE_PATH -keystore $KEYSTORE_LOCATION -keypass $PASSWORD sudo keytool -list -alias $CA_CERT_ALIAS -keystore $KEYSTORE_LOCATION That’s it for the base setup - you are ready to try out the Kafka CLI client!\nPlease note that the configuration steps for the Kafka CLI as detailed below will also work for the Java clients as well - give it a try!\nExtract the LoadBalancer public IP for Kafka cluster\nexport KAFKA_CLUSTER_NAME=my-kafka-cluster kubectl get service/${KAFKA_CLUSTER_NAME}-kafka-external-bootstrap --output=jsonpath={.status.loadBalancer.ingress[0].ip} Create a file called client-ssl.properties with the following contents:\nbootstrap.servers=[LOADBALANCER_PUBLIC_IP]:9094 security.protocol=SSL ssl.truststore.location=[TRUSTSTORE_LOCATION] //for JDK truststore, the default password is \"changeit\" ssl.truststore.password=changeit To use the Kafka CLI, download Kafka if you don’t have it already - https://kafka.apache.org/downloads\nAll you need to do is use the kafka-console-producer and kafka-console-consumer by pointing it to the client-ssl.properties file you just created\nexport KAFKA_HOME=[replace with Kafka installation path] e.g. /Users/foobar/kafka_2.12-2.3.0 export LOADBALANCER_PUBLIC_IP=[replace with public IP of Load Balancer] export TOPIC_NAME=test-strimzi-topic # on a terminal, start producer and send a few messages $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list $LOADBALANCER_PUBLIC_IP:9094 --topic $TOPIC_NAME --producer.config client-ssl.properties # on another terminal, start consumer $KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server $LOADBALANCER_PUBLIC_IP:9094 --topic $TOPIC_NAME --consumer.config client-ssl.properties --from-beginning You should see producer and consumer working in tandem. Great!\nIf you face SSL Handshake errors, please check whether the CA cert has been correctly imported along with its correct password. If the Kafka cluster is not reachable, ensure you are using the right value for the public IP\nNow, let’s try a programmatic client. Since the Java client behavior (required config properties) are same as the CLI, I am using a Go client to try something different. Don’t worry, if you are not a Go programmer, it should be easy to follow along - I will not walk through the entire program, just the part where we create the connection related configuration.\nHere is the snippet:\nbootstrapServers = os.Getenv(\"KAFKA_BOOTSTRAP_SERVERS\") caLocation = os.Getenv(\"CA_CERT_LOCATION\") topic = os.Getenv(\"KAFKA_TOPIC\") config := \u0026kafka.ConfigMap{\"bootstrap.servers\": bootstrapServers, \"security.protocol\": \"SSL\", \"ssl.ca.location\": caLocation} Notice that the bootstrap.servers and security.protocol are the same as ones you used in the Kafka CLI client (same for Java as well). The only difference is that ssl.ca.location is used to point to the CA certificate directly as opposed to a truststore\nIf you have Go installed, you can try it out. Clone the Git repo…\ngit clone https://github.com/abhirockzz/kafka-kubernetes-strimzi cd part-2/go-client-app .. and run the program:\nexport KAFKA_BOOTSTRAP_SERVERS=[replace with loadbalancer_ip:9094] e.g. 42.42.424.424:9094 export CA_CERT_LOCATION=[replace with path to ca.crt file which you downloaded] export KAFKA_TOPIC=test-strimzi-topic go run kafka-client.go You should see logs similar to this and confirm that messages are being produced and consumed\npress ctrl+c to exit the app\nstarted consumer started producer delivery goroutine started producer goroutine delivered messaged test-strimzi-topic[0]@122 delivered messaged test-strimzi-topic[0]@123 delivered messaged test-strimzi-topic[0]@124 received message from test-strimzi-topic[0]@122: value-2020-06-08 16:23:05.913303 +0530 IST m=+0.020529419 received message from test-strimzi-topic[0]@123: value-2020-06-08 16:23:07.915252 +0530 IST m=+2.022455867 received message from test-strimzi-topic[0]@124: value-2020-06-08 16:23:09.915875 +0530 IST m=+4.023055601 received message from test-strimzi-topic[0]@125: value-2020-06-08 16:23:11.915977 +0530 IST m=+6.023134961 .... That’s all for now, but there is more to come! So we made some progress! We now have a Kafka cluster on Kubernetes which is publicly accessible but is (partially) secure thanks to TLS encryption. We also did some sanity testing using not one, but two (different) client applications. In the next part, we’ll improve this further… stay tuned!","the-moment-of-truth#The moment of truth\u0026hellip;.":"","what-do-i-need-to-try-this-out#What do I need to try this out?":""},"title":"Kafka on Kubernetes, the Strimzi way! (Part 2)"},"/blog/kafka-kubernetes-strimzi-3/":{"data":{"":"Over the course of the first two parts of this blog series, we setup a single-node Kafka cluster on Kubernetes, secured it using TLS encryption and accessed the broker using both internal and external clients. Let’s keep iterating! In this post, we will continue the Kafka on Kubernetes journey with Strimzi and cover:\nHow to apply different authentication types: TLS and SASL SCRAM-SHA-512 Use Strimzi Entity operator to manage Kafka users and topics How to configure Kafka CLI and Go client applications to securely connect to the Kafka cluster The code is available on GitHub - https://github.com/abhirockzz/kafka-kubernetes-strimzi/","create-a-kafka-cluster-with-tls-authentication#Create a Kafka cluster with TLS authentication":"To enforce 2-way mutual TLS auth, all we need to do is tweak the Strimzi Kafka resource. I am highlighting the key part below. The other parts remain the same (here is the manifest from part 2) i.e. single node Kafka and Zookeeper, ephemeral storage along with TLS encryption\nexternal: type: loadbalancer tls: true authentication: type: tls All we did is all the tls authentication type as a property of the external listener. In addition to this, we also include the entityOperator configuration as such:\nentityOperator: userOperator: {} topicOperator: {} This activates the Strimzi Entity Operator which in turn comprises of the Topic Operator and User Operator. Just as the Kafka CRD allows you to control Kafka clusters on Kubernetes, a Topic Operator allows you to manage topics in a Kafka cluster through a custom resource called KafkaTopic i.e. you can create, delete and update topics in your Kafka cluster.\nThe interesting part is that it’s a two-way sync i.e. you can still create topics by accessing the Kafka cluster directly and it would reflect in the KafkaTopic resources being created/updated/deleted\nThe goal of the User Operator is to make Kafka user management easier with help of a KafkaUser CRD. All you do is create instances of KafkaUser CRDs and Strimzi takes care of the Kafka specific user management parts\nUnlike Topic Operator, this is not a two-way sync\nRead more about Entity Operator here https://strimzi.io/docs/operators/master/using.html#assembly-kafka-entity-operator-deployment-configuration-kafka\nWe will dive into the practical bit of these two operators in upcoming sections.\nTo create the Kafka cluster:\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-3/kafka-tls-auth.yaml What did the Strimzi Operator do for us in this case?\nWe covered most of these in part 1 - StatefulSet (and Pods), LoadBalancer Service, ConfigMap, Secret etc. How is the TLS auth config enforced? To figure that out, let’s introspect the Kafka server configuration\nAs explained in part 1, this is stored in a ConfigMap\nexport CLUSTER_NAME=my-kafka-cluster kubectl get configmap/${CLUSTER_NAME}-kafka-config -o yaml Look at the External listener section in server.config:\nlistener.name.external-9094.ssl.client.auth=required listener.name.external-9094.ssl.truststore.location=/tmp/kafka/clients.truststore.p12 listener.name.external-9094.ssl.truststore.password=${CERTS_STORE_PASSWORD} listener.name.external-9094.ssl.truststore.type=PKCS12 The snippet highlighted above is the part which was added - notice listener.name.external-9094.ssl.client.auth=required was added along with the truststore details.\nLet’s not forget the Entity Operator\nThe Entity Operator runs a separate Deployment\nexport CLUSTER_NAME=my-kafka-cluster kubectl get deployment $CLUSTER_NAME-entity-operator kubectl get pod -l=app.kubernetes.io/name=entity-operator NAME READY STATUS my-kafka-cluster-entity-operator-666f8758f6-gj54h 3/3 Running The entity operator Pod runs three containers - topic-operator, user-operator, tls-sidecar\nWe have configured our cluster to authenticate client connections, but what about the user credentials which will be used by client apps?","enforce-scram-sha-512-auth#Enforce SCRAM-SHA-512 auth":"SCRAM stands for “Salted Challenge Response Authentication Mechanism”. I will not pretend to be a security or SCRAM expert, but do want to highlight that it is one of the supported and commonly used authentication mechanism in Kafka (in addition to other such as PLAIN)\nPlease note that Strimzi does not support SASL PLAIN auth at the time of writing\nUpdate the Kafka cluster\nTo apply the SCRAM authentication scheme - all you need is to set the authentication.type to scram-sha-512\nexternal: type: loadbalancer tls: true authentication: type: scram-sha-512 Update the Kafka cluster to use SCRAM-SHA authentication\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-3/kafka-tls-auth.yaml Let’s take a look at how the Kafka server config looks like in this case:\nexport CLUSTER_NAME=my-kafka-cluster kubectl get configmap/${CLUSTER_NAME}-kafka-config -o yaml Introspect External listener section in server.config and notice how the the config has been updated to reflect\nlistener.name.external-9094.scram-sha-512.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required; listener.name.external-9094.sasl.enabled.mechanisms=SCRAM-SHA-512 Create SCRAM credentials (KafkaUser)\nJust like we did with TLS auth, we need to create client credentials for SCRAM as well. It only differs from its TLS equivalent in terms of name and the type (of course!)\napiVersion: kafka.strimzi.io/v1beta1 kind: KafkaUser metadata: name: kafka-scram-client-credentials labels: strimzi.io/cluster: my-kafka-cluster spec: authentication: type: scram-sha-512 notice that authentication.type is scram-sha-512\nCreate the KafkaUser\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-3/user-scram-auth.yaml Introspect the Secret (it has the same name as the KafkaUser):\nkubectl get secret/kafka-scram-client-credentials -o yaml The Secret contains the password in base64 encoded form\napiVersion: v1 kind: Secret name: kafka-scram-client-credentials data: password: SnpteEQwek1DNkdi ... Username is same as the KafkaUser/Secret name, which is kafka-scram-client-credentials in this example\nRun client applications\nIn order run the client examples, download the the password:\nexport USER_NAME=kafka-scram-client-credentials kubectl get secret $USER_NAME -o jsonpath='{.data.password}' | base64 --decode \u003e user-scram.password To test the Kafka CLI client, create a file client-scram-auth.properties with the following contents:\nbootstrap.servers=[replace with public-ip:9094] security.protocol=SASL_SSL sasl.mechanism=SCRAM-SHA-512 ssl.truststore.location=[replace with path to truststore with kafka CA cert] # \"changeit\" is the default password for JDK truststore, please use the one applicable to yours ssl.truststore.password=changeit sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"kafka-scram-client-credentials\" password=\"[replace with contents of user-scram.password file]\"; Refer to the instructions above to run the console producer and consumer\nplease make sure you use the client-scram-auth.properties and not the client-tls-auth.properties file\nBefore wrapping up, lets look at the Go client and see how it handles SCRAM authentication. As always, I will only highlight the part which showcases the configuration:\nbootstrapServers = os.Getenv(\"KAFKA_BOOTSTRAP_SERVERS\") caLocation = os.Getenv(\"CA_CERT_LOCATION\") topic = os.Getenv(\"KAFKA_TOPIC\") kafkaScramUsername = os.Getenv(\"SCRAM_USERNAME\") kafkaScramPassword = os.Getenv(\"SCRAM_PASSWORD\") producerConfig := \u0026kafka.ConfigMap{\"bootstrap.servers\": bootstrapServers, \"security.protocol\": \"SASL_SSL\", \"ssl.ca.location\": caLocation, \"sasl.mechanism\": \"SCRAM-SHA-512\", \"sasl.username\": kafkaScramUsername, \"sasl.password\": kafkaScramPassword} The security.protocol and sasl.mechanism have been updated to SASL_SSL and SCRAM-SHA-512 respectively. Along with that, we use the sasl.username and sasl.password to specify the client credentials\nTo run the Go client app:\nexport KAFKA_BOOTSTRAP_SERVERS=[replace with public-ip:9094] export CA_CERT_LOCATION=[path to ca.crt file] f.g. /Users/code/kafka-kubernetes-strimzi/part-3/ca.crt export KAFKA_TOPIC=strimzi-test-topic export SCRAM_USERNAME=kafka-scram-client-credentials export SCRAM_PASSWORD=[contents of user-scram.password file] go run kafka-scram-auth-client.go ","time-to-use-the-user-operator#Time to use the User Operator!":"The User Operator allows us to create KafkaUsers to represent client authentication credentials. As mentioned in the beginning of the blog post, supported authentication types include TLS and SCRAM-SHA-512. Behind the scenes, a Kubernetes Secret is created by Strimzi to store the credentials\nOAuth 2.0 is also supported but its not handled by the User Operator\nLet’s create a KafkaUser to store client credentials for TLS auth. Here is what the user info looks like:\napiVersion: kafka.strimzi.io/v1beta1 kind: KafkaUser metadata: name: kafka-tls-client-credentials labels: strimzi.io/cluster: my-kafka-cluster spec: authentication: type: tls We name the user kafka-tls-client-credentials, associate with the Kafka cluster we created earlier (using the label strimzi.io/cluster: my-kafka-cluster) and specify the tls authentication type\nYou can also define authorization rules (not covered in this blog) within a KafkaUser definition - see https://strimzi.io/docs/operators/master/using.html#type-KafkaUser-reference\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-3/user-tls-auth.yaml Introspect the Secret (it has the same name as the KafkaUser):\nkubectl get secret/kafka-tls-client-credentials -o yaml ","tls-client-authentication#TLS client authentication":"That’s it! Now its up to the client to use the credentials. We will use a Kafka CLI and Go client application to try this out. First things first:\nExtract and configure the user credentials\nexport KAFKA_USER_NAME=kafka-tls-client-credentials kubectl get secret $KAFKA_USER_NAME -o jsonpath='{.data.user\\.crt}' | base64 --decode \u003e user.crt kubectl get secret $KAFKA_USER_NAME -o jsonpath='{.data.user\\.key}' | base64 --decode \u003e user.key kubectl get secret $KAFKA_USER_NAME -o jsonpath='{.data.user\\.p12}' | base64 --decode \u003e user.p12 kubectl get secret $KAFKA_USER_NAME -o jsonpath='{.data.user\\.password}' | base64 --decode \u003e user.password Import the entry in user.p12 into another keystore\nexport USER_P12_FILE_PATH=user.p12 export USER_KEY_PASSWORD_FILE_PATH=user.password export KEYSTORE_NAME=kafka-auth-keystore.jks export KEYSTORE_PASSWORD=foobar export PASSWORD=`cat $USER_KEY_PASSWORD_FILE_PATH` sudo keytool -importkeystore -deststorepass $KEYSTORE_PASSWORD -destkeystore $KEYSTORE_NAME -srckeystore $USER_P12_FILE_PATH -srcstorepass $PASSWORD -srcstoretype PKCS12 sudo keytool -list -alias $KAFKA_USER_NAME -keystore $KEYSTORE_NAME Just like we did in part 2, TLS encryption config requires importing the cluster CA cert in the client truststore\nExtract and configure server CA cert\nExtract the cluster CA certificate and password\nexport CLUSTER_NAME=my-kafka-cluster kubectl get secret $CLUSTER_NAME-cluster-ca-cert -o jsonpath='{.data.ca\\.crt}' | base64 --decode \u003e ca.crt kubectl get secret $CLUSTER_NAME-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' | base64 --decode \u003e ca.password Import it into truststore - I am using the built-in truststore which comes in with a JDK (Java) installation - but this is just for convenience and you’re free to use other truststore\nexport CERT_FILE_PATH=ca.crt export CERT_PASSWORD_FILE_PATH=ca.password # replace this with the path to your truststore export KEYSTORE_LOCATION=/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home/jre/lib/security/cacerts export PASSWORD=`cat $CERT_PASSWORD_FILE_PATH` # you will prompted for the truststore password. for JDK truststore, the default password is \"changeit\" # Type yes in response to the 'Trust this certificate? [no]:' prompt sudo keytool -importcert -alias strimzi-kafka-cert -file $CERT_FILE_PATH -keystore $KEYSTORE_LOCATION -keypass $PASSWORD sudo keytool -list -alias strimzi-kafka-cert -keystore $KEYSTORE_LOCATION You should now be able to authenticate to the Kafka cluster using the Kafka CLI client\nPlease note that the configuration steps for the Kafka CLI as detailed below will also work for the Java clients as well - feel free to try that out as well\nCreate properties file for Kafka CLI clients\nExtract the LoadBalancer public IP for Kafka cluster\nexport KAFKA_CLUSTER_NAME=my-kafka-cluster kubectl get service/${KAFKA_CLUSTER_NAME}-kafka-external-bootstrap --output=jsonpath={.status.loadBalancer.ingress[0].ip} Create a file called client-ssl-auth.properties with the following contents:\nbootstrap.servers=[LOADBALANCER_PUBLIC_IP]:9094 security.protocol=SSL ssl.truststore.location=[TRUSTSTORE_LOCATION] ssl.truststore.password=changeit ssl.keystore.location=kafka-auth-keystore.jks ssl.keystore.password=foobar ssl.key.password=[contents of user.password file] changeit is the default truststore password. Please use a different one if needed\nDownload Kafka if you don’t have it already - https://kafka.apache.org/downloads\nOne last thing before you proceed\nCreate a KafkaTopic\nAs I mentioned earlier, the Topic Operator makes this possible to embed topic info in form of a KafkaTopic manifest as such:\napiVersion: kafka.strimzi.io/v1beta1 kind: KafkaTopic metadata: name: strimzi-test-topic labels: strimzi.io/cluster: my-kafka-cluster spec: partitions: 3 replicas: 1 To create the topic:\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-3/topic.yaml Here is the reference for a KafkaTopic CRD https://strimzi.io/docs/operators/master/using.html#type-KafkaTopic-reference\nAll you need to do is use the kafka-console-producer and kafka-console-consumer by pointing it to the client-ssl-auth.properties file you just created\nexport KAFKA_HOME=[replace with kafka installation] e.g. /Users/foobar/kafka_2.12-2.3.0 export LOADBALANCER_PUBLIC_IP=[replace with public-ip] export TOPIC_NAME=strimzi-test-topic # on a terminal, start producer and send a few messages $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list $LOADBALANCER_PUBLIC_IP:9094 --topic $TOPIC_NAME --producer.config client-ssl-auth.properties # on another terminal, start consumer $KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server $LOADBALANCER_PUBLIC_IP:9094 --topic $TOPIC_NAME --consumer.config client-ssl-auth.properties --from-beginning You should see producer and consumer working in tandem. Great!\nIf you face SSL Handshake errors, please check whether keys and certificates has been correctly imported and you’re using the correct password. If the Kafka cluster is not reachable, ensure you are using the right value for the public IP\nNow, let’s try a programmatic client. Since the Java client behavior (required config properties) are same as the CLI, I am using a Go client to try something different. Don’t worry, if you are not a Go programmer, it should be easy to follow along.\nI will not walk through the entire program, just the part where we create the connection related configuration. Here is the snippet:\nbootstrapServers = os.Getenv(\"KAFKA_BOOTSTRAP_SERVERS\") caLocation = os.Getenv(\"CA_CERT_LOCATION\") topic = os.Getenv(\"KAFKA_TOPIC\") userCertLocation = os.Getenv(\"USER_CERT_LOCATION\") userKeyLocation = os.Getenv(\"USER_KEY_LOCATION\") userKeyPassword = os.Getenv(\"USER_KEY_PASSWORD\") producerConfig := \u0026kafka.ConfigMap{\"bootstrap.servers\": bootstrapServers, \"security.protocol\": \"SSL\", \"ssl.ca.location\": caLocation, \"ssl.certificate.location\": userCertLocation, \"ssl.key.location\": userKeyLocation, \"ssl.key.password\": userKeyPassword} Notice that the bootstrap.servers and security.protocol are the same as ones you used in the Kafka CLI client (same for Java as well).\nFor TLS encryption: ssl.ca.location is used to point to the CA certificate directly as opposed to a truststore For client authentication: ssl.certificate.location, ssl.key.location and ssl.key.password refer to the user certificate, user key and password respectively If you have Go installed, you can try it out. Clone the Git repo\ngit clone https://github.com/abhirockzz/kafka-kubernetes-strimzi cd part-3/go-client-app .. and run the program:\nexport KAFKA_BOOTSTRAP_SERVERS=[replace with public-ip:9094] e.g. 20.43.176.7:9094 export CA_CERT_LOCATION=[replace with location of ca.crt file] e.g. /Users/code/kafka-kubernetes-strimzi/part-3/ca.crt export KAFKA_TOPIC=test-strimzi-topic export USER_CERT_LOCATION=[path to user.crt file] e.g. /Users/code/kafka-kubernetes-strimzi/part-3/user.crt export USER_KEY_LOCATION=[path to user.key file] e.g. /Users/code/kafka-kubernetes-strimzi/part-3/user.key export USER_KEY_PASSWORD=[contents of user.password file] go run kafka-tls-auth-client.go The logs should confirm whether messages are being produced and consumed","what-do-i-need-to-go-through-this-tutorial#What do I need to go through this tutorial?":"kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl/\nI will be using Azure Kubernetes Service (AKS) to demonstrate the concepts, but by and large it is independent of the Kubernetes provider (e.g. feel free to use a local setup such as minikube). If you want to use AKS, all you need is a Microsoft Azure account which you can get for FREE if you don’t have one already.\nI will not be repeating some of the common sections (such as Installation/Setup (Helm, Strimzi, Azure Kubernetes Service), Strimzi overview) in this or subsequent part of this series and would request you to refer to part one","wrap-up-for-now#Wrap up.. for now":"This post covered a decent amount of ground! We learnt how to apply different authentication types, use Entity Operators to manage Kafka users and topics and more importantly, understand how client applications need to configured to connect securely using a combination of TLS encryption and the chosen authentication scheme.\nWe’re far from done! All this while, we’ve been creating ephemeral clusters with no persistence - we will fix that in upcoming posts."},"title":"Kafka on Kubernetes, the Strimzi way! (Part 3)"},"/blog/kafka-kubernetes-strimzi-4/":{"data":{"":"","add-persistence#Add persistence":"","final-countdown-#Final countdown \u0026hellip;":"","its-a-wrap#It\u0026rsquo;s a wrap!":"Welcome to part four of this blog series! So far, we have a Kafka single-node cluster with TLS encryption on top of which we configured different authentication modes (TLS and SASL SCRAM-SHA-512), defined users with the User Operator, connected to the cluster using CLI and Go clients and saw how easy it is to manage Kafka topics with the Topic Operator. So far, our cluster used ephemeral persistence, which in the case of a single-node cluster, means that we will lose data if the Kafka or Zookeeper nodes (Pods) are restarted due to any reason.\nLet’s march on! In this part we will cover:\nHow to configure Strimzi to add persistence for our cluster: Explore the components such as PersistentVolume and PersistentVolumeClaim How to modify the storage quality Try and expand the storage size for our Kafka cluster The code is available on GitHub - https://github.com/abhirockzz/kafka-kubernetes-strimzi/\nWhat do I need to go through this tutorial? kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl/\nI will be using Azure Kubernetes Service (AKS) to demonstrate the concepts, but by and large it is independent of the Kubernetes provider. If you want to use AKS, all you need is a Microsoft Azure account which you can get for FREE if you don’t have one already.\nI will not be repeating some of the common sections (such as Installation/Setup (Helm, Strimzi, Azure Kubernetes Service), Strimzi overview) in this or subsequent part of this series and would request you to refer to part one\nAdd persistence We will start off by creating a persistent cluster. Here is a snippet of the specification (you can access the complete YAML on GitHub)\napiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: my-kafka-cluster spec: kafka: version: 2.4.0 replicas: 1 storage: type: persistent-claim size: 2Gi deleteClaim: true .... zookeeper: replicas: 1 storage: type: persistent-claim size: 1Gi deleteClaim: true The key things to notice:\nstorage.type is persistent-claim (as opposed to ephemeral) in previous examples storage.size for Kafka and Zookeeper nodes is 2Gi and 1Gi respectively deleteClaim: true means that the corresponding PersistentVolumeClaims will be deleted when the cluster is deleted/un-deployed You can take a look at the reference for storage https://strimzi.io/docs/operators/master/using.html#type-PersistentClaimStorage-reference\nTo create the cluster:\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-4/kafka-persistent.yaml Let’s see the what happens in response to the cluster creation\nStrimzi Kubernetes magic… Strimzi does all the heavy lifting of creating required Kubernetes resources in order to operate the cluster. We covered most of these in part 1 - StatefulSet (and Pods), LoadBalancer Service, ConfigMap, Secret etc. In this blog, we will just focus on the persistence related components - PersistentVolume and PersistentVolumeClaim\nIf you’re using Azure Kubernetes Service (AKS), this will create an Azure Managed Disk - more on this soon\nTo check the PersistentVolumeClaims\nkubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-my-kafka-cluster-kafka-0 Bound pvc-b4ece32b-a46c-4fbc-9b58-9413eee9c779 2Gi RWO default 94s data-my-kafka-cluster-zookeeper-0 Bound pvc-d705fea9-c443-461c-8d18-acf8e219eab0 1Gi RWO default 3m20s … and the PersistentVolumes they are Bound to\nkubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-b4ece32b-a46c-4fbc-9b58-9413eee9c779 2Gi RWO Delete Bound default/data-my-kafka-cluster-kafka-0 default 107s pvc-d705fea9-c443-461c-8d18-acf8e219eab0 1Gi RWO Delete Bound default/data-my-kafka-cluster-zookeeper-0 default 3m35s Notice that the disk size is as specified in the manifest ie. 2 and 1 Gib for Kafka and Zookeeper respectively\nWhere is the data? If we want to see the data itself, let’s first check the ConfigMap which stores the Kafka server config:\nexport CLUSTER_NAME=my-kafka-cluster kubectl get configmap/${CLUSTER_NAME}-kafka-config -o yaml In server.config section, you will find an entry as such:\n########## # Kafka message logs configuration ########## log.dirs=/var/lib/kafka/data/kafka-log${STRIMZI_BROKER_ID} This tells us that the Kafka data is stored in /var/lib/kafka/data/kafka-log${STRIMZI_BROKER_ID}. In this case STRIMZI_BROKER_ID is 0 since we all we have is a single node\nWith this info, let’s look the the Kafka Pod:\nexport CLUSTER_NAME=my-kafka-cluster kubectl get pod/${CLUSTER_NAME}-kafka-0 -o yaml If you look into the kafka container section, you will notice the following:\nOne of the volumes configuration:\nvolumes: - name: data persistentVolumeClaim: claimName: data-my-kafka-cluster-kafka-0 The volume named data is associated with the data-my-kafka-cluster-kafka-0 PVC, and the corresponding volumeMounts uses this volume to ensure that Kafka data is stored in /var/lib/kafka/data\nvolumeMounts: - mountPath: /var/lib/kafka/data name: data To see the contents,\nexport STRIMZI_BROKER_ID=0 kubectl exec -it my-kafka-cluster-kafka-0 -- ls -lrt /var/lib/kafka/data/kafka-log${STRIMZI_BROKER_ID} You can repeat the same for Zookeeper node as well\n…what about the Cloud? As mentioned before, in case of AKS, the data will end up being stored in an Azure Managed Disk. The type of disk is as per the default storage class in your AKS cluster. In my case, it is:\nkubectl get sc azurefile kubernetes.io/azure-file 58d azurefile-premium kubernetes.io/azure-file 58d default (default) kubernetes.io/azure-disk 2d18h managed-premium kubernetes.io/azure-disk 2d18h //to get details of the storage class kubectl get sc/default -o yaml More on the semantics for default storage class in AKS in the documentation\nTo query the disk in Azure, extract the PersistentVolume info using kubectl get pv/\u003cname of kafka pv\u003e -o yaml and get the ID of the Azure Disk i.e. spec.azureDisk.diskURI\nYou can use the Azure CLI command az disk show command\naz disk show --ids \u003cdiskURI value\u003e You will see that the storage type as defined in sku section is StandardSSD_LRS which corresponds to a Standard SSD\nThis table provides a comparison of different Azure Disk types\n\"sku\": { \"name\": \"StandardSSD_LRS\", \"tier\": \"Standard\" } … and the tags attribute highlight the PV and PVC association\n\"tags\": { \"created-by\": \"kubernetes-azure-dd\", \"kubernetes.io-created-for-pv-name\": \"pvc-b4ece32b-a46c-4fbc-9b58-9413eee9c779\", \"kubernetes.io-created-for-pvc-name\": \"data-my-kafka-cluster-kafka-0\", \"kubernetes.io-created-for-pvc-namespace\": \"default\" } You can repeat the same for Zookeeper disks as well\nQuick test … Follow these steps to confirm that the cluster is working as expected..\nCreate a producer Pod:\nexport KAFKA_CLUSTER_NAME=my-kafka-cluster kubectl run kafka-producer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list $KAFKA_CLUSTER_NAME-kafka-bootstrap:9092 --topic my-topic In another terminal, create a consumer Pod:\nexport KAFKA_CLUSTER_NAME=my-kafka-cluster kubectl run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_CLUSTER_NAME-kafka-bootstrap:9092 --topic my-topic --from-beginning What if(s) … Let’s explore how to tackle a couple of requirements which you’ll come across:\nUsing a different storage type - In case of Azure for example, you might want to use Azure Premium SSD for production workloads Re-sizing the storage - at some point you’ll want to add storage to your Kafka cluster Change the storage type Recall that the default behavior is for Strimzi to create a PersistentVolumeClaim that references the default Storage Class. To customize this, you can simply include the class attribute in the storage specification in spec.kafka (and/or spec.zookeeper).\nIn Azure, the managed-premium storage class corresponds to a Premium SSD: kubectl get sc/managed-premium -o yaml\nHere is a snippet from the storage config, where class: managed-premium has been added.\nstorage: type: persistent-claim size: 2Gi deleteClaim: true class: managed-premium Please note that you cannot update the storage type for an existing cluster. To try this out:\nDelete the existing cluster - kubectl delete kafka/my-kafka-cluster (wait for a while) Create a new cluster - kubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-4/kafka-persistent-premium.yaml //Delete the existing cluster kubectl delete kafka/my-kafka-cluster //Create a new cluster kubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-4/kafka-persistent-premium.yaml To confirm, check the PersistentVolumeClain for Kafka node - notice the STORAGECLASS colum\nkubectl get pvc/data-my-kafka-cluster-kafka-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-my-kafka-cluster-kafka-0 Bound pvc-3f46d6ed-9da5-4c49-87ef-86684ab21cf8 2Gi RWO managed-premium 21s We only configured the Kafka broker to use the Premium storage, so the Zookeeper Pod will use the StandardSSD storage type.\nRe-size storage (TL;DR - does not work yet) Azure Disks allow you to add more storage to it. In the case of Kubernetes, it is the storage class which defines whether this is supported or not - for AKS, if you check the default (or the managed-premium) storage class, you will notice the property allowVolumeExpansion: true, which confirms that you can do so in the context of Kubernetes PVC as well.\nStrimzi makes it really easy to increase the storage for our Kafka cluster - all you need to do is update the storage.size field to the desired value\nCheck the PVC now: kubectl describe pvc data-my-kafka-cluster-kafka-0\nConditions: Type Status LastProbeTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- Resizing True Mon, 01 Jan 0001 00:00:00 +0000 Mon, 22 Jun 2020 23:15:26 +0530 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning VolumeResizeFailed 3s (x11 over 13s) volume_expand error expanding volume \"default/data-my-kafka-cluster-kafka-0\" of plugin \"kubernetes.io/azure-disk\": compute.DisksClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status=\u003cnil\u003e Code=\"OperationNotAllowed\" Message=\"Cannot resize disk kubernetes-dynamic-pvc-3f46d6ed-9da5-4c49-87ef-86684ab21cf8 while it is attached to running VM /subscriptions/9a42a42f-ae42-4242-b6a7-dda0ea91d342/resourceGroups/mc_my-k8s-vk_southeastasia/providers/Microsoft.Compute/virtualMachines/aks-agentpool-42424242-1. Resizing a disk of an Azure Virtual Machine requires the virtual machine to be deallocated. Please stop your VM and retry the operation.\" Notice the \"Cannot resize disk... error message. This is happening because the Azure Disk is currently attached with AKS cluster node and that is because of the Pod is associated with the PersistentVolumeClaim - this is a documented limitation\nI am not the first one to run into this problem of course. Please refer to issues such as this one for details.\nThere are workarounds but they have not been discussed in this blog. I included the section since I wanted you to be aware of this caveat\nFinal countdown … We want to leave on a high note, don’t we? Alright, so to wrap it up, let’s scale our cluster out from one to three nodes. It’d dead simple!\nAll you need to do is to increase the replicas to the desired number - in this case, I configured it to 3 (for Kafka and Zookeeper)\n... spec: kafka: version: 2.4.0 replicas: 3 zookeeper: replicas: 3 ... In addition to this, I also added an external load balancer listener (this will create an Azure Load Balancer, as discussed in part 2)\n... listeners: plain: {} external: type: loadbalancer ... To create the new, simply use the new manifest\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kafka-kubernetes-strimzi/master/part-4/kafka-persistent-multi-node.yaml Please note that the overall cluster readiness will take time since there will be additional components (Azure Disks, Load Balancer public IPs etc.) that’ll be created prior to the Pods being activated\nIn your k8s cluster, you will see… Three Pods each for Kafka and Zookeeper\nkubectl get pod -l=app.kubernetes.io/instance=my-kafka-cluster NAME READY STATUS RESTARTS AGE my-kafka-cluster-kafka-0 2/2 Running 0 54s my-kafka-cluster-kafka-1 2/2 Running 0 54s my-kafka-cluster-kafka-2 2/2 Running 0 54s my-kafka-cluster-zookeeper-0 1/1 Running 0 4m44s my-kafka-cluster-zookeeper-1 1/1 Running 0 4m44s my-kafka-cluster-zookeeper-2 1/1 Running 0 4m44s Three pairs (each for Kafka and Zookeeper) of PersistentVolumeClaims …\nkubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-my-kafka-cluster-kafka-0 Bound pvc-0f52dee1-970a-4c55-92bd-a97dcc41aee6 3Gi RWO managed-premium 10m data-my-kafka-cluster-kafka-1 Bound pvc-f8b613cb-3da0-4932-acea-7e5e96df1433 3Gi RWO managed-premium 4m24s data-my-kafka-cluster-kafka-2 Bound pvc-fedf431c-d87a-4bf7-80d0-d43b1337c079 3Gi RWO managed-premium 4m24s data-my-kafka-cluster-zookeeper-0 Bound pvc-1fda3714-3c37-428f-9e4b-bdb5da71cda6 1Gi RWO default 12m data-my-kafka-cluster-zookeeper-1 Bound pvc-702556e0-890a-4c07-ae5c-e2354d74d006 1Gi RWO default 6m42s data-my-kafka-cluster-zookeeper-2 Bound pvc-176ffd68-7e3a-4e04-abb1-52c54dcb84f0 1Gi RWO default 6m42s … and the respective PersistentVolumes they are bound to\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-0f52dee1-970a-4c55-92bd-a97dcc41aee6 3Gi RWO Delete Bound default/data-my-kafka-cluster-kafka-0 managed-premium 12m pvc-176ffd68-7e3a-4e04-abb1-52c54dcb84f0 1Gi RWO Delete Bound default/data-my-kafka-cluster-zookeeper-2 default 8m45s pvc-1fda3714-3c37-428f-9e4b-bdb5da71cda6 1Gi RWO Delete Bound default/data-my-kafka-cluster-zookeeper-0 default 14m pvc-702556e0-890a-4c07-ae5c-e2354d74d006 1Gi RWO Delete Bound default/data-my-kafka-cluster-zookeeper-1 default 8m45s pvc-f8b613cb-3da0-4932-acea-7e5e96df1433 3Gi RWO Delete Bound default/data-my-kafka-cluster-kafka-1 managed-premium 6m27s pvc-fedf431c-d87a-4bf7-80d0-d43b1337c079 3Gi RWO Delete Bound default/data-my-kafka-cluster-kafka-2 managed-premium 6m22s … and Load Balancer IPs. Notice that these are created for each Kafka broker as well as a bootstrap IP which is recommended when connecting from client applications.\nkubectl get svc -l=app.kubernetes.io/instance=my-kafka-cluster NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-kafka-cluster-kafka-0 LoadBalancer 10.0.11.154 40.119.248.164 9094:30977/TCP 10m my-kafka-cluster-kafka-1 LoadBalancer 10.0.146.181 20.43.191.219 9094:30308/TCP 10m my-kafka-cluster-kafka-2 LoadBalancer 10.0.223.202 40.119.249.20 9094:30313/TCP 10m my-kafka-cluster-kafka-bootstrap ClusterIP 10.0.208.187 \u003cnone\u003e 9091/TCP,9092/TCP 16m my-kafka-cluster-kafka-brokers ClusterIP None \u003cnone\u003e 9091/TCP,9092/TCP 16m my-kafka-cluster-kafka-external-bootstrap LoadBalancer 10.0.77.213 20.43.191.238 9094:31051/TCP 10m my-kafka-cluster-zookeeper-client ClusterIP 10.0.3.155 \u003cnone\u003e 2181/TCP 18m my-kafka-cluster-zookeeper-nodes ClusterIP None \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 18m To access the cluster, you can use the steps outlined in part 2\nIt’s a wrap! That’s it for this blog series on which covered some of the aspects of running Kafka on Kubernetes using the open source Strimzi operator.\nIf this topic is of interest to you, I encourage you to check out other solutions such as Confluent operator and Banzai Cloud Kafka operator","quick-test-#Quick test \u0026hellip;":"","strimzi-kubernetes-magic#Strimzi Kubernetes magic\u0026hellip;":"","what-do-i-need-to-go-through-this-tutorial#What do I need to go through this tutorial?":"","what-ifs-#What if(s) \u0026hellip;":""},"title":"Kafka on Kubernetes, the Strimzi way! (Part 4)"},"/blog/kubernetes-services-ngrok/":{"data":{"":"","#":"Very often, there is a need to expose Kubernetes apps running in minikube to the public internet. This post just provides a manual, yet quick and dirty hack for this using ngrok (it’s probably been done before, but here it goes anyway)\nWe’ll use a simple nginx app to test things out i.e. we’ll expose an nginx server (running as a single replica Kubernetes Deployment) as a publicly accessible URL.\nTest app (nginx) Start by creating the nginx Deployment\nkubectl apply -f [https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/nginx-deployment.yaml](https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/nginx-deployment.yaml) Expose it inside the cluster by creating a corresponding Service (typeClusterIP)\nkubectl apply -f [https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/nginx-service.yaml](https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/nginx-service.yaml) Enter ngrok.. To expose the nginx Service we just created, we can create a ngrok deployment which will run the ngrok process with an HTTP tunnel to the nginx Service(using the Service name)\nCreate the ngrok Deployment\nkubectl apply -f [https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/ngrok-deployment.yaml](https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/ngrok-deployment.yaml) Now you need to extract the ngrok URL. The below command does a couple of things\ngets the Pod for the ngrok Deployment\nuses the HTTP endpoint inside of the ngrok Pod to get the details (using kubectl exec inside the running Pod)\nkubectl exec $(kubectl get pods -l=app=ngrok -o=jsonpath=’{.items[0].metadata.name}’) – curl http://localhost:4040/api/tunnels\nOutput will be similar to what you see below — refer to the public_url field to grab the ngrok URL accessible via public internet (in this example, it’s https://b42658ec.ngrok.io)\nAccess the URL and it should lead you to ngnix home page\nnginx in k8s exposed via ngrok\nOptionally…. …. if you want to access the ngrok dashboard\nExpose it using aService (type NodePort)\nkubectl apply -f [https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/ngrok-service.yaml](https://raw.githubusercontent.com/abhirockzz/ngrok-kubernetes/master/ngrok-service.yaml) Check the random port\nkubectl get svc ngrok-service -o=jsonpath='{.spec.ports[?(@.port==4040)].nodePort}' //e.g. 30552 Get the Minikube IP\n$ minikube ip 192.168.99.100 Open it in your browser\n[http://\u003c](http://192.168.99.100)minikube-ip\u003e:\u003cservice-node-port\u003e/status e.g. [http://192.168.99.100:30552/status](http://192.168.99.100:30552/status) You should see the dashboard\nngrok dashboard\nThat’s all there is to it. The code (just a bunch of YAMLs) is on GitHub abhirockzz/ngrok-kubernetes *Contribute to abhirockzz/ngrok-kubernetes development by creating an account on GitHub.*github.com\nCheers!"},"title":"Expose Kubernetes services with ngrok"},"/blog/kubexpose-operator/":{"data":{"":"Say you have a web service running as a Kubernetes Deployment. There are a bunch of ways to access it over a public URL, but Kubexpose makes it easy to do so. It’s a Kubernetes Operator backed by a Custom Resource Definition and the corresponding controller implementation.\nKubexpose built using kubebuilder and available on GitHub\nTo try it out, jump into the next section or scroll down to How it works? to learn more","how-does-it-work#How does it work?":"Behind the scenes, Kubexpose uses the awesome ngrok project to get the job done! When you create a kubexpose resource, the operator:\nCreates a ClusterIP type Service for the Deployment you want to access (naming format: \u003cdeployment name\u003e-svc-\u003ckubexpose resource name\u003e) Creates a Deployment (using this ngrok Docker image) that runs ngrok - which is configured to point to the Service (naming format: \u003cdeployment name\u003e-expose-\u003ckubexpose resource name\u003e). It’s equivalent to starting ngrok as such: ngrok http foo-svc-bar 80 The Deployment and Service and owned and managed by the Kubexpose resource instance.\nEnjoy!","quick-start#Quick start":"Any Kubernetes cluster will work (minikube, kind, Docker Desktop, on the cloud, whatever…).\nTo deploy the operator and required components:\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kubexpose-operator/master/kubexpose-all-in-one.yaml Make sure the Operator is up and running:\nexport OPERATOR_NAMESPACE=kubexpose-operator-system # check Pods kubectl get pods -n $OPERATOR_NAMESPACE # check logs kubectl logs -f $(kubectl get pods --namespace $OPERATOR_NAMESPACE -o=jsonpath='{.items[0].metadata.name}') -c manager -n $OPERATOR_NAMESPACE Create a nginx Deployment to test things out — this is the one you want to expose over the internet using a public URL).\nAlong with it, create a kubexpose resource — which will help you access ngnix Deployment over the Internet!\nkubectl apply -f https://raw.githubusercontent.com/abhirockzz/kubexpose-operator/master/quickstart/nginx.yaml kubectl apply -f https://raw.githubusercontent.com/abhirockzz/kubexpose-operator/master/quickstart/kubexpose.yaml Wait for a few seconds and check the public URL at which the Nginx Deployment can be accessed:\nkubectl get kubexpose/kubexpose-test -o=jsonpath='{.status.url}' Access the public URL using your browser or test it using curl\nConfirm that the Service and Deployment have been created as well:\nkubectl get svc/nginx-test-svc-kubexpose-test kubectl get deployment/nginx-test-expose-kubexpose-test You can try out other scenarios such as trying to Deployment and/or Service - the Operator will reconcile or bring things back to the state as specified in the resource.\nTo delete the kubexpose resource:\nkubectl delete kubexpose/kubexpose-test This will also delete the Service and Deployment which were created for this resource\nTo uninstall the Operator:\nkubectl delete -f https://raw.githubusercontent.com/abhirockzz/kubexpose-operator/master/kubexpose-all-in-one.yaml This will delete the CRD, kubexpose operator and other resources."},"title":"Kubexpose: A Kubernetes Operator, for fun and profit!"},"/blog/lambda-events-kinesis-go/":{"data":{"":"This blog post is for folks interested in learning how to use Golang and AWS Lambda to build a serverless solution. You will be using the aws-lambda-go library along with the AWS Go SDK v2 for an application that will process records from an Amazon Kinesis data stream and store them in a DynamoDB table. But that’s not all! You will also use Go bindings for AWS CDK to implement “Infrastructure-as-code” for the entire solution and deploy it with the AWS CDK CLI.\nWhat’s covered?\nIntroduction Pre-requisites Use AWS CDK to deploy the solution Verify the solution Don’t forget to clean up Code walk through Wrap up ","code-walk-through#Code walk through":"Some of the code (error handling, logging etc.) has been omitted for brevity since we only want to focus on the important parts.\nAWS CDK\nYou can refer to the CDK code here\nWe start by creating the DynamoDB table:\ntable := awsdynamodb.NewTable(stack, jsii.String(\"dynamodb-table\"), \u0026awsdynamodb.TableProps{ PartitionKey: \u0026awsdynamodb.Attribute{ Name: jsii.String(\"email\"), Type: awsdynamodb.AttributeType_STRING}, }) table.ApplyRemovalPolicy(awscdk.RemovalPolicy_DESTROY) We create the Lambda function (CDK will take care of building and deploying the function) and make sure we provide it appropriate permissions to write to the DynamoDB table.\nfunction := awscdklambdagoalpha.NewGoFunction(stack, jsii.String(\"kinesis-function\"), \u0026awscdklambdagoalpha.GoFunctionProps{ Runtime: awslambda.Runtime_GO_1_X(), Environment: \u0026map[string]*string{\"TABLE_NAME\": table.TableName()}, Entry: jsii.String(functionDir), }) table.GrantWriteData(function) Then, we create the Kinesis stream and add that as an event source to the Lambda function.\nkinesisStream := awskinesis.NewStream(stack, jsii.String(\"lambda-test-stream\"), nil) function.AddEventSource(awslambdaeventsources.NewKinesisEventSource(kinesisStream, \u0026awslambdaeventsources.KinesisEventSourceProps{ StartingPosition: awslambda.StartingPosition_LATEST, })) Finally, we export the Kinesis stream and DynamoDB table name as CloudFormation outputs.\nawscdk.NewCfnOutput(stack, jsii.String(\"kinesis-stream-name\"), \u0026awscdk.CfnOutputProps{ ExportName: jsii.String(\"kinesis-stream-name\"), Value: kinesisStream.StreamName()}) awscdk.NewCfnOutput(stack, jsii.String(\"dynamodb-table-name\"), \u0026awscdk.CfnOutputProps{ ExportName: jsii.String(\"dynamodb-table-name\"), Value: table.TableName()}) Lambda function\nYou can refer to the Lambda Function code here\nThe Lambda function handler iterates over each record in the Kinesis stream, and for each of them:\nUnmarshals the JSON payload in the Kinese stream into a Go struct Stores the stream data partition key as the primary key attribute (email) of the DynamoDB table Rest of the information is picked up from the stream data and also stored in the table. func handler(ctx context.Context, kinesisEvent events.KinesisEvent) error { for _, record := range kinesisEvent.Records { data := record.Kinesis.Data var user CreateUserInfo err := json.Unmarshal(data, \u0026user) item, err := attributevalue.MarshalMap(user) if err != nil { return err } item[\"email\"] = \u0026types.AttributeValueMemberS{Value: record.Kinesis.PartitionKey} _, err = client.PutItem(context.Background(), \u0026dynamodb.PutItemInput{ TableName: aws.String(table), Item: item, }) } return nil } type CreateUserInfo struct { Name string `json:\"name\"` City string `json:\"city\"` } ","introduction#Introduction":"Amazon Kinesis is a platform for real-time data processing, ingestion, and analysis. Kinesis Data Streams is a serverless streaming data service (part of the Kinesis streaming data platform, along with Kinesis Data Firehose, Kinesis Video Streams, and Kinesis Data Analytics) that enables developers to collect, process, and analyze large amounts of data in real-time from various sources such as social media, IoT devices, logs, and more. AWS Lambda, on the other hand, is a serverless compute service that allows developers to run their code without having to manage the underlying infrastructure.\nThe integration of Amazon Kinesis with AWS Lambda provides an efficient way to process and analyze large data streams in real-time. A Kinesis data stream is a set of shards and each shard contains a sequence of data records. A Lambda function can act as a consumer application and process data from a Kinesis data stream. You can map a Lambda function to a shared-throughput consumer (standard iterator), or to a dedicated-throughput consumer with enhanced fan-out. For standard iterators, Lambda polls each shard in your Kinesis stream for records using HTTP protocol. The event source mapping shares read throughput with other consumers of the shard.\nAmazon Kinesis and AWS Lambda can be used together to build many solutions including real-time analytics (allowing businesses to make informed decisions), log processing (use logs to proactively identify and address issues in server/applications etc. before they become critical), IoT data processing (analyze device data in real-time and trigger actions based on the results), clickstream analysis (provide insights into user behavior), fraud detection (detect and prevent fraudulent card transactions) and more.\nAs always, the code is available on GitHub","pre-requisites#Pre-requisites":"Before you proceed, make sure you have the Go programming language (v1.18 or higher) and AWS CDK installed.\nClone the GitHub repository and change to the right directory:\ngit clone https://github.com/abhirockzz/kinesis-lambda-events-golang cd kinesis-lambda-events-golang ","use-aws-cdk-to-deploy-the-solution#Use AWS CDK to deploy the solution":"To start the deployment, simply invoke cdk deploy and wait for a bit. You will see a list of resources that will be created and will need to provide your confirmation to proceed.\ncd cdk cdk deploy # output Bundling asset KinesisLambdaGolangStack/kinesis-function/Code/Stage... ✨ Synthesis time: 5.94s This deployment will make potentially sensitive changes according to your current security approval level (--require-approval broadening). Please confirm you intend to make the following modifications: //.... omitted Do you wish to deploy these changes (y/n)? y This will start creating the AWS resources required for our application.\nIf you want to see the AWS CloudFormation template which will be used behind the scenes, run cdk synth and check the cdk.out folder\nYou can keep track of the progress in the terminal or navigate to AWS console: CloudFormation \u003e Stacks \u003e KinesisLambdaGolangStack\nOnce all the resources are created, you can try out the application. You should have:\nA Lambda function A Kinesis stream A DynamoDB table along with a few other components (like IAM roles etc.) ","verify-the-solution#Verify the solution":"You can check the table and Kinesis stream info in the stack output (in the terminal or the Outputs tab in the AWS CloudFormation console for your Stack):\nPublish few messages to the Kinesis stream. For the purposes of this demo, you can use the AWS CLI:\nexport KINESIS_STREAM=\u003center the Kinesis stream name from cloudformation output\u003e aws kinesis put-record --stream-name $KINESIS_STREAM --partition-key user1@foo.com --data $(echo -n '{\"name\":\"user1\", \"city\":\"seattle\"}' | base64) aws kinesis put-record --stream-name $KINESIS_STREAM --partition-key user2@foo.com --data $(echo -n '{\"name\":\"user2\", \"city\":\"new delhi\"}' | base64) aws kinesis put-record --stream-name $KINESIS_STREAM --partition-key user3@foo.com --data $(echo -n '{\"name\":\"user3\", \"city\":\"new york\"}' | base64) Check the DynamoDB table to confirm that the file metadata has been stored. You can use the AWS console or the AWS CLI aws dynamodb scan --table-name \u003center the table name from cloudformation output\u003e\nDon’t forget to clean up Once you’re done, to delete all the services, simply use:\ncdk destroy #output prompt (choose 'y' to continue) Are you sure you want to delete: KinesisLambdaGolangStack (y/n)? You were able to setup and try the complete solution. Before we wrap up, let’s quickly walk through some of important parts of the code to get a better understanding of what’s going the behind the scenes.","wrap-up#Wrap up":"In this blog, you saw an example of how to use Lambda to process messages in a Kinesis stream and store them in DynamoDB, thanks to the Kinesis and Lamdba integration. The entire infrastructure life-cycle was automated using AWS CDK.\nAll this was done using the Go programming language, which is well supported in DynamoDB, AWS Lambda and AWS CDK.\nHappy building!"},"title":"Use Golang for data processing with Amazon Kinesis and AWS Lambda"},"/blog/lambda-function-url-awsome-slack-backend/":{"data":{"":"","build-zip-and-deploy-the-function#Build, zip and deploy the function!":"","clean-up#Clean up":"Once you’re done, delete the function along with the IAM policy and role.\naws lambda delete-function --function-name $FUNC_NAME aws iam detach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam delete-role --role-name $ROLE_NAME ","conclusion#Conclusion":"You configured and deployed a serverless backend for Slack and in the process, learnt about some of the aspects of Lambda Function URLs through the lens of this sample app. I would encourage you to explore other capabilities such as AWS_IAM authentication, CORS config, throttling limits, monitoring etc.\nHappy coding!","configure-slack#Configure Slack":"","how-it-works#How it works":"","pre-requisites#Pre-requisites":"","update-the-function#Update the function":"","youre-all-set#You\u0026rsquo;re all set!":"A combination of AWS Lambda and Amazon API Gateway is a widely-used architecture for serverless microservices and API based solutions. They enable developers to focus on their applications, instead of spending time provisioning and managing servers.\nAPI Gateway is a feature rich offering that includes with support for different API types (HTTP, REST, WebSocket), multiple authentication schemes, API versioning, canary deployments and much more! However, if your requirements are simpler and all you need is an HTTP(S) endpoint for your Lambda function (for example, to serve as a webhook), you can use Lambda Function URLs! When you create a function URL, Lambda automatically generates a unique HTTP(S) endpoint that is dedicated for your Lambda function.\nThis blog post demonstrates how to use Lambda function URL with a practical example. You will build a Go Lambda function to serve as a serverless webhook backend for Slack.\nIt’s a step-by-step guide that covers:\nOverview of the application Configure and deploy the function (along with some gotchas you need to watch out for!) How to configure Slack to enable the end to end integration Test the app and have fun! By the end of this blog, you would have configured, integrated and deployed a useful (and hopefully fun?) app using Lambda function URL. In the process, you will get an overview of this feature that you can utilise when building your own solutions!\nThe code is available on GitHub\nHow it works The sample app presented in this blog is a trimmed down version of Giphy for Slack. The (original) Giphy Slack app returns a bunch of GIFs for a search term and the user can pick one of them. To keep things simple, I’ve tweaked things a bit such that the serverless backend simply returns a (single) random image for a search keyword using the Giphy Random API.\nSince the solution will be integrated as a Slash Command in Slack, the end user (you!) will invoke it from a Slack workspace using /awsome \u003cyour search term\u003e (where awsome is nothing but the name of the slash command). This in turn invokes the Lambda function URL (the configuration is covered later in the blog), which takes care of the rest.\nFor example, invoking it from your Slack workspace using /awsome serverless will return a random GIF (you will try this later!)\nHere is an overview of what the Lambda function does:\nSlack slash command invocation results in a base64 encoded string payload being sent to the Lambda function URL - so the first step is to decode it. The function is only supposed to invoked by Slack and we need to make sure we confirm that. Slack makes this possible by allowing apps to verify requests using a signing secret - the function simply implements a Go version of the signature matching recipe presented here If the signature match is successful (we return an error to the client if it fails), the Slack request is parsed to extract the search text that user sent. Then, the Giphy Random API is invoked with the search term. If we get a successful response, we parse it and send it back to Slack in it’s desired format Finally, the user gets to see a GIF in their Slack workspace!\nI will skip the code walk-through in order to focus on other aspects of the solution, but the function signature deserves a mention - it is similar to what you would’ve used in case of an API Gateway based solution:\nfunc Funcy(r events.LambdaFunctionURLRequest) (events.LambdaFunctionURLResponse, error) { ... } We are using events.LambdaFunctionURLRequest as input and returning events.LambdaFunctionURLResponse. Behind the scenes, Lambda maps the request to an event object before passing it to the function. Finally, the function response is then mapped to an HTTP response that Lambda sends back to the client through the function URL.\nYou can read up on the details in the documentation\nThat’s quite convenient right? You can use API Gateway conventions without actually having to setup and configure one!\nWith that background info, let’s move on to the part where you deploy the function and try it out with Slack. But, before that make sure you have the following ready:\nPre-requisites Create an AWS account (if you do not already have one) and log in. The IAM user that you use must have sufficient permissions to make necessary AWS service calls and manage AWS resources. Install and configure AWS CLI Install Go Install Git Create a Slack workspace if you don’t have one. Create a GIHPY account (it’s free!) and create an app. Each application you create will have its own API Key. Please note down your GIPHY API key as you will be using it later\nClone the Github repo and move into the right directory:\ngit clone https://github.com/abhirockzz/awsome-slack-backend cd awsome-slack-backend/function The subsequent steps use AWS CLI - I’ve purposely used the AWS CLI in order to highlight specific aspects of the process. Please check this tutorial for CloudFormation and SAM\nBuild, zip and deploy the function! export FUNC_NAME=awsome-slack-backend export FUNC_GO_BINARY_NAME=awsome export ZIP_NAME=function.zip GOOS=linux go build -o $FUNC_GO_BINARY_NAME main.go zip function.zip $FUNC_GO_BINARY_NAME First, create an IAM Role for Lambda and attach the AWSLambdaBasicExecutionRole policy:\nexport ROLE_NAME=demo-lambda-role ROLE_ARN=$(aws iam create-role \\ --role-name $ROLE_NAME \\ --assume-role-policy-document '{\"Version\": \"2012-10-17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}' \\ --query 'Role.[Arn]' --output text) aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole Create the function:\naws lambda create-function \\ --function-name $FUNC_NAME \\ --runtime go1.x \\ --zip-file fileb://$ZIP_NAME \\ --handler $FUNC_GO_BINARY_NAME \\ --role $ROLE_ARN After the function gets created, go ahead and add the Function URL:\naws lambda create-function-url-config \\ --function-name $FUNC_NAME \\ --auth-type NONE For the purposes of this sample app, we’re using NONE as the authentication type. This means that the Lambda function URL will be publicly accessible - more on this shortly\nIf you navigate to the AWS console and open the function you just created, you should see the Function URL associated with it:\nLet’s invoke the function - copy the Function URL and paste it in a browser or use any other tool (e.g. curl)\ncurl -i \u003cFUNCTION_URL\u003e You should get a {\"Message\":\"Forbidden\"} response with a HTTP 403 Forbidden status code\nDon’t worry, this is expected - I wanted to make sure you encounter this issue and understand the root cause.\nEven though we use NONE as the authentication scheme, users must still have lambda:InvokeFunctionUrl permissions in order to successfully invoke the function URL. The (slightly) tricky bit is that when you create a function URL (with auth type NONE) via the console or AWS Serverless Application Model (AWS SAM), Lambda automatically creates the resource-based policy statement for you (details in the documentation). That’s not the case if you’re using the AWS CLI (as in this blog), AWS CloudFormation, or the Lambda API directly - you must add permissions yourself.\nLet’s do that:\naws lambda add-permission \\ --function-name $FUNC_NAME \\ --action lambda:InvokeFunctionUrl \\ --statement-id FunctionURLAllowPublicAccess \\ --principal \"*\" \\ --function-url-auth-type NONE To see the policy, navigate to your Function in the AWS console: Configuration \u003e Permissions\nInvoke the function again:\ncurl -i \u003cFUNCTION_URL\u003e This time, you will get a different error with a HTTP 401 Unauthorized status code. This is expected as well!\nLet’s finish the rest of the configuration to get things working.\nConfigure Slack Please note that most of the instructions in this section have been adapted from the Slack documentation\nStart by signing into your Slack Workspace and creating a new Slack App.\nOnce that’s done, create a Slash Command - head to your app’s settings page, and then click the Slash Commands feature in the navigation menu. You’ll be presented with a button marked Create New Command, and when you click on it, you’ll see a screen where you’ll be asked to define your new Slash Command with the required information.\nEnter the required information - Enter /awsome for the Command and enter the Lambda Function URL in Request URL\nFinally, install the app to your workspace - click the Basic Information feature in the navigation menu, choose Install your app to your workspace and click Install App to Workspace. This will install the app to your Slack workspace to test your app and generate the tokens you need to interact with the Slack API.\nAs soon as you finish installing the app, the App Credentials will show up on the same page. You need to grab your Slack Signing Secret from there\nMake a note of your app Signing Secret as you’ll be using it later\nUpdate the function Now that you’ve the Slack signing secret key, you need to make sure to configure it in the function as well. Also, don’t forget the GIPHY API key since the function needs that to invoke GIPHY REST endpoint.\nLet’s update the function to include these as environment variables:\naws lambda update-function-configuration \\ --function-name $FUNC_NAME \\ --environment \"Variables={SLACK_SIGNING_SECRET=\u003center Slack signing secret\u003e,GIPHY_API_KEY=\u003center Giphy API key\u003e}\" The sample app uses Lambda environment variables to store keys for Slack and GIPHY - this is just for demonstration purposes. You should use a solution such as AWS Secrets Manager to securely store and manage credentials.\nYou’re all set! Head over to your Slack workspace and invoke the command. For example, to get a random cat GIF, just type:\n/awsome cat I got this response. How about you? :-)\nFeel free to play around with the app!"},"title":"Using AWS Lambda Function URL to build a Serverless backend for Slack"},"/blog/lambda-function-url-dynamodb-sam/":{"data":{"":"This is a Go Lambda function that’s packaged and deployed using AWS SAM\nLambda Function URL is a relatively new feature (at the time of writing this blog) that provides dedicated HTTP(S) endpoint for your Lambda function. It is really useful when all you need is a single endpoint for your function (e.g. to serve as a webhook) and don’t want to setup and configure an API Gateway. Looks like I can’t seem to get enough of it! I have written a couple of blog posts on this topic that include a practical example of using it to build a serverless backend and then deploying that solution using AWS CDK.\nThis is yet another blog post (it’s a short one!) that demonstrates how you can use Lambda Function URL to write a simple application backed by DynamoDB. You will be able to invoke an API endpoint exposed by the Lambda Function URL,which in turn will execute operations (GetItem, PutItem, Scan) on DynamoDB. The function is written in Go using the DynamoDB package in AWS Go SDK and AWS Serverless Application Model (SAM) is used to quickly build and deploy the solution.\nThe code is available on GitHub for your reference\nLet’s get right to it! Before you move on, make sure you have the following ready:","cleanup#Cleanup":"Once you’re done, please delete the stack:\nsam delete --stack-name STACK_NAME Confirm the stack has been deleted\naws cloudformation list-stacks --query \"StackSummaries[?contains(StackName,'STACK_NAME')].StackStatus\" ","deployment#Deployment":"Start by cloning the Github repo and change to the right directory:\ngit clone https://github.com/abhirockzz/lambda-functionurl-dynamodb-sam-go cd lambda-functionurl-dynamodb-sam-go Use AWS SAM to build the app:\nsam build Now you’re ready to deploy the application:\nsam deploy --guided When prompted, enter the required information such as stack name etc. See example below:\nConfiguring SAM deploy ====================== Looking for config file [samconfig.toml] : Not found Setting default arguments for 'sam deploy' ========================================= Stack Name [sam-app]: AWS Region [us-east-1]: #Shows you resources changes to be deployed and require a 'Y' to initiate deploy Confirm changes before deploy [y/N]: y #SAM needs permission to be able to create roles to connect to the resources in your template Allow SAM CLI IAM role creation [Y/n]: y #Preserves the state of previously provisioned resources when an operation fails Disable rollback [y/N]: n DemoFunction Function Url may not have authorization defined, Is this okay? [y/N]: y Save arguments to configuration file [Y/n]: SAM configuration file [samconfig.toml]: SAM configuration environment [default]: Once you have run sam deploy --guided mode once and saved arguments to a configuration file (samconfig.toml), you can use sam deploy in future to use these defaults.\nIf successful, you should now have the following ready:\nLambda Function with a HTTP(S) URL A DynamoDB table (called users) IAM Role with minimum required permissions for Lambda as well as DynamoDB (PutItem, GetItem, and Scan) Note the output from the SAM deployment process. This contains the HTTP URL endpoint for your function required for testing","pre-requisites#Pre-requisites":" Create an AWS account if you do not already have one and log in. The IAM user that you use must have sufficient permissions to make necessary AWS service calls and manage AWS resources. AWS CLI installed and configured Go (1.16 or above) installed AWS Serverless Application Model (AWS SAM) installed Git Installed ","test-the-application#Test the application":"Testing the integration involves sending an HTTP requests to the Lambda Function URL. This example used the curl CLI, but you can also use other options.\nExport the Lambda Function URL endpoint as an environment variable.\nexport LAMBDA_FUNCTION_URL_ENDPOINT=\u003cSAM deployment output\u003e e.g. export LAMBDA_FUNCTION_URL_ENDPOINT=https://qfmpu2n74ik7m2m3ipv3m6yw5a0qiwmp.lambda-url.us-east-1.on.aws/ Invoke the endpoint to create new entries in the DynamoDB table:\ncurl -i -X POST -H \"Content-Type: application/json\" -d '{\"email\":\"user1@foo.com\", \"username\":\"user-1\"}' $LAMBDA_FUNCTION_URL_ENDPOINT curl -i -X POST -H \"Content-Type: application/json\" -d '{\"email\":\"user2@foo.com\", \"username\":\"user-2\"}' $LAMBDA_FUNCTION_URL_ENDPOINT You should get an HTTP 201 Created response in both the cases. This indicates that the items have been added to the users table in DynamoDB.\nLet’s retrieve the info for the record we just created (we do that by adding a query parameter (email) to the URL):\ncurl -i $LAMBDA_FUNCTION_URL_ENDPOINT?email=user2@foo.com To retrieve all the items that you just added:\ncurl -i $LAMBDA_FUNCTION_URL_ENDPOINT Try to search for an item that you haven’t yet added:\ncurl -i $LAMBDA_FUNCTION_URL_ENDPOINT?email=notthere@foo.com You will get back a HTTP 404 Not Found response in this case.\nFinally, try to insert a duplicate record:\ncurl -i -X POST -d '{\"email\":\"user2@foo.com\", \"username\":\"user-2\"}' $LAMBDA_FUNCTION_URL_ENDPOINT The Lambda function returns an HTTP 409 Conflict in this case since a Condition Expression (attribute_not_exists(email)) prevents an existing item (with the same email) from being overwritten by the PutItem call.\nThat’s it! You have tried all the operations exposed by the Lambda function.","wrap-up#Wrap up!":"This was a short (but hopefully useful) tutorial! You can use this to quickly bootstrap a Serverless app with a DynamoDB backend with functionality exposed by a Lambda Function URL.\nHappy coding!"},"title":"Use Lambda Function URL to write a Serverless app backed by DynamoDB"},"/blog/lambda-function-url-slack-backend-cdk/":{"data":{"":"Deploy a Serverless backend for Slack using Infrastructure-as-code (IaaC)\nOne of my previous blog post covered how to build a Serverless backend for Slack using by using Lambda Function URL as a webhook. Since I wanted to focus on the application itself, the infrastructure setup part was simplified - using AWS CLI, the function was packaged as a zip file, configured and finally a Function URL was created along with the required permissions.\nIn this blog, you will end up deploying the same solution, but this time using IaaC (Infrastructure-as-code) with AWS Cloud Development Kit (CDK) which is a framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. You can choose from a list of supported programming languages (at the time of writing - TypeScript, JavaScript, Python, Java, C#/.Net, and Go (in developer preview)) to define your infrastructure components as code, just like you would with any other application!\nYou will learn how to use the Go CDK library to deal with the infrastructure components:\nDefine a Lambda function, Add a Lambda Function URL, and, Deploy the function as a Docker container (not a zip file) By the end of this blog post, you should have the same setup as described in the earlier blog.\nThe code is available on GitHub, as always!","cdk-for-iaac---quick-walkthrough#CDK for IaaC - quick walkthrough":"The CDK code is pretty succinct but it gets the job done! Let’s go through it quickly.\nFirst, we define the function and it’s packaging format (as a Docker container):\n// environment variable for Lambda function lambdaEnvVars := \u0026map[string]*string{slackSecretEnvVar: jsii.String(slackSecret), giphyAPIKeyEnvVar: jsii.String(giphyAPIKey)} function := awslambda.NewDockerImageFunction(stack, jsii.String(\"awsome-func-docker\"), \u0026awslambda.DockerImageFunctionProps{FunctionName: jsii.String(functionName), Environment: lambdaEnvVars, Code: awslambda.DockerImageCode_FromImageAsset(jsii.String(\"../function\"), nil)}) Notice how NewDockerImageFunction has been used (traditionally, one would use NewFunction and refer to a zip file for deployment). In this case, we point to the folder where our function code resides (using DockerImageCode_FromImageAsset).\nFor the function to be packaged as a Docker image, I used the Go:1.x base image (see Dockerfile). But, you can explore other options as well. During deployment, the Docker image is built locally, pushed to a private ECR registry and finally the Lambda function is created - all this, with a few lines of code!\nThe Function URL bit is straightforward using NewFunctionUrl:\nfuncURL := awslambda.NewFunctionUrl(stack, jsii.String(\"awsome-func-url\"), \u0026awslambda.FunctionUrlProps{AuthType: awslambda.FunctionUrlAuthType_NONE, Function: function}) For the purposes of this sample app, we’re using NONE as the authentication type. This means that the Lambda function URL will be publicly accessible","cleanup#Cleanup":"Once you’re done, you can delete the function and related resources:\ncdk destroy In this blog post, we covered how to ease the deployment process for our Serverless backend using AWS CDK!","createconfigure-the-command-in-slack#Create/Configure the command in Slack":"Start by signing into your Slack Workspace and creating a new Slack App.\nOnce that’s done, create a Slash Command - head to your app’s settings page, and then click the Slash Commands feature in the navigation menu. You’ll be presented with a button marked Create New Command, and when you click on it, you’ll see a screen where you’ll be asked to define your new Slash Command with the required information.\nEnter the required information:\n/awsome for the Command. In Request URL section, enter a dummy URL for now e.g. https://comingsoon.com This is temporary and will be replaced by the Lambda Function URL after deployment\nFinally, install the app to your workspace - click the Basic Information feature in the navigation menu, choose Install your app to your workspace and click Install App to Workspace. This will install the app to your Slack workspace to test your app and generate the tokens you need to interact with the Slack API.\nAs soon as you finish installing the app, the App Credentials will show up on the same page. You need to grab your Slack Signing Secret from there\nMake a note of your app Signing Secret as you’ll be using it later","deploy-the-function-using-cdk-and-update-slack-config#Deploy the function using CDK and update Slack config":"Clone the Github repo and move into the right directory:\ngit clone https://github.com/abhirockzz/awsome-slack-backend cd awsome-slack-backend/function Build the Go function:\nGOOS=linux go build -o awsome Deploy the function:\nexport SLACK_SIGNING_SECRET=\u003center the slack signing secret\u003e export GIPHY_API_KEY=\u003center the giphy API key\u003e cd ../cdk \u0026\u0026 cdk deploy Make a note of the Lambda Function URL that you receive as an output\nGo back to the Slack and update the configuration to reflect the Lambda Function URL.\nEverything has been setup and configured. Now head over to your Slack workspace and invoke the Slack command you just configured! Try this:\n/awsome serverless ","pre-requisites#Pre-requisites":" Create an AWS account (if you do not already have one) and log in. The IAM user that you use must have sufficient permissions to make necessary AWS service calls and manage AWS resources. Install AWS CDK Setup Docker Install Go Install Git Create a Slack workspace if you don’t have one. Create a GIHPY account (it’s free!) and create an app. Each application you create will have its own API Key. "},"title":"Package and deploy a Lambda function as a Docker container with AWS CDK"},"/blog/manage-eventhubs-kubernetes/":{"data":{"":"","-install-azure-service-operator#\u0026hellip; Install Azure Service Operator":"","but-what-just-happened-#But, what just happened \u0026hellip;?":"Azure Service Operator is an open source project to help you provision and manage Azure services using Kubernetes. Developers can use it to provision Azure services from any environment, be it Azure, any other cloud provider or on-premises - Kubernetes is the only common denominator!\nIt can also be included as a part of CI/CD pipelines to create, use and tear down Azure resources on-demand. Behind the scenes, all the heavy lifting is taken care of by a combination of Custom Resource Definitions which define Azure resources and the corresponding Kubernetes Operator(s) which ensure that the state defined by the Custom Resource Definition is reflected in Azure as well.\nRead more in the recent announcement here - https://cloudblogs.microsoft.com/opensource/2020/06/25/announcing-azure-service-operator-kubernetes/\nIn this blog post:\nYou will get a high level overview of Azure Service Operator (sometimes referred to as ASO in this blog) How to set it up and use it to provision Azure Event Hubs Deploy apps to Kubernetes which use the Azure Event Hubs cluster All the artefacts are available on this GitHub repo https://github.com/abhirockzz/eventhubs-using-aso-on-k8s\nGetting started…. Azure Service Operator supports many Azure services including databases (Azure Cosmos DB, PostgreSQL, MySQL, Azure SQL etc.), core infrastructure components (Virtual Machines, VM Scale sets, Virtual Networks etc.) and others as well.\nIt also supports Azure Event Hubs which is a fully managed data streaming platform and event ingestion service with support for Apache Kafka and other tools in the Kafka ecosystem. With Azure Service Operator you can provision and manage Azure Event Hubs namespaces, Event Hub and Consumer Groups.\nSo, let’s dive in without further ado! Before we do that, please note that you will need the following in order to try out this tutorial:\nPre-requisites Start by getting an Azure account if you don’t have one already - you can get for FREE! Please make sure you’ve kubectl and Helm 3 installed as well.\nAlthough the steps outlined in this blog should work with any Kubernetes cluster (including minikube etc.), I used Azure Kubernetes Service (AKS). You can setup a cluster using Azure CLI, Azure portal or even an ARM template. Once that’s done, simply configure kubectl to point to it\naz aks get-credentials --resource-group \u003cCLUSTER_RESOURCE_GROUP\u003e --name \u003cCLUSTER_NAME\u003e Ok, you’re now ready to…\n… Install Azure Service Operator Nothing too fancy about it… just following the steps to install it using Helm\nStart by installing cert-manager\nSetup cert-manager kubectl create namespace cert-manager kubectl label namespace cert-manager cert-manager.io/disable-validation=true kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.12.0/cert-manager.yaml //make sure cert manager is up and running kubectl rollout status -n cert-manager deploy/cert-manager-webhook Authentication… Since the operator will create resource on Azure, we need to authorize it to do so by providing the appropriate credentials. Currently, you can use Managed Identity or Service Principal\nI will be using a Service Principal, so let’s start by creating one (with Azure CLI) using the az ad sp create-for-rbac command\naz ad sp create-for-rbac -n \"aso-rbac-sp\" //JSON output { \"appId\": \"eb4280db-4242-4ed0-a7d2-42424242f0d0\", \"displayName\": \"aso-rbac-sp\", \"name\": \"http://aso-rbac-sp\", \"password\": \"7d69a422-428d-42d4-a242-cd1d425424b2\", \"tenant\": \"42f988bf-42f1-42af-42ab-2d7cd421db42\" } Install Setup required environment variables:\nexport AZURE_SUBSCRIPTION_ID=\u003center Azure subscription ID\u003e export AZURE_TENANT_ID=\u003center value from the \"tenant\" attribute in the JSON payload above\u003e export AZURE_CLIENT_ID=\u003center value from the \"appId\" attribute in the JSON payload above\u003e export AZURE_CLIENT_SECRET=\u003center value from the \"password\" attribute in the JSON payload above\u003e export AZURE_SERVICE_OPERATOR_NAMESPACE=\u003cname of the namespace into which ASO will be installed\u003e Add the repo, create namespace\nhelm repo add azureserviceoperator https://raw.githubusercontent.com/Azure/azure-service-operator/master/charts kubectl create namespace $AZURE_SERVICE_OPERATOR_NAMESPACE Use helm upgrade to initiate setup:\nhelm upgrade --install aso azureserviceoperator/azure-service-operator \\ -n $AZURE_SERVICE_OPERATOR_NAMESPACE \\ --set azureSubscriptionID=$AZURE_SUBSCRIPTION_ID \\ --set azureTenantID=$AZURE_TENANT_ID \\ --set azureClientID=$AZURE_CLIENT_ID \\ --set azureClientSecret=$AZURE_CLIENT_SECRET Before you proceed, wait for the Azure Service Operator Pod to startup\nkubectl get pods -n $AZURE_SERVICE_OPERATOR_NAMESPACE NAME READY STATUS RESTARTS AGE azureoperator-controller-manager-68f44fd4-cm6wl 2/2 Running 0 6m Setup Azure Event Hubs components… Start by cloning the repo:\ngit clone https://github.com/abhirockzz/eventhubs-using-aso-on-k8s cd eventhubs-using-aso-on-k8s Create an Azure Resource Group\nI have used the southeastasia location. Please update eh-resource-group.yaml if you need to use a different one\nkubectl apply -f deploy/eh-resource-group.yaml //confirm that its created kubectl get resourcegroups/eh-aso-rg Create Event Hubs namespace\nI have used the southeastasia location. Please update eh-namespace.yaml if you need to use a different one\nkubectl apply -f deploy/eh-namespace.yaml //wait for creation kubectl get eventhubnamespaces -w Once done, you should see this:\nNAME PROVISIONED MESSAGE eh-aso-ns true successfully provisioned You can get details with kubectl describe eventhubnamespaces and also double-check using az eventhubs namespace show\nThe namespace is ready, we can now create an Event Hub\nkubectl apply -f deploy/eh-hub.yaml kubectl get eventhubs/eh-aso-hub //once done... NAME PROVISIONED MESSAGE eh-aso-hub true successfully provisioned You can get details with kubectl describe eventhub and also double-check using az eventhubs eventhub show\nAs a final step, create the consumer group\nThis is addition to the default consumer group (appropriately named $Default)\nkubectl apply -f deploy/eh-consumer-group.yaml kubectl get consumergroups/eh-aso-cg NAME PROVISIONED MESSAGE eh-aso-cg true successfully provisioned You can get details with kubectl describe consumergroup and also double-check using eazventhubs eventhub consumer-group show\nWhat’s next? Let’s make use of what we just setup! We’ll deploy a pair of producer and consumer apps to Kubernetes that will send and receive messages from Event Hubs respectively. Both these client apps are written in Go and use the Sarama library for Kafka. I am not going to dive into the details since they are relatively straightforward\nDeploy the consumer app:\nkubectl apply -f deploy/consumer.yaml //wait for it to start kubectl get pods -l=app=eh-consumer -w Keep a track of the logs for the consumer app:\nkubectl logs -f $(kubectl get pods -l=app=eh-consumer --output=jsonpath={.items..metadata.name}) You should see something similar to:\nEvent Hubs broker [eh-aso-ns.servicebus.windows.net:9093] Sarama client consumer group ID eh-aso-cg new consumer group created Event Hubs topic eh-aso-hub Waiting for program to exit Partition allocation - map[eh-aso-hub:[0 1 2]] Using another terminal, deploy the producer app:\nkubectl apply -f deploy/producer.yaml Once the producer app is up and running, the consumer should kick in, start consumer the messages and print them to the console. So you’ll see logs similar to this:\n... Message topic:\"eh-aso-hub\" partition:0 offset:6 Message content value-2020-07-06 15:37:06.116674866 +0000 UTC m=+67.450171692 Message topic:\"eh-aso-hub\" partition:0 offset:7 Message content value-2020-07-06 15:37:09.133115988 +0000 UTC m=+70.466612714 Message topic:\"eh-aso-hub\" partition:0 offset:8 Message content value-2020-07-06 15:37:12.149068005 +0000 UTC m=+73.482564831 ... In case you want to check producer logs as well: kubectl logs -f $(kubectl get pods -l=app=eh-producer --output=jsonpath={.items..metadata.name})\nAlright, it worked!\nWe created an Event Hubs namespace, Event Hub along and a consumer group.. all using kubectl (and YAMLs of course) Deployed a simple producer and consumer for testing But, what just happened …? … how did the consumer and producer apps connect to Event Hubs without connection info, credentials etc.?\nNotice this part of Event Hub manifest (eh-hub.yaml file):\n... spec: secretName: eh-secret location: southeastasia resourceGroup: eh-aso-rg ... secretName: eh-secret ensured that a Kubernetes Secret was created with the required connectivity details including connection strings (primary, secondary), keys (primary, secondary), along with the basic info such as Event Hubs namespace and hub name.\nThe producer and consumer Deployments were simply able to refer to this. Take a look at this snippet from the consumer app Deployment\n... containers: - name: eh-consumer image: abhirockzz/eh-kafka-consumer env: - name: EVENTHUBS_CONNECTION_STRING valueFrom: secretKeyRef: name: eh-secret key: primaryConnectionString - name: EVENTHUBS_NAMESPACE valueFrom: secretKeyRef: name: eh-secret key: eventhubNamespace - name: EVENTHUBS_BROKER value: $(EVENTHUBS_NAMESPACE).servicebus.windows.net:9093 - name: EVENTHUBS_TOPIC valueFrom: secretKeyRef: name: eh-secret key: eventhubName - name: EVENTHUBS_CONSUMER_GROUPID value: eh-aso-cg ... The app uses env vars EVENTHUBS_CONNECTION_STRING, EVENTHUBS_NAMESPACE and EVENTHUBS_TOPIC whose values were sourced from the Secret (eh-secret). The value for EVENTHUBS_CONSUMER_GROUPID is hardcoded to eh-aso-cg which was the name of the consumer group specified in eh-consumer-group.yaml.","clean-up#Clean up":"To remove all the resources including Event Hubs and the client apps, simply use kubectl delete -f deploy","conclusion#Conclusion":"Azure Service Operator provides a layer of abstraction on top Azure specific primitives. It allows you to manage Azure resources and also provide ways to connect to them using other applications deployed in the same Kubernetes cluster.\nI covered Azure Event Hubs as an example, but as I mentioned earlier, Azure Service Operator also supports other services too. Head over to the GitHub repo and give them a try!","getting-started#Getting started\u0026hellip;.":"","setup-azure-event-hubs-components#Setup Azure Event Hubs components\u0026hellip;":"","whats-next#What\u0026rsquo;s next?":""},"title":"Orchestrate Azure Event Hubs via Kubernetes"},"/blog/manage-redis-on-aws-kubernetes/":{"data":{"":"","cdk8s-in-action#\u003ccode\u003ecdk8s\u003c/code\u003e in action!":"Using AWS Controller for Kubernetes and CDK for Kubernetes\nIn this blog post, you will learn how to use ACK with Amazon EKS for creating a Redis cluster on AWS (with Amazon MemoryDB).\nAWS Controllers for Kubernetes (also known as ACK) leverage Kubernetes Custom Resource and Custom Resource Definitions and give you the ability to manage and use AWS services directly from Kubernetes without needing to define resources outside of the cluster. It supports many AWS services including S3, DynamoDB, MemoryDB etc.\nNormally you would define custom resources in ACK using YAML. But, in this case we will leverage cdk8s (Cloud Development Kit for Kubernetes), an open-source framework (part of CNCF) that allows you to define your Kubernetes applications using regular programming languages (instead of yaml). Thanks to cdk8s support for Kubernetes Custom Resource definitions, we will import MemoryDB ACK CRDs as APIs and then define a cluster using code (I will be using Go for this).\nThat’s not all! In addition to the infrastructure, we will take care of the application that will represent the application which will connect with the MemoryDB cluster. To do this, we will use the cdk8s-plus library to define a Kubernetes Deployment (and Service to expose it), thereby building an end to end solution. In the process, you will learn about some of other nuances of ACK such as FieldExport etc.\nPre-requisites To follow along step-by-step, in addition to an AWS account, you will need to have AWS CLI, cdk8s CLI, kubectl, helm and the Go programming language installed.\nThere are a variety of ways in which you can create an Amazon EKS cluster. I prefer using eksctl CLI because of the convenience it offers!\nFirst, set up the MemoryDB controller Most of the below steps are adapted from the ACK documentation - Install an ACK Controller\nInstall it using Helm:\nexport SERVICE=memorydb export RELEASE_VERSION=`curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep '\"tag_name\":' | cut -d'\"' -f4` export ACK_SYSTEM_NAMESPACE=ack-system # you can change the region as required export AWS_REGION=us-east-1 aws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws helm install --create-namespace -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller \\ oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION --set=aws.region=$AWS_REGION To confirm, run:\nkubectl get crd # output (multiple CRDs) NAME CREATED AT acls.memorydb.services.k8s.aws 2022-08-13T19:15:46Z adoptedresources.services.k8s.aws 2022-08-13T19:15:53Z clusters.memorydb.services.k8s.aws 2022-08-13T19:15:47Z eniconfigs.crd.k8s.amazonaws.com 2022-08-13T19:02:10Z fieldexports.services.k8s.aws 2022-08-13T19:15:56Z parametergroups.memorydb.services.k8s.aws 2022-08-13T19:15:48Z securitygrouppolicies.vpcresources.k8s.aws 2022-08-13T19:02:12Z snapshots.memorydb.services.k8s.aws 2022-08-13T19:15:51Z subnetgroups.memorydb.services.k8s.aws 2022-08-13T19:15:52Z users.memorydb.services.k8s.aws 2022-08-13T19:15:53Z Since the controller has to interact with AWS Services (make API calls), we need to configure IAM Roles for Service Accounts (also known as IRSA).\nRefer to Configure IAM Permissions for details\nIRSA configuration\nFirst, create an OIDC identity provider for your cluster.\nexport EKS_CLUSTER_NAME=\u003cname of your EKS cluster\u003e export AWS_REGION=\u003ccluster region\u003e eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --region $AWS_REGION --approve The goal is to create an IAM role and attach appropriate permissions via policies. We can then create a Kubernetes Service Account and attach the IAM role to it. Thus, the controller Pod will be able to make AWS API calls. Note that we are using providing all MemoryDB permissions to our control via the arn:aws:iam::aws:policy/AmazonMemoryDBFullAccess policy.\nThanks to eksctl, this can be done with a single line!\nexport SERVICE=memorydb export ACK_K8S_SERVICE_ACCOUNT_NAME=ack-$SERVICE-controller # recommend using the same name export ACK_SYSTEM_NAMESPACE=ack-system export EKS_CLUSTER_NAME=\u003center EKS cluster name\u003e export POLICY_ARN=arn:aws:iam::aws:policy/AmazonMemoryDBFullAccess # IAM role has a format - do not change it. you can't use any arbitrary name export IAM_ROLE_NAME=ack-$SERVICE-controller-role eksctl create iamserviceaccount \\ --name $ACK_K8S_SERVICE_ACCOUNT_NAME \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --cluster $EKS_CLUSTER_NAME \\ --role-name $IAM_ROLE_NAME \\ --attach-policy-arn $POLICY_ARN \\ --approve \\ --override-existing-serviceaccounts The policy (AmazonMemoryDBFullAccess) is chosen as per https://github.com/aws-controllers-k8s/memorydb-controller/blob/main/config/iam/recommended-policy-arn\nTo confirm, you can check whether the IAM role was created and also introspect the Kubernetes service account\naws iam get-role --role-name=$IAM_ROLE_NAME --query Role.Arn --output text kubectl describe serviceaccount/$ACK_K8S_SERVICE_ACCOUNT_NAME -n $ACK_SYSTEM_NAMESPACE # you will see similar output Name: ack-memorydb-controller Namespace: ack-system Labels: app.kubernetes.io/instance=ack-memorydb-controller app.kubernetes.io/managed-by=eksctl app.kubernetes.io/name=memorydb-chart app.kubernetes.io/version=v0.0.2 helm.sh/chart=memorydb-chart-v0.0.2 k8s-app=memorydb-chart Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::568863012249:role/ack-memorydb-controller-role meta.helm.sh/release-name: ack-memorydb-controller meta.helm.sh/release-namespace: ack-system Image pull secrets: \u003cnone\u003e Mountable secrets: ack-memorydb-controller-token-2cmmx Tokens: ack-memorydb-controller-token-2cmmx Events: \u003cnone\u003e For IRSA to take effect, you need to restart the ACK Deployment:\n# Note the deployment name for ACK service controller from following command kubectl get deployments -n $ACK_SYSTEM_NAMESPACE kubectl -n $ACK_SYSTEM_NAMESPACE rollout restart deployment ack-memorydb-controller-memorydb-chart Confirm that the Deployment has restarted (currently Running) and the IRSA is properly configured:\nkubectl get pods -n $ACK_SYSTEM_NAMESPACE kubectl describe pod -n $ACK_SYSTEM_NAMESPACE ack-memorydb-controller-memorydb-chart-5975b8d757-k6x9k | grep \"^\\s*AWS_\" # The output should contain following two lines: AWS_ROLE_ARN=arn:aws:iam::\u003cAWS_ACCOUNT_ID\u003e:role/\u003cIAM_ROLE_NAME\u003e AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token Now that we’re done with the configuration, its time for…\ncdk8s in action! We will go step by step:\nBuild and push the application Docker images to private registry in Amazon ECR Deploy MemoryDB along with the application and required configuration Test the application Build Docker image and push to ECR Create ECR private repository\nLogin to ECR:\naws ecr get-login-password --region \u003center region\u003e | docker login --username AWS --password-stdin \u003center aws_account_id\u003e.dkr.ecr.\u003center region\u003e.amazonaws.com Create private repository:\naws ecr create-repository \\ --repository-name memorydb-app \\ --region \u003center region\u003e Build image and push to ECR\n# if you're on Mac M1 #export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker build -t memorydb-app . docker tag memorydb-app:latest \u003center aws_account_id\u003e.dkr.ecr.\u003center region\u003e.amazonaws.com/memorydb-app:latest docker push \u003center aws_account_id\u003e.dkr.ecr.\u003center region\u003e.amazonaws.com/memorydb-app:latest Use cdk8s and kubectl to deploy MemoryDB and the application This is a ready-to-use cdk8s project that you can use. The entire logic is in main.go file - I will dive into the nitty gritty of the code in the next section.\nClone the project from Github and change to the right directory:\ngit clone https://github.com/abhirockzz/memorydb-ack-cdk8s-go.git cd memorydb-ack-cdk8s-go Generate and deploy manifests\nUse cdk8s synth to generate the manifest for MemoryDB, the application as well as required configuration. We can then apply it using kubectl.\nexport SUBNET_ID_LIST=\u003center comma-separated list of subnet IDs. should be same as your EKS cluster\u003e # for example: # export SUBNET_ID_LIST=subnet-086c4a45ec9a206e1,subnet-0d9a9c6d2ca7a24df,subnet-028ca54bb859a4994 export SECURITY_GROUP_ID=\u003center security group ID\u003e # for example # export SECURITY_GROUP_ID=sg-06b6535ee64980616 export DOCKER_IMAGE=\u003center ECR repo that you created earlier\u003e # example # export DOCKER_IMAGE=1234567891012.dkr.ecr.us-east-1.amazonaws.com/memorydb-app:latest You can also add other environment variables MEMORYDB_CLUSTER_NAME, MEMORYDB_USERNAME, MEMORYDB_PASSWORD. These are not mandatory and default to memorydb-cluster-ack-cdk8s, demouser and Password123456789 respectively\nTo generate the manifests:\ncdk8s synth # check the \"dist\" folder - you should see these files: 0000-memorydb.k8s.yaml 0001-config.k8s.yaml 0002-deployment.k8s.yaml Let’s deploy them one by one, starting with the one which creates the MemoryDB cluster. In addition to the cluster, it will also provision the supporting components including ACL, User and Subnet Groups.\nkubectl apply -f dist/0000-memorydb.k8s.yaml #output secret/memdb-secret created users.memorydb.services.k8s.aws/demouser created acls.memorydb.services.k8s.aws/demo-acl created subnetgroups.memorydb.services.k8s.aws/demo-subnet-group created clusters.memorydb.services.k8s.aws/memorydb-cluster-ack-cdk8s Kubernetes Secret is used to hold the password for MemoryDB cluster user.\nThis initiates the cluster creation. You can check the status using the AWS console. Once the creation is complete, you can test connectivity with redis-cli:\n# run this from EC2 instance in the same subnet as the cluster export REDIS=\u003center cluster endpoint\u003e # example # export REDIS=clustercfg.memorydb-cluster-ack-cdk8s.smtjf4.memorydb.us-east-1.amazonaws.com redis-cli -h $REDIS -c --user demouser --pass Password123456789 --tls --insecure Let’s apply second manifest. This will create configuration related components i.e. ConfigMap and FieldExports - these are required by our application (to be deployed after this)\nkubectl apply -f dist/0001-config.k8s.yaml #output configmap/export-memorydb-info created fieldexports.services.k8s.aws/export-memorydb-endpoint created fieldexports.services.k8s.aws/export-memorydb-username created In this case, we create two FieldExports to extract data from the cluster (from .status.clusterEndpoint.address) and user (.spec.name) resources that we created before and seed it into a ConfigMap.\nConfigMap and FieldExport:\nFieldExport is an ACK component that can “export any spec or status field from an ACK resource into a Kubernetes ConfigMap or Secret”. You can read up on the details in the ACK docs along with some examples.\nYou should be able to confirm by checking the FieldExport and ConfigMap:\nkubectl get fieldexport #output NAME AGE export-memorydb-endpoint 20s export-memorydb-username 20s kubectl get configmap/export-memorydb-info -o yaml We started out with a blank ConfigMap, but ACK magically populated it with the required attributes:\napiVersion: v1 data: default.export-memorydb-endpoint: clustercfg.memorydb-cluster-ack-cdk8s.smtjf4.memorydb.us-east-1.amazonaws.com default.export-memorydb-username: demouser immutable: false kind: ConfigMap #....omitted Let’s create the application resources - Deployment and the Service.\nkubectl apply -f dist/0003-deployment.k8s.yaml #output deployment.apps/memorydb-app created service/memorydb-app-service configured Since the Service type is LoadBalancer, an appropriate AWS Load Balancer will be provisioned to allow for external access.\nCheck Pod and Service:\nkubectl get pods kubectl get service/memorydb-app-service # to get the load balancer IP APP_URL=$(kubectl get service/memorydb-app-service -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\") echo $APP_URL # output example a0042d5b5b0ad40abba9c6c42e6342a2-879424587.us-east-1.elb.amazonaws.com You have deployed the application and know the endpoint over which it’s publicly accessible. Here is a high-level view of the current architecture:\nNow you can access the application… It’s quite simple - it exposes a couple of HTTP endpoints to write and read data from Redis (you can check it on GitHub):\n# create a couple of users - this will be added as a `HASH` in Redis curl -i -X POST -d '{\"email\":\"user1@foo.com\", \"name\":\"user1\"}' http://$APP_URL:9090/ curl -i -X POST -d '{\"email\":\"user2@foo.com\", \"name\":\"user2\"}' http://$APP_URL:9090/ HTTP/1.1 200 OK Content-Length: 0 # search for user via email curl -i http://$APP_URL:9090/user2@foo.com HTTP/1.1 200 OK Content-Length: 41 Content-Type: text/plain; charset=utf-8 {\"email\":\"user2@foo.com\",\"name\":\"user2\"} If you get a Could not resolve host error while accessing the LB URL, wait for a minute or so and re-try\nOnce you’re done…\n… don’t forget to delete resources.. # delete MemoryDB cluster, configuration and the application kubectl delete -f dist/ # to uninstall the ACK controller export SERVICE=memorydb helm uninstall -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller # delete the EKS cluster. if created via eksctl: eksctl delete cluster --name \u003center name of eks cluster\u003e So far, you deployed the ACK controller for MemoryDB, setup a cluster, an application that connects to it and tested the end to end solution. Great!\nNow let’s look at the cdk8s code that makes it all happen. The logic is divided into three Charts. I will only focus on key sections of the code and rest will be omitted for brevity.\nYou can refer to the complete code on GitHub","code-walk-through#Code walk through":"MemoryDB and related components\nWe start by defining the MemoryDB cluster along with the required components - ACL, User and Subnet Group.\nfunc NewMemoryDBChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //... secret = cdk8splus22.NewSecret(chart, jsii.String(\"password\"), \u0026cdk8splus22.SecretProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(secretName)}, StringData: \u0026map[string]*string{\"password\": jsii.String(memoryDBPassword)}, }) user = users_memorydbservicesk8saws.NewUser(chart, jsii.String(\"user\"), \u0026users_memorydbservicesk8saws.UserProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(memoryDBUsername)}, Spec: \u0026users_memorydbservicesk8saws.UserSpec{ Name: jsii.String(memoryDBUsername), AccessString: jsii.String(memoryDBUserAccessString), AuthenticationMode: \u0026users_memorydbservicesk8saws.UserSpecAuthenticationMode{ Type: jsii.String(\"Password\"), Passwords: \u0026[]*users_memorydbservicesk8saws.UserSpecAuthenticationModePasswords{ {Name: secret.Name(), Key: jsii.String(secretKeyName)}, }, }, }, }) ACL references the User defined above:\nacl := acl_memorydbservicesk8saws.NewAcl(chart, jsii.String(\"acl\"), \u0026acl_memorydbservicesk8saws.AclProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(memoryDBACLName)}, Spec: \u0026acl_memorydbservicesk8saws.AclSpec{ Name: jsii.String(memoryDBACLName), UserNames: jsii.Strings(*user.Name()), }, }) The subnet IDs (for subnet group) as well as the security group ID for the cluster are read from environment variables.\nsubnetGroup := subnetgroups_memorydbservicesk8saws.NewSubnetGroup(chart, jsii.String(\"sg\"), \u0026subnetgroups_memorydbservicesk8saws.SubnetGroupProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(memoryDBSubnetGroup)}, Spec: \u0026subnetgroups_memorydbservicesk8saws.SubnetGroupSpec{ Name: jsii.String(memoryDBSubnetGroup), SubnetIDs: jsii.Strings(strings.Split(subnetIDs, \",\")...), //same as EKS cluster }, }) Finally, the MemoryDB cluster is defined - it references all the resources created above (it has been omitted on purpose):\nmemoryDBCluster = memorydbservicesk8saws.NewCluster(chart, jsii.String(\"memorydb-ack-cdk8s\"), \u0026memorydbservicesk8saws.ClusterProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(memoryDBClusterName)}, Spec: \u0026memorydbservicesk8saws.ClusterSpec{ Name: jsii.String(memoryDBClusterName), //omitted }, }) return chart } Configuration\nThen we move on to the next chart that handles the configuration related aspects. It defines a ConfigMap (which is empty) and FieldExports - one each for the MemoryDB cluster endpoint and username (the password is read from the Secret)\nAs soon as these are created, the ConfigMap is populated with the required data as per from and to configuration in the FieldExport.\nfunc NewConfigChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //... cfgMap = cdk8splus22.NewConfigMap(chart, jsii.String(\"config-map\"), \u0026cdk8splus22.ConfigMapProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{ Name: jsii.String(configMapName)}}) fieldExportForClusterEndpoint = servicesk8saws.NewFieldExport(chart, jsii.String(\"fexp-cluster\"), \u0026servicesk8saws.FieldExportProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(fieldExportNameForClusterEndpoint)}, Spec: \u0026servicesk8saws.FieldExportSpec{ From: \u0026servicesk8saws.FieldExportSpecFrom{Path: jsii.String(\".status.clusterEndpoint.address\"), Resource: \u0026servicesk8saws.FieldExportSpecFromResource{ Group: jsii.String(\"memorydb.services.k8s.aws\"), Kind: jsii.String(\"Cluster\"), Name: memoryDBCluster.Name()}}, To: \u0026servicesk8saws.FieldExportSpecTo{ Name: cfgMap.Name(), Kind: servicesk8saws.FieldExportSpecToKind_CONFIGMAP}}}) fieldExportForUsername = servicesk8saws.NewFieldExport(chart, jsii.String(\"fexp-username\"), \u0026servicesk8saws.FieldExportProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{Name: jsii.String(fieldExportNameForUsername)}, Spec: \u0026servicesk8saws.FieldExportSpec{ From: \u0026servicesk8saws.FieldExportSpecFrom{Path: jsii.String(\".spec.name\"), Resource: \u0026servicesk8saws.FieldExportSpecFromResource{ Group: jsii.String(\"memorydb.services.k8s.aws\"), Kind: jsii.String(\"User\"), Name: user.Name()}}, To: \u0026servicesk8saws.FieldExportSpecTo{ Name: cfgMap.Name(), Kind: servicesk8saws.FieldExportSpecToKind_CONFIGMAP}}}) return chart } The application chart\nFinally, we deal with the Deployment (in its dedicated Chart) - it makes use of the configuration objects we defined in the earlier chart:\nfunc NewDeploymentChart(scope constructs.Construct, id string, props *MyChartProps) cdk8s.Chart { //.... dep := cdk8splus22.NewDeployment(chart, jsii.String(\"memorydb-app-deployment\"), \u0026cdk8splus22.DeploymentProps{ Metadata: \u0026cdk8s.ApiObjectMetadata{ Name: jsii.String(\"memorydb-app\")}}) The next important part is the container and it’s configuration. We specify the ECR image repository along with the environment variables - they reference the ConfigMap we defined in the previous chart (everything is connected!):\n//... container := dep.AddContainer( \u0026cdk8splus22.ContainerProps{ Name: jsii.String(\"memorydb-app-container\"), Image: jsii.String(appDockerImage), Port: jsii.Number(appPort)}) container.Env().AddVariable(jsii.String(\"MEMORYDB_CLUSTER_ENDPOINT\"), cdk8splus22.EnvValue_FromConfigMap( cfgMap, jsii.String(\"default.\"+*fieldExportForClusterEndpoint.Name()), \u0026cdk8splus22.EnvValueFromConfigMapOptions{Optional: jsii.Bool(false)})) container.Env().AddVariable(jsii.String(\"MEMORYDB_USERNAME\"), cdk8splus22.EnvValue_FromConfigMap( cfgMap, jsii.String(\"default.\"+*fieldExportForUsername.Name()), \u0026cdk8splus22.EnvValueFromConfigMapOptions{Optional: jsii.Bool(false)})) container.Env().AddVariable(jsii.String(\"MEMORYDB_PASSWORD\"), cdk8splus22.EnvValue_FromSecretValue( \u0026cdk8splus22.SecretValue{ Secret: secret, Key: jsii.String(\"password\")}, \u0026cdk8splus22.EnvValueFromSecretOptions{})) Finally, we define the Service (type LoadBalancer) which enables external application access and tie it all together in the main function:\n//... dep.ExposeViaService( \u0026cdk8splus22.DeploymentExposeViaServiceOptions{ Name: jsii.String(\"memorydb-app-service\"), ServiceType: cdk8splus22.ServiceType_LOAD_BALANCER, Ports: \u0026[]*cdk8splus22.ServicePort{ {Protocol: cdk8splus22.Protocol_TCP, Port: jsii.Number(lbPort), TargetPort: jsii.Number(appPort)}}}) //... func main() { app := cdk8s.NewApp(nil) memorydb := NewMemoryDBChart(app, \"memorydb\", nil) config := NewConfigChart(app, \"config\", nil) config.AddDependency(memorydb) deployment := NewDeploymentChart(app, \"deployment\", nil) deployment.AddDependency(memorydb, config) app.Synth() } That’s all for now!","first-set-up-the-memorydb-controller#First, set up the MemoryDB controller":"","pre-requisites#Pre-requisites":"","wrap-up#Wrap up..":"Combining AWS Controllers for Kubernetes and cdk8s can prove useful if you want to manage AWS services as well as the Kubernetes applications - using code (not yaml). In this blog you saw how to do this in the context of MemoryDB and an application that was composed of a Deployment and Service. I encourage you to try out other AWS services as well - here is a complete list.\nUntil then, Happy Building!"},"title":"Manage Redis on AWS from Kubernetes"},"/blog/memorydb-apprunner-cdk/":{"data":{"":"Run your Go app on AWS App Runner service, integrate it with MemoryDB for Redis and use AWS CDK to package and deploy the app along with infrastructure\nAWS App Runner allows you to deploy and run cloud-native applications in a fast, simple, and cost-effective manner. You can choose the programming language of your choice since App Runner can deploy directly from source code (in GitHub for example) or a Docker container image (from private or public repo in ECR) - all this without worrying about provisioning and managing the underlying infrastructure.\nThis blog post showcases how to run a Go application on AWS App Runner which will further integrate with Amazon MemoryDB for Redis (a Redis compatible, durable, in-memory database service). You will deploy the application and its infrastructure using AWS CDK. This includes App Runner VPC Connector config to connect with MemoryDB as well as using CDK to package your Go application as a Docker image, uploading to ECR and seamlessly deployment to App Runner (no manual steps needed). I will close the blog with a brief walk through of the CDK code which is written in Go, thanks to the CDK Go support (which is in Developer Preview at the time of writing).\nThe code is available on GitHub\nBefore you proceed, make sure you have the following ready:","clean-up#Clean up":"Once you’ve completed this tutorial, delete the stack(s):\ncdk destroy --all ","pre-requisites#Pre-requisites":" Create an AWS account (if you do not already have one) and log in. The IAM user that you use must have sufficient permissions to make necessary AWS service calls and manage AWS resources. Install and configure AWS CLI Install and bootstrap AWS CDK Setup Docker Install Go ","quick-walk-through-of-the-cdk-code#Quick walk through of the CDK code":"The Infrastructure part (IaaC to be specific) is comprised of two (CDK) Stacks (in the context of a single CDK App).\nI will provide a walk through of the CDK code which is written in Go, thanks to the CDK Go support (which is Developer Preview at the time of writing).\nPlease note that some of the code has been redacted/omitted for brevity - you can always refer to complete code in the GitHub repo\nHere is the first stack:\nTo summarise:\nA single line of code to create VPC and related components! We create ACL, User, Subnet group for MemoryDB cluster and refer to them when during cluster creation with awsmemorydb.NewCfnCluster We also create required security groups for MemoryDB as well as AppRunner (VPC config) ... vpc = awsec2.NewVpc(stack, jsii.String(\"demo-vpc\"), nil) authInfo := map[string]interface{}{\"Type\": \"password\", \"Passwords\": []string{getMemoryDBPassword()}} user = awsmemorydb.NewCfnUser(stack, jsii.String(\"demo-memorydb-user\"), \u0026awsmemorydb.CfnUserProps{UserName: jsii.String(\"demo-user\"), AccessString: jsii.String(accessString), AuthenticationMode: authInfo}) acl := awsmemorydb.NewCfnACL(stack, jsii.String(\"demo-memorydb-acl\"), \u0026awsmemorydb.CfnACLProps{AclName: jsii.String(\"demo-memorydb-acl\"), UserNames: \u0026[]*string{user.UserName()}}) //snip... subnetGroup := awsmemorydb.NewCfnSubnetGroup(stack, jsii.String(\"demo-memorydb-subnetgroup\"), \u0026awsmemorydb.CfnSubnetGroupProps{SubnetGroupName: jsii.String(\"demo-memorydb-subnetgroup\"), SubnetIds: \u0026subnetIDsForSubnetGroup}) memorydbSecurityGroup := awsec2.NewSecurityGroup(stack, jsii.String(\"memorydb-demo-sg\"), \u0026awsec2.SecurityGroupProps{Vpc: vpc, SecurityGroupName: jsii.String(\"memorydb-demo-sg\"), AllowAllOutbound: jsii.Bool(true)}) memorydbCluster = awsmemorydb.NewCfnCluster(stack, jsii.String(\"demo-memorydb-cluster\"), \u0026awsmemorydb.CfnClusterProps{ClusterName: jsii.String(\"demo-memorydb-cluster\"), NodeType: jsii.String(memoryDBNodeType), AclName: acl.AclName(), NumShards: jsii.Number(numMemoryDBShards), EngineVersion: jsii.String(memoryDBRedisEngineVersion), Port: jsii.Number(memoryDBRedisPort), SubnetGroupName: subnetGroup.SubnetGroupName(), NumReplicasPerShard: jsii.Number(numMemoryDBReplicaPerShard), TlsEnabled: jsii.Bool(true), SecurityGroupIds: \u0026[]*string{memorydbSecurityGroup.SecurityGroupId()}, ParameterGroupName: jsii.String(memoryDBDefaultParameterGroupName)}) //snip... appRunnerVPCConnSecurityGroup = awsec2.NewSecurityGroup(stack, jsii.String(\"apprunner-demo-sg\"), \u0026awsec2.SecurityGroupProps{Vpc: vpc, SecurityGroupName: jsii.String(\"apprunner-demo-sg\"), AllowAllOutbound: jsii.Bool(true)}) memorydbSecurityGroup.AddIngressRule(awsec2.Peer_SecurityGroupId(appRunnerVPCConnSecurityGroup.SecurityGroupId(), nil), awsec2.Port_Tcp(jsii.Number(memoryDBRedisPort)), jsii.String(\"for apprunner to access memorydb\"), jsii.Bool(false)) ... For the App Runner service:\nDocker image build and upload to private ECR repo process is done by CDK We specify the environment variables that the service will need App Runner source config refers to the IAM role, ECR image and environment variables Then there is the networking config that encapsulates the VPC connector config, and finally, the App Runner service is created ... ecrAccessPolicy := awsiam.ManagedPolicy_FromManagedPolicyArn(stack, jsii.String(\"ecr-access-policy\"), jsii.String(appRunnerServicePolicyForECRAccessARN)) apprunnerECRIAMrole := awsiam.NewRole(stack, jsii.String(\"role-apprunner-ecr\"), \u0026awsiam.RoleProps{AssumedBy: awsiam.NewServicePrincipal(jsii.String(appRunnerServicePrincipal), nil), RoleName: jsii.String(\"role-apprunner-ecr\"), ManagedPolicies: \u0026[]awsiam.IManagedPolicy{ecrAccessPolicy}}) ecrAccessRoleConfig := awsapprunner.CfnService_AuthenticationConfigurationProperty{AccessRoleArn: apprunnerECRIAMrole.RoleArn()} memoryDBEndpointURL := fmt.Sprintf(\"%s:%s\", *memorydbCluster.AttrClusterEndpointAddress(), strconv.Itoa(int(*memorydbCluster.Port()))) appRunnerServiceEnvVarConfig := []awsapprunner.CfnService_KeyValuePairProperty{{Name: jsii.String(\"MEMORYDB_CLUSTER_ENDPOINT\"), Value: jsii.String(memoryDBEndpointURL)}, {Name: jsii.String(\"MEMORYDB_USERNAME\"), Value: user.UserName()}, {Name: jsii.String(\"MEMORYDB_PASSWORD\"), Value: jsii.String(getMemoryDBPassword())}} imageConfig := awsapprunner.CfnService_ImageConfigurationProperty{RuntimeEnvironmentVariables: appRunnerServiceEnvVarConfig, Port: jsii.String(getAppRunnerServicePort())} appDockerImage := awsecrassets.NewDockerImageAsset(stack, jsii.String(\"app-image\"), \u0026awsecrassets.DockerImageAssetProps{Directory: jsii.String(\"../app/\")}) sourceConfig := awsapprunner.CfnService_SourceConfigurationProperty{AuthenticationConfiguration: ecrAccessRoleConfig, ImageRepository: awsapprunner.CfnService_ImageRepositoryProperty{ImageIdentifier: jsii.String(*appDockerImage.ImageUri()), ImageRepositoryType: jsii.String(ecrImageRepositoryType), ImageConfiguration: imageConfig}} //snip... vpcConnector := awsapprunner.NewCfnVpcConnector(stack, jsii.String(\"apprunner-vpc-connector\"), \u0026awsapprunner.CfnVpcConnectorProps{Subnets: \u0026subnetIDsForSubnetGroup, SecurityGroups: \u0026[]*string{appRunnerVPCConnSecurityGroup.SecurityGroupId()}, VpcConnectorName: jsii.String(\"demo-apprunner-vpc-connector\")}) networkConfig := awsapprunner.CfnService_NetworkConfigurationProperty{EgressConfiguration: awsapprunner.CfnService_EgressConfigurationProperty{EgressType: jsii.String(appRunnerEgressType), VpcConnectorArn: vpcConnector.AttrVpcConnectorArn()}} app := awsapprunner.NewCfnService(stack, jsii.String(\"apprunner-app\"), \u0026awsapprunner.CfnServiceProps{SourceConfiguration: sourceConfig, ServiceName: jsii.String(getAppRunnerServiceName()), NetworkConfiguration: networkConfig}) ... ","test-the-application#Test the application":"The application itself is fairly simple and exposes a couple of HTTP endpoints to create sample data. Locate the App Runner service URL from the details page.\nYou can test the application using any HTTP client (I have used curl in this example):\n# create a couple of user entries curl -i -X POST -d '{\"email\":\"user1@foo.com\", \"name\":\"user1\"}' \u003center APPRUNNER_APP_URL\u003e curl -i -X POST -d '{\"email\":\"user2@foo.com\", \"name\":\"user2\"}' \u003center APPRUNNER_APP_URL\u003e HTTP/1.1 200 OK Date: Fri, 20 May 2022 08:05:06 GMT Content-Length: 0 # search for user via email curl -i \u003center APPRUNNER_APP_URL\u003e/user2@foo.com HTTP/1.1 200 OK Date: Fri, 20 May 2022 08:05:11 GMT Content-Length: 41 Content-Type: text/plain; charset=utf-8 {\"email\":\"user2@foo.com\",\"name\":\"user2\"} # is a user does not exist curl -i \u003center APPRUNNER_APP_URL\u003e/not_there@foo.com HTTP/1.1 404 Not Found Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Fri, 20 May 2022 08:05:36 GMT Content-Length: 38 user does not exist not_there@foo.co ","time-to-wrap-up#Time to wrap up!":"You deployed a Go application to App Runner using AWS CDK (along with the infra!). In the process, you also learnt how to configure App Runner to integrate with MemoryDB for Redis using the VPC connector as well as a high-level overview of the CDK code for the entire solution.\nThat’s all for this blog. Stay tuned for more and Happy coding!","use-a-single-command-to-deploy-infra-and-app#Use a single command to deploy infra and app":"Clone the GitHub repo and change to the right directory:\ngit clone https://github.com/abhirockzz/go-redis-apprunner cd cdk Set environment variables:\nApp Runner service name and port A password of your choice for MemoryDB (this is just for demonstration purposes - for production, you will have specific processes in place to handle sensitive info) Be mindful of the password requirements. From the documentation:\n“In particular, be aware of these user password constraints when using ACLs for MemoryDB:\nPasswords must be 16–128 printable characters. The following non-alphanumeric characters are not allowed: , \"” / @.\" export APPRUNNER_SERVICE_NAME=apprunner-memorydb-go-app export APPRUNNER_SERVICE_PORT=8080 export MEMORYDB_PASSWORD=\u003cpassword of your choice e.g. P@ssw0rd12345678\u003e cdk deploy --all This will kick-off the stack creation. All you need to do now is …. wait.\nWhy?? Well, that’s because CDK is doing everything for us behind the scenes. Starting with VPC (and subnets, NAT gateway etc.), MemoryDB cluster, security groups, packaging and uploading our app as a Docker image (to a private ECR repo) and finally deploying it as a App Runner service - that’s quite a lot!\nFeel free to navigate to the AWS Console \u003e CloudFormation \u003e Stacks to see what’s going on…\nOnce both the Stack run to completion, you can explore all the components:\nMemoryDB Subnet Group - VPC and two subnets, each in one availability zone.\nMemoryDB cluster - CDK code was hard-coded to create two-node cluster (single shard) i.e. with one primary and one replica node. Note that the primary and replica nodes are spread across different AZs (as per Subnet Group config above)\nAlso, the ACL and user setting for MemoryDB - this will be used for authentication (username/password) and access control (authorization).\nRemember that MemoryDB runs in a different VPC and it’s not possible to connect your App Runner service to it by default. You need to associate your the service to the MemoryDB VPC - that’s where App Runner VPC connector comes in and allows it to communicate with MemoryDB.\nThe VPC and subnets are same as that of MemoryDB. Notice the security group as well - more on this in a minute\nCheck the MemoryDB security group. There is an Inbound rule that says: the the source security group (that is associated with App Runner VPC Connector config in this case) can access TCP port 6379 of instance associated with target security group (MemoryDB in this case)\nYou can also confirm the IAM access role that was created for App Runner as well as the service environment variables:\nEnvironment variables have been used for demonstration purposes. For production apps, you should use AWS Secrets Manager for storing and retrieving sensitive information such as passwords, auth tokens etc."},"title":"Build Cloud-Native apps with AWS App Runner, Redis and AWS CDK"},"/blog/migrating-data-from-dynamodb-to-azure-cosmos-db/":{"data":{"":"\nMigrating stateful systems, such as databases, is a complex process. A frequent requirement for customers is to transfer data from DynamoDB to Azure Cosmos DB for NoSQL. This process involves several stages, including exporting data from DynamoDB, performing necessary transformations, and importing the data into Azure Cosmos DB.","about-azure-cosmos-db#About Azure Cosmos DB":"Azure Cosmos DB is a fully managed and serverless NoSQL and vector database for modern app development, including AI applications. With its SLA-backed speed and availability as well as instant dynamic scalability, it is ideal for real-time NoSQL and MongoDB applications that require high performance and distributed computing over massive volumes of NoSQL and vector data.\nTry Azure Cosmos DB for free here. To stay in the loop on Azure Cosmos DB updates, follow us on X, YouTube, and LinkedIn.","common-migration-techniques-offline-and-online#Common Migration Techniques: Offline and Online":"Migration strategies include offline and online approaches, which can be used independently or combined based on your needs. Online migration is ideal for applications requiring real-time data transfer with zero downtime, while offline migration suits scenarios where applications can be paused during a maintenance window, allowing data to be exported from DynamoDB to an intermediate location before importing it into Azure Cosmos DB. You could also use a hybrid approach where bulk data migration occurs offline, followed by real-time synchronization to maintain consistency if you need to (temporarily) continue using DynamoDB in parallel with Azure Cosmos DB.","leave-a-review#Leave a Review":"Tell us about your Azure Cosmos DB experience! Leave a review on PeerSpot, and we’ll gift you $50. Get started here.","offline-data-migration-from-dynamodb-to-azure-cosmos-db-for-nosql#Offline Data Migration from DynamoDB to Azure Cosmos DB for NoSQL":"One of the approaches involves using a combination of Azure Data Factory, Azure Storage (with Azure Data Lake Storage Gen v2), and Apache Spark on Azure Databricks.\nFirst, data from DynamoDB table is exported to S3 (in DynamoDB JSON format) using native DynamoDB export capability. The DynamoDB table data in S3 is written to Azure Data Lake Storage (ADLS Gen v2) using an Azure Data Factory (ADF) pipeline. Finally, data in Azure storage is processed with Spark on Azure Databricks and written to Azure Cosmos DB using the Azure Cosmos DB Spark connector for NoSQL API.\nThis approach decouples storage and processing, which can be beneficial when dealing with large datasets. With Apache Spark you can scale data processing across multiple worker nodes, and it also provides a lot of flexibility when it comes to data transformations. However, the downside of this approach includes a multi-stage process which increases complexity, and overall latency. It also requires knowledge of Apache Spark and could introduce a learning curve depending on the skillset of your team.\nAlternative options You can also explore other approaches such as exporting from DynamoDB to S3, and directly use ADF to read from S3 and write to Azure Cosmos DB. You could also leverage Spark on Azure Databricks to read from DynamoDB and write to Azure Cosmos DB. Both these options have their pros and cons.","online-migration-approaches#Online Migration Approaches":"Online migration from DynamoDB typically employs a Change-Data-Capture (CDC) mechanism to stream data changes from DynamoDB. Although this is a near real-time process, you will need to build additional component to process the streaming data and write it to Azure Cosmos DB. This could be an AWS Lambda function that gets triggered by DynamoDB Streams or use Kinesis Data Streams and process the data with Kinesis or Flink.\nAs always, each approach has its strengths and weaknesses. For example, DynamoDB Streams provides ordering guarantees, but has a data retention for 24 hours, which may or may not be suitable if you are migrating large data volumes to Azure Cosmos DB. On the other hand, you can write Flink job(s) to consume from Kinesis Data Streams and perform complex aggregations on data before writing it to Azure Cosmos DB. However, note that Kinesis Data Streams does not provide ordering guarantee.","want-to-dive-in-deeper#Want to Dive in Deeper?":"If you are interested in exploring this further, I would encourage you to documentation page for Data migration from DynamoDB to Azure Cosmos DB for NoSQL that covers a complete walkthrough of an offline migration approach and explores the pros/cons of some of the offline/online migration options. You can also access the Spark notebook and the ADF pipeline from the migration-dynamodb-to-cosmosdb-nosql GitHub repository and tweak it based on your requirements.\nLet us know how that goes!"},"title":"Migrating data from DynamoDB to Azure Cosmos DB"},"/blog/mongodb-kafka-kubernetes/":{"data":{"":"In Kafka Connect on Kubernetes, the easy way!, I had demonstrated Kafka Connect on Kubernetes using Strimzi along with the File source and sink connector. This blog will showcase how to build a simple data pipeline with MongoDB and Kafka with the MongoDB Kafka connectors which will be deployed on Kubernetes with Strimzi.\nI will be using the following Azure services:\nPlease note that there are no hard dependencies on these components and the solution should work with alternatives as well\nAzure Event Hubs for Apache Kafka (any other Kafka cluster should work fine) Azure Kubernetes Service (feel free to use minikube, kind etc.) Azure Cosmos DB as the MongoDB database, thanks to Azure Cosmos DB’s API for MongoDB In this tutorial, Kafka Connect components are being deployed to Kubernetes, but it is also applicable to any Kafka Connect deployment\nWhat’s covered?\nMongoDB Kafka Connector and Strimzi overview Azure specific (optional) - Azure Event Hubs, Azure Cosmos DB and Azure Kubernetes Service Setup and operate Source and Sink connectors Test end to end scenario All the artefacts are available on GitHub","base-install#Base install":"To start off, we will install Strimzi and Kafka Connect, followed by the MongoDB connectors\nInstall Strimzi Installing Strimzi using Helm is pretty easy:\n//add helm chart repo for Strimzi helm repo add strimzi https://strimzi.io/charts/ //install it! (I have used strimzi-kafka as the release name) helm install strimzi-kafka strimzi/strimzi-kafka-operator This will install the Strimzi Operator (which is nothing but a Deployment), Custom Resource Definitions and other Kubernetes components such as Cluster Roles, Cluster Role Bindings and Service Accounts\nFor more details, check out this link\nTo confirm that the Strimzi Operator had been deployed, check it’s Pod (it should transition to Running status after a while)\nkubectl get pods -l=name=strimzi-cluster-operator NAME READY STATUS RESTARTS AGE strimzi-cluster-operator-5c66f679d5-69rgk 1/1 Running 0 43s Now that we have the “brain” (the Strimzi Operator) wired up, let’s use it!\nKafka Connect We will need to create some helper Kubernetes components before we deploy Kafka Connect.\nClone the GitHub repo\ngit clone https://github.com/abhirockzz/mongodb-kafkaconnect-kubernetes cd mongodb-kafkaconnect-kubernetes Kafka Connect will need to reference an existing Kafka cluster (which in this case is Azure Event Hubs). We can store the authentication info for the cluster as a Kubernetes Secret which can later be used in the Kafka Connect definition.\nUpdate the eventhubs-secret.yaml file to include the credentials for Azure Event Hubs. Enter the connection string in the eventhubspassword attribute.\ne.g.\napiVersion: v1 kind: Secret metadata: name: eventhubssecret type: Opaque stringData: eventhubsuser: $ConnectionString eventhubspassword: Endpoint=sb://\u003ceventhubs-namespace\u003e.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=\u003caccess-key\u003e Leave eventhubsuser: $ConnectionString unchanged\nTo create the Secret:\nkubectl apply -f deploy/eventhubs-secret.yaml Here is the Kafka Connect Strimzi definition:\napiVersion: kafka.strimzi.io/v1beta1 kind: KafkaConnect metadata: name: my-connect-cluster annotations: strimzi.io/use-connector-resources: \"true\" spec: image: abhirockzz/strimzi-kafkaconnect-mongodb:latest version: 2.4.0 replicas: 1 bootstrapServers: [EVENT_HUBS_NAMESPACE.servicebus.windows.net]:9093 config: group.id: strimzi-connect-cluster offset.storage.topic: mongo-connect-cluster-offsets config.storage.topic: mongo-connect-cluster-configs status.storage.topic: mongo-connect-cluster-status authentication: type: plain username: $ConnectionString passwordSecret: secretName: eventhubssecret password: eventhubspassword tls: trustedCertificates: [] I have used a custom Docker image to package the MongoDB Kafka connector. It uses the Strimzi Kafka image as (strimzi/kafka) the base\nimage: abhirockzz/strimzi-kafkaconnect-mongodb:latest For details, check out https://strimzi.io/docs/latest/#creating-new-image-from-base-str\nHere is the Dockerfile - you can tweak it, use a different one, upload to any Docker registry and reference that in the Kafka Connect manifest\nFROM strimzi/kafka:0.17.0-kafka-2.4.0 USER root:root COPY ./connectors/ /opt/kafka/plugins/ USER 1001 We are almost ready to create a Kafka Connect instance. Before that, make sure that you update the bootstrapServers property with the one for Azure Event Hubs endpoint e.g.\nspec: version: 2.4.0 replicas: 1 bootstrapServers: \u003creplace-with-eventhubs-namespace\u003e.servicebus.windows.net:9093 To create the Kafka Connect instance:\nkubectl apply -f deploy/kafka-connect.yaml To confirm:\nkubectl get kafkaconnects NAME DESIRED REPLICAS my-connect-cluster 1 This will create a Deployment and a corresponding Pod\nkubectl get pod -l=strimzi.io/cluster=my-connect-cluster NAME READY STATUS RESTARTS AGE my-connect-cluster-connect-5bf9db5d9f-9ttg4 1/1 Running 0 1h You have a Kafka Connect cluster in Kubernetes! Check out the logs using kubectl logs \u003cpod name\u003e\nCheck Azure Event Hubs - in the Azure Portal, open your Azure Event Hubs namespace and click on the Event Hubs tab, you should see Kafka Connect (internal) topics","clean-up#Clean up":"Once you are done exploring the application, you can delete the resources. If you placed Azure services (AKS, Event Hubs, Cosmos DB) under the same resource group, its easy executing a single command.\nPlease be aware that this will delete all the resources in the group which includes the ones you created as part of the tutorial as well as any other service instances you might have if you used an already existing resource group\naz group delete --name $AZURE_RESOURCE_GROUP_NAME ","conclusion#Conclusion":"As mentioned before, this was a simplified example to help focus on the different components and moving parts e.g. Kafka, Kubernetes, MongoDB, Kafka Connect etc. I demonstrated a use case where the record was modified before finally storing in the sink collection, but there are numerous other options which the connector offers, all of which are config based and do not require additional code (although the there are integration hooks as well). Some of the example include, using custom pipelines in the source connector, [post-processors] (https://docs.mongodb.com/kafka-connector/current/kafka-sink-postprocessors/#post-processing-of-documents) in the sink connector etc.","mongodb-kafka-connectors-1#MongoDB Kafka Connectors":"Source connector We will now setup the source connector. Here is the definition:\napiVersion: kafka.strimzi.io/v1alpha1 kind: KafkaConnector metadata: name: mongodb-source-connector labels: strimzi.io/cluster: my-connect-cluster spec: class: com.mongodb.kafka.connect.MongoSourceConnector tasksMax: 2 config: connection.uri: [AZURE_COSMOSDB_CONNECTION_STRING] topic.prefix: mongo database: [MONGODB_DATABASE_NAME] collection: [MONGODB_COLLECTION_NAME] copy.existing: true key.converter\": org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable: false value.converter: org.apache.kafka.connect.json.JsonConverter value.converter.schemas.enable: false publish.full.document.only: true pipeline: \u003e [{\"$match\":{\"operationType\":{\"$in\":[\"insert\",\"update\",\"replace\"]}}},{\"$project\":{\"_id\":1,\"fullDocument\":1,\"ns\":1,\"documentKey\":1}}] We use the label to refer to the kafka cluster we had just setup\nmetadata: name: mongodb-source-connector labels: strimzi.io/cluster: my-connect-cluster In the config section, we enter the connector config including the MongoDB connection string, database and collection names, whether we want to copy over existing data etc. The topic.prefix attribute is added to database \u0026 collection names to generate the name of the Kafka topic to publish data to. e.g. if the database and collection names are test_db, test_coll respectively, then the Kafka topic name will be mongo.test_db.test_coll. Also, the publish.full.document.only is set to true - this means that, only the document which has been affected (created, updated, replaced) will be published to Kafka, and not the entire change stream document (which contains a lot of other info)\nFor details, refer to the docs: https://docs.mongodb.com/kafka-connector/current/kafka-source/#source-connector-configuration-properties\nIn addition to this, I want to highlight the pipeline attribute:\npipeline: \u003e [{\"$match\":{\"operationType\":{\"$in\":[\"insert\",\"update\",\"replace\"]}}},{\"$project\":{\"_id\":1,\"fullDocument\":1,\"ns\":1,\"documentKey\":1}}] This is nothing but JSON (embedded within YAML.. what a joy!) which defines a custom pipeline. In case of the MongoDB API for Azure Cosmos DB, this is mandatory, due to the constraints in the Change Streams feature (at the time of writing). Please refer to this section in the [Azure Cosmos DB documentation] for details\nLet’s do one last thing before deploying the connector. To confirm that our setup for the source connector is indeed working, we will need to keep an eye on the Kafka topic in Event Hubs\nSince we had specified copy.existing: true config for the connector, the existing items in the collection should be sent to the Kafka topic.\nThere are many ways you can do this. This document includes a lot of helpful links including, kafkacat, Kafka CLI etc.\nI will be using kafkacat\nInstall kafkacat - https://github.com/edenhill/kafkacat#install e.g. brew install kafkacat on mac. replace the properties in kafkacat.conf file (in the GitHub repo)\nmetadata.broker.list=[EVENTHUBS_NAMESPACE].servicebus.windows.net:9093 security.protocol=SASL_SSL sasl.mechanisms=PLAIN sasl.username=$ConnectionString sasl.password=Endpoint=sb://[EVENTHUBS_NAMESPACE].servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=[EVENTHUBS_ACCESS_KEY] Export environment variables\nexport KAFKACAT_CONFIG=kafkacat.conf export BROKER=[EVENTHUBS_NAMESPACE].servicebus.windows.net:9093 export TOPIC=[KAFKA_TOPIC e.g. mongo.test_db.test_coll] The value for TOPIC follows a template, depending on the following connector config properties:\n\u003ctopic.prefix\u003e.\u003cdatabase\u003e.\u003ccollection\u003e … and invoke kafkacat:\nkafkacat -b $BROKER -t $TOPIC -o beginning In the connector manifest file, update the Azure Cosmos DB connection string, name of MongoDB database as well as collection\ne.g.\n... connection.uri: mongodb://\u003cCOSMOSDB_ACCOUNT_NAME\u003e:\u003cCOSMOSDB_PRIMARY_KEY\u003e@\u003cCOSMOSDB_ACCOUNT_NAME\u003e.mongo.cosmos.azure.com:10255/?ssl=true\u0026replicaSet=globaldb\u0026maxIdleTimeMS=120000\u0026appName=@\u003cCOSMOSDB_ACCOUNT_NAME\u003e@ topic.prefix: mongo database: my_source_db collection: my_source_coll ... Ok, you’re all set. From a different terminal, deploy the connector\nkubectl apply -f deploy/mongodb-source-connector.yaml To confirm, simply list the connectors:\nkubectl get kafkaconnectors NAME AGE mongodb-source-connector 70s The connector should spin up and start weaving its magic. If you want to introspect the Kafka Connect logs:\nkubectl logs -f $(kubectl get pod -l=strimzi.io/cluster=my-connect-cluster -o jsonpath='{.items[0].metadata.name}') As per instructions, if you had created items in the source MongoDB collection, check the kafkacat terminal - you should see the Kafka topic records popping up. Go ahead and add a few more items to the MongoDB collection and confirm that you can see them in the kafkacat consumer terminal\nResume feature: the connector has the ability to continue processing from a specific point in time. As per connector docs - “The top-level _id field is used as the resume token which is used to start a change stream from a specific point in time.”\nSink connector We have the first half of the setup using which we can post MongoDB operations details to a Kafka topic. Let’s finish the other half which will transform the data in the Kafka topic and store it in a destination MongoDB collection. For this, we will use the Sink connector - here is the definition\napiVersion: kafka.strimzi.io/v1alpha1 kind: KafkaConnector metadata: name: mongodb-sink-connector labels: strimzi.io/cluster: my-connect-cluster spec: class: com.mongodb.kafka.connect.MongoSinkConnector tasksMax: 2 config: topics: [EVENTHUBS_TOPIC_NAME] connection.uri: [AZURE_COSMOSDB_CONNECTION_STRING] database: [MONGODB_DATABASE_NAME] collection: [MONGODB_COLLECTION_NAME] post.processor.chain: com.mongodb.kafka.connect.sink.processor.DocumentIdAdder,com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder key.converter: org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable: false value.converter: org.apache.kafka.connect.json.JsonConverter value.converter.schemas.enable: false In the config section we need to specify the source Kafka topic (using topics) - this is the same Kafka topic to which the source connector has written the records to. database and collection should be populated with the names of the destination database and collection respectively. Note the post.processor.chain attribute contains com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder - this automatically adds an attribute (topic-partition-offset) to the MongoDB document and captures the Kafka topic, partition and offset values\ne.g. \"topic-partition-offset\" : \"mongo.test_db1.test_coll1-0-74\", where mongo.test_db1.test_coll1 is the topic name, 0 is the partition and 74 is the offset\nBefore creating the sink connector, update the manifest with MongoDB connection string, name of the source Kafka topic as well as the sink database and collection\ne.g.\n... config: topics: mongo.my_source_db.my_source_coll connection.uri: mongodb://\u003cCOSMOSDB_ACCOUNT_NAME\u003e:\u003cCOSMOSDB_PRIMARY_KEY\u003e@\u003cCOSMOSDB_ACCOUNT_NAME\u003e.mongo.cosmos.azure.com:10255/?ssl=true\u0026replicaSet=globaldb\u0026maxIdleTimeMS=120000\u0026appName=@\u003cCOSMOSDB_ACCOUNT_NAME\u003e@ database: my_sink_db collection: my_sink_coll ... You can now deploy the connector:\nkubectl apply -f deploy/mongodb-sink-connector.yaml To confirm, simply list the connectors:\nkubectl get kafkaconnectors NAME AGE mongodb-source-connector 70s mongodb-sink-connector 70s To start with, the connector copies over existing records in the Kafka topic (if any) into the sink collection. If you had initially created items in source Azure Cosmos DB collection, they should have been copied over to Kafka topic (by the source connector) and subsequently persisted to the sink Azure Cosmos DB collection by the sink connector - to confirm this, query Azure Cosmos DB using any of the methods mentioned previously\nHere is a sample record (notice the topic-partition-offset attribute)\n{ \"_id\" : ObjectId(\"5eb937e5a68a237befb2bd44\"), \"name\" : \"foo72\", \"email\" : \"foo72@bar.com\", \"status\" : \"online\", \"topic-partition-offset\" : \"mongo.test_db1.test_coll1-0-74\", \"CREATE_TIME\" : 1589196724357 } You can continue to experiment with the setup. Add, update and delete items in the source MongoDB collection and see the results…","overview#Overview":"Here is an overview of the different components:\nI have used a contrived/simple example in order to focus on the plumbing, moving parts\nMongoDB Kafka Connector(s) The MongoDB Kafka Connect integration provides two connectors: Source and Sink\nSource Connector: It pulls data from a MongoDB collection (that acts as a source) and writes them to Kafka topic Sink connector: It is used to process the data in Kafka topic(s), persist them to another MongoDB collection (thats acts as a sink) These connectors can be used independently as well, but in this blog, we will use them together to stitch the end-to-end solution\nStrimzi overview Strimzi simplifies the process of running Apache Kafka in a Kubernetes cluster by providing container images and Operators for running Kafka on Kubernetes. It is a part of the Cloud Native Computing Foundation as a Sandbox project (at the time of writing)\nStrimzi Operators are fundamental to the project. These Operators are purpose-built with specialist operational knowledge to effectively manage Kafka. Operators simplify the process of: Deploying and running Kafka clusters and components, Configuring and securing access to Kafka, Upgrading and managing Kafka and even taking care of managing topics and users.","pre-requisites#Pre-requisites":"kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl/\nIf you choose to use Azure Event Hubs, Azure Kubernetes Service or Azure Cosmos DB you will need a Microsoft Azure account. Go ahead and sign up for a free one!\nAzure CLI or Azure Cloud Shell - you can either choose to install the Azure CLI if you don’t have it already (should be quick!) or just use the Azure Cloud Shell from your browser.\nHelm I will be using Helm to install Strimzi. Here is the documentation to install Helm itself - https://helm.sh/docs/intro/install/\nYou can also use the YAML files directly to install Strimzi. Check out the quick start guide here - https://strimzi.io/docs/quickstart/latest/#proc-install-product-str\nLet’s start by setting up the required Azure services (if you’re not using Azure, skip this section but please ensure you have the details for your Kafka cluster i.e. broker URLs and authentication credentials, if applicable)\nI recommend installing the below services as a part of a single Azure Resource Group which makes it easy to clean up these services\nAzure Cosmos DB You need to create an Azure Cosmos DB account with the MongoDB API support enabled along with a Database and Collection. Follow these steps to setup Azure Cosmos DB using the Azure portal:\nCreate an Azure Cosmos DB account Add a database and collection and get the connection string Learn more about how to Work with databases, containers, and items in Azure Cosmos DB\nIf you want to use the Azure CLI or Cloud Shell, here is the sequence of commands which you need to execute:\nCreate an Azure Cosmos DB account (notice --kind MongoDB)\naz cosmosdb create --resource-group \u003cRESOURCE_GROUP\u003e --name \u003cCOSMOS_DB_NAME\u003e --kind MongoDB Create the database\naz cosmosdb mongodb database create --account-name \u003cCOSMOS_DB_ACCOUN\u003e --name \u003cCOSMOS_DB_NAME\u003e --resource-group \u003cRESOURCE_GROUP\u003e Finally, create a collection within the database\naz cosmosdb mongo collection create --account-name \u003cCOSMOS_DB_ACCOUNT\u003e --database-name \u003cCOSMOS_DB_NAME\u003e --name \u003cCOSMOS_COLLECTION_NAME\u003e --resource-group-name \u003cRESOURCE_GROUP\u003e --shard \u003cSHARDING_KEY_PATH\u003e Get the connection string and save it. You will be using it later\naz cosmosdb list-connection-strings --name \u003cCOSMOS_DB_ACCOUNT\u003e --resource-group \u003cRESOURCE_GROUP\u003e -o tsv --query connectionStrings[0].connectionString Seed the collection with some data. There are many ways you could do this. For the purposes of this tutorial, I would recommend quick and easy, such as:\nThe Data Explorer tab available in the Azure portal (when you create an Azure Cosmos DB account) Azure Cosmos DB explorer (a standalone web-based interface ) Native Mongo shell (via the Data Explorer tab in Azure Portal) Later on, when we deploy the source connector, we will double check to see if these (existing) items/records are picked up by the connector and sent to Kafka\nAzure Event Hubs Azure Event Hubs is a data streaming platform and event ingestion service and it also provides a Kafka endpoint that can be used by existing Kafka based applications as an alternative to running your own Kafka cluster. Event Hubs supports Apache Kafka protocol 1.0 and later, and works with existing Kafka client applications and other tools in the Kafka ecosystem including Kafka Connect (demonstrated in this blog), MirrorMaker etc.\nTo setup an Azure Event Hubs cluster, you can choose from a variety of options including the Azure portal, Azure CLI, Azure PowerShell or an ARM template. Once the setup is complete, you will need the connection string (that will be used in subsequent steps) for authenticating to Event Hubs - use this guide to finish this step.\nPlease ensure that you also create an Event Hub (same as a Kafka topic) to act as the target for our Kafka Connect connector (details in subsequent sections)\nAzure Kubernetes Service Azure Kubernetes Service (AKS) makes it simple to deploy a managed Kubernetes cluster in Azure. It reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. Here are examples of how you can setup an AKS cluster using Azure CLI, Azure portal or ARM template\nLet’s move on to the Kubernetes components now:\nPlease note that I am re-using part of the sections from the previous blog post (installation is the same after all!), but trying to keep it short at the same time to avoid repetition. For the parts that have been omitted e.g. explanation of the Strimzi component spec for Kafka Connect etc., I would request you to check out that blog","resources#Resources":"That’s all for this blog. As always, stay tuned for more!\nI’ll leave you with a few resources:\nMongoDB Kafka Connector documentation - https://docs.mongodb.com/kafka-connector/current/ MongoDB Kafka Connector GitHub repo - https://github.com/mongodb/mongo-kafka Strimzi documentation - https://strimzi.io/docs/latest/ Kafka Connect - https://kafka.apache.org/documentation/#connect "},"title":"Data pipeline using MongoDB and Kafka Connect on Kubernetes"},"/blog/msk-dynamodb-data-pipeline-1/":{"data":{"":"Integrate DynamoDB with MSK and MSK Connect\nThere are many ways to stitch data pipelines - open source components, managed services, ETL tools, etc. In the Kafka world, Kafka Connect is the tool of choice for “streaming data between Apache Kafka and other systems”. It has an extensive set of pre-built source and sink connectors as well as a common framework for Kafka connectors which standardises integration of other data systems with Kafka and making it simpler to develop your own connectors, should there be a need to do so.\nThis is a two-part blog series which provides a step-by-step walkthrough of data pipelines with Kafka and Kafka Connect. I will be using AWS for demonstration purposes, but the concepts apply to any equivalent options (e.g. running these locally using Docker). Here are some of the key AWS services I will be using:\nAmazon Managed Streaming for Apache Kafka (MSK) - Central component of the data infrastructure. MSK Connect - It will be used to deploy fully managed connectors built for Kafka Connect in order to move data into or pull data from various sources. Amazon DynamoDB is a fully managed NoSQL database service and in the context of this blog series, it serves as the target/sink for our data pipeline. Amazon Aurora MySQL is a fully managed, MySQL-compatible, relational database engine and is used in the second part of this blog series. Here is a quick peek of what each part will cover:\nThe first part will keep things relatively simple - it’s all about get started easily. I will be using the Kafka Connect Datagen source connector to pump data some sample data into MSK topic and then use the AWS DynamoDB sink connector to persist that data in a DynamoDB table. The second part will take it up a notch - we will explore Change Data Capture. The Datagen connector will be replaced by the Debezium connector for MySQL which will extract data in real-time from tables in Aurora MySQL, push that to MSK topics. Then, we will continue to use the DynamoDB sink connector just like we did before. ","conclusion#Conclusion":"Managed environments like MSK Connect take care of the heavy lifting and let you focus on building your data architectures. This blog focused on getting you up and running with a simple data pipeline with DynamoDB as the sink. The next part will include Change Data Capture and walk you through how to build a solution using the components covered in this post.","delete-resources#Delete resources":"Unless you intend to work through the second part of this blog series (coming soon), delete the resources.\nDelete the contents of the Amazon S3 bucket (msk-lab-\u003cYOUR ACCOUNT_ID\u003e-plugins-bucket) Delete the CloudFormation stack Delete the DynamoDB table Delete the MSK Connect connectors, Plugins and Custom configuration ","pipeline-part-1#Pipeline part 1":"Let’s start by creating the first half of the pipeline that will leverage Datagen source connector to pump sample events to a topic in MSK.\nIn this section, you will:\nDownload the Datagen connector artefacts Create Custom Plugin in MSK Deploy the Datagen source connector to MSK Connect At the end, you will have the first half of the data pipeline ready to go!\nCreate a Custom plugin and Connector Upload the Datagen connector file to Amazon S3\nFrom the Kafka client EC2 instance, run these commands:\nsudo -u ec2-user -i mkdir kafka-connect-datagen \u0026\u0026 cd kafka-connect-datagen wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-datagen/versions/0.5.3/confluentinc-kafka-connect-datagen-0.5.3.zip aws s3 cp ./confluentinc-kafka-connect-datagen-0.5.3.zip s3://msk-lab-\u003cENTER_YOUR_AWS_ACCOUNT_ID\u003e-plugins-bucket/ cd .. Create Custom Plugin\nFor step by step instructions on how to create a MSK Connect Plugin, refer to Creating a custom plugin using the AWS Management Console in the official documentation.\nWhile creating the Custom Plugin, make sure to choose the Datagen connector zip file you uploaded to Amazon S3 in the previous step.\nCreate the Datagen source connector\nFor step by step instructions on how to create a MSK Connect Connector, refer to Creating a connector in the official documentation.\nTo create a connector:\nChoose the plugin you just created. Enter the connector name and choose the MSK cluster along with IAM authentication You can enter the content provided below in the connector configuration section connector.class=io.confluent.kafka.connect.datagen.DatagenConnector kafka.topic=orders quickstart=orders key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.json.JsonConverter value.converter.schemas.enable=false max.interval=10000 iterations=-1 tasks.max=1 Leave the rest of configuration unchanged\nUnder Access permissions, choose the correct IAM role (the one with DatagenConnectorIAMRole in its name) for the connector Click Next to move to the Security options - leave them unchanged Click Next. For Log delivery, choose Deliver to Amazon CloudWatch Logs. Locate and select /msk-connect-demo-cwlog-group Click Next - On the final page, scroll down and click Create connector to start the process and wait for the connector to start. Once that’s done and the connector has transitioned to Running state, proceed with the below steps.\nTest the pipeline Before you proceed further:\nDownload AWS IAM JAR file and include it in the classpath Create a properties file for Kafka CLI consumer In the EC2 instance, run the following commands:\nsudo -u ec2-user -i mkdir iam-auth \u0026\u0026 cd ./iam-auth wget https://github.com/aws/aws-msk-iam-auth/releases/download/1.1.0/aws-msk-iam-auth-1.1.0-all.jar cd ../ cat \u003c\u003cEOF \u003e /home/ec2-user/kafka/config/client-config.properties # Sets up TLS for encryption and SASL for authN. security.protocol = SASL_SSL # Identifies the SASL mechanism to use. sasl.mechanism = AWS_MSK_IAM # Binds SASL client implementation. sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required; # Encapsulates constructing a SigV4 signature based on extracted credentials. # The SASL client bound by \"sasl.jaas.config\" invokes this class. sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler EOF export CLASSPATH=/home/ec2-user/iam-auth/aws-msk-iam-auth-1.1.0-all.jar echo \"export CLASSPATH=${CLASSPATH}\" | tee -a ~/.bash_profile To start with, list down the Kafka topics:\nexport MSK_BOOTSTRAP_ADDRESS=\u003cENTER MSK CLUSTER ENDPOINT\u003e /home/ec2-user/kafka/bin/kafka-topics.sh --bootstrap-server $MSK_BOOTSTRAP_ADDRESS --command-config /home/ec2-user/kafka/config/client-config.properties --list Start Kafka CLI consumer:\n/home/ec2-user/kafka/bin/kafka-console-consumer.sh --bootstrap-server $MSK_BOOTSTRAP_ADDRESS --consumer.config /home/ec2-user/kafka/config/client-config.properties --from-beginning --topic orders | jq --color-output . If everything is setup correctly, you should see JSON output similar to this:\n... { \"ordertime\": 1488059390707, \"orderid\": 50, \"itemid\": \"Item_845\", \"orderunits\": 4.801443003705596, \"address\": { \"city\": \"City_\", \"state\": \"State_6\", \"zipcode\": 34225 } } { \"ordertime\": 1496655341559, \"orderid\": 51, \"itemid\": \"Item_71\", \"orderunits\": 6.184874231875158, \"address\": { \"city\": \"City_63\", \"state\": \"State_91\", \"zipcode\": 77633 } } ... ","pipeline-part-2#Pipeline part 2":"The Datagen source connector will continue to produce sample order data as long as it’s running. It will produce 1 record every 10 seconds - as per our configuration (max.interval=10000 and iterations=-1).\nNext up, we will implement the second half of the pipeline that’s responsible for taking data from MSK topic to DynamoDB table with the help of the DynamoDB Sink connector.\nIn this section, you will:\nDownload the DynamoDB connector artifacts Create Custom Plugin in MSK Deploy the DynamoDB sink connector to MSK Connect Create a Custom plugin and Connector Upload the DynamoDB connector to Amazon S3\nLog into the Kafka client EC2 instance:\nsudo -u ec2-user -i mkdir kafka-connect-dynamodb \u0026\u0026 cd kafka-connect-dynamodb wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-aws-dynamodb/versions/1.3.0/confluentinc-kafka-connect-aws-dynamodb-1.3.0.zip aws s3 cp ./confluentinc-kafka-connect-aws-dynamodb-1.3.0.zip s3://msk-lab-\u003cENTER_YOUR_AWS_ACCOUNT_ID\u003e-plugins-bucket/ cd .. Create Custom Plugin\nFor step by step instructions on how to create a MSK Connect Plugin, refer to Creating a custom plugin using the AWS Management Console in the official documentation.\nWhile creating the Custom Plugin, make sure to choose the DynamoDB connector zip file you uploaded to Amazon S3 in the previous step.\nCreate the DynamoDB sink connector\nFor step by step instructions on how to create a MSK Connect Connector, refer to Creating a connector in the official documentation.\nTo create a connector:\nChoose the plugin you just created.\nEnter the connector name and choose the MSK cluster along with IAM authentication\nYou can enter the content provided below in the connector configuration section. Make sure you replace the following configuration as per your setup:\nUse the right topic name (its is orders in our example) Enter MSK broker endpoint in confluent.topic.bootstrap.servers For aws.dynamodb.endpoint and aws.dynamodb.region, enter the region where you created the DynamoDB table e.g. us-east-1 Leave the rest of configuration unchanged\nconnector.class=io.confluent.connect.aws.dynamodb.DynamoDbSinkConnector tasks.max=1 topics=orders aws.dynamodb.region=\u003cENTER AWS REGION e.g. us-east-1\u003e aws.dynamodb.endpoint=https://dynamodb.\u003cENTER AWS REGION\u003e.amazonaws.com aws.dynamodb.pk.hash=value.orderid aws.dynamodb.pk.sort= table.name.format=kafka_${topic} transforms=flatten transforms.flatten.type=org.apache.kafka.connect.transforms.Flatten$Value transforms.flatten.delimiter=_ key.converter.schemas.enable=false value.converter.schemas.enable=false key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.json.JsonConverter confluent.topic.bootstrap.servers=\u003cENTER MSK CLUSTER ENDPOINT\u003e confluent.topic.security.protocol=SASL_SSL confluent.topic.sasl.mechanism=AWS_MSK_IAM confluent.topic.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required; confluent.topic.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler Under Access permissions, choose the correct IAM role (the one with DynamoDBConnectorIAMRole in its name) for the connector Click Next to move to the Security options - leave them unchanged Click Next. For Log delivery, choose Deliver to Amazon CloudWatch Logs. Locate and select /msk-connect-demo-cwlog-group Click Next - On the final page, scroll down and click Create connector to start the process and wait for the connector to start. Once that’s done and the connector has transitioned to Running state, proceed with the below steps.\nBefore we go ahead and test the pipeline, a couple of things you should know:\nChoosing DynamoDB primary keys\nIn the above configuration we set aws.dynamodb.pk.hash to value.orderid which implies that the orderid field from the Kafka topic event payload will be used as the partition key (the aws.dynamodb.pk.sort was left empty, but can be used to specify the DynamoDB Sort/Range key if needed).\nFlattening records using Kafka Connect SMT\nThe address field in the event payload has a nested structure.\n\"address\": { \"city\": \"City_63\", \"state\": \"State_91\", \"zipcode\": 77633 } To un-nest or flatten (for the lack of a better word) it, we’ve made use of the Flatten transform (org.apache.kafka.connect.transforms.Flatten$Value). This extracts individual fields from address and makes them available as individual attributes - address_city, address_state, address_zipcode. You will see the same in DynamoDB table as well, soon!\nTest the end to end pipeline Navigate to the DynamoDB console. You will see that the kafka_orders table is already present - this was automatically created by the DynamoDB sink connector.\nThe table has orderid as the Partition key\nIf you have the AWS CLI handy, you can look at the data quickly using - aws dynamodb scan --table-name kafka_orders.\nYou will get a similar output (notice the address_* fields):\n{ \"Items\": [ { \"orderid\": { \"N\": \"251\" }, \"address_zipcode\": { \"N\": \"68100\" }, \"address_state\": { \"S\": \"State_2\" }, \"itemid\": { \"S\": \"Item_46\" }, \"ordertime\": { \"N\": \"1491423463934\" }, \"address_city\": { \"S\": \"City_6\" }, \"orderunits\": { \"N\": \"3.1272028351151926\" } }, ..... Go ahead, query and play around the data in the DynamoDB table as you like. That’s your homework!\nAs the connector weaves its magic, it will continue to synchronise records in the Kafka topic to the DynamoDB table. Remember that the data pipeline (from Datagen source -\u003e MSK topic -\u003e DynamoDB) will continue to be operational as long as the connectors are running - records will keep getting added to the orders topic in MSK and they will be persisted to DynamoDB table.","prepare-infrastructure-components-and-services#Prepare infrastructure components and services":"To start with, you need to deploy all the resources required for this tutorial. There are lots of them, but don’t worry because I have a CloudFormation template ready for you!\nBefore you proceed, download the template from this link\nHere is a high level diagram of the solution presented in this blog post.\nFor step by step instructions, refer to Creating a stack on the AWS CloudFormation console in the official documentation\nUse the AWS Console to deploy the CloudFormation template - on the Create stack wizard, choose Upload a template file and upload the file you just downloaded.\nClick Next, enter the name of the stack. Click Next to proceed - on the final page in the wizard, click Create stack to initiate the resource creation.\nYou can track the progress in the CloudFormation console. Once successful, you should have all the resources including:\nCore infrastructure: VPC, Subnets etc. Services: MSK cluster, Aurora MySQL etc. Others: IAM roles, CloudWatch log groups etc. Connect to the EC2 instance via Session Manager\nIn the CloudFormation list of resources, locate KafkaClientEC2Instance EC2 instance (highlighted in the above diagram)\nConnect to it using Session Manager:\nEnable automatic topic creation\nMSK Connect requires topic creation on the fly. We can create a Custom configuration in MSK to enable automatic topic creation.\nFrom the EC2 instance, run the below commands to create custom configuration:\nsudo -u ec2-user -i mkdir kafka-configuration \u0026\u0026 cd kafka-configuration cat \u003c\u003cEOF \u003e configuration.txt auto.create.topics.enable=true EOF export AWS_REGION=\u003center MSK cluster region e.g. us-east-1\u003e export AWS_REGION=us-east-1 aws kafka create-configuration \\ --name \"CustomConfiguration\" \\ --description \"Topic auto-creation enabled\" \\ --kafka-versions \"2.6.2\" \\ --region $AWS_REGION \\ --server-properties file://configuration.txt Go ahead and apply this configuration:\nGo to your MSK cluster \u003e Properties \u003e Configuration and choose Edit. Select the configuration you just created and Save. This will restart your MSK cluster - wait for this to complete before you proceed."},"title":"Build a data pipeline on AWS with Kafka, Kafka connect and DynamoDB"},"/blog/msk-serverless-lambda-go-getting-started/":{"data":{"":"In this blog post you will learn how to deploy a Go Lambda function and trigger it in response to events sent to a topic in a MSK Serverless cluster.\nPrerequisites Infrastructure setup Send data to MSK Serverless using producer application Configure and deploy the Lambda function Verify the integration Conclusion The following topics have been covered:\nHow to use the franz-go Go Kafka client to connect to MSK Serverless using IAM authentication. Write a Go Lambda function to process data in MSK topic. Create the infrastructure - VPC, subnets, MSK cluster, Cloud9 etc. Configure Lambda and Cloud9 to access MSK using IAM roles and fine-grained permissions. ","conclusion#Conclusion":"You were able to setup, configure and deploy a Go Lambda function and trigger it in response to events sent to a topic in a MSK Serverless cluster!","configure-and-deploy-the-lambda-function#Configure and deploy the Lambda function":"Create Lambda execution IAM role and attach the policy\naws iam create-role --role-name LambdaMSKRole --assume-role-policy-document file://lambda-trust-policy.json aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaMSKExecutionRole --role-name LambdaMSKRole Before creating the policy, update the msk-consumer-policy.json file to reflect the required details including MSK cluster ARN etc.\naws iam put-role-policy --role-name LambdaMSKRole --policy-name MSKConsumerPolicy --policy-document file://msk-consumer-policy.json Build and deploy the Go function and create a zip file\nBuild and zip the function code:\nGOOS=linux go build -o app zip func.zip app Deploy to Lambda:\nexport LAMBDA_ROLE_ARN=\u003center the ARN of the LambdaMSKRole created above e.g. arn:aws:iam::\u003cyour AWS account ID\u003e:role/LambdaMSKRole\u003e aws lambda create-function \\ --function-name msk-consumer-function \\ --runtime go1.x \\ --zip-file fileb://func.zip \\ --handler app \\ --role $LAMBDA_ROLE_ARN Lambda VPC configuration\nMake sure you choose the same VPC and private subnets as the MSK cluster. Also, select the same security group ID as MSK (for convenience) - if you select a different one, make sure to update MSK security group to add an inbound rule (for port 9098), just like you did for the Cloud9 instance in an earlier step.\nConfigure the MSK trigger for the function\nMake sure to choose the right MSK Serverless cluster and enter the correct topic name.","infrastructure-setup#Infrastructure setup":"Create VPC and other resources\nUse a CloudFormation template for this.\naws cloudformation create-stack --stack-name msk-vpc-stack --template-body file://template.yaml Wait for the stack creation to complete before proceeding to other steps.\nCreate MSK Serverless cluster\nUse AWS Console to create the cluster.\nConfigure the VPC and private subnets created in the previous step.\nCreate AWS Cloud9 instance\nMake sure its in the same VPC as the MSK Serverless cluster and choose the public subnet that you created earlier.\nConfigure MSK cluster security group\nAfter the Cloud9 instance is created, edit the MSK cluster security group to allow access from the Cloud9 instance.\nConfigure Cloud9 to send data to MSK Serverless cluster\nThe code that we run from Cloud9 is going to produce data to the MSK Serverless cluster. So we need to ensure that it has the right privileges. For this, we need to create an IAM role and attach required permissions policy.\naws iam create-role --role-name Cloud9MSKRole --assume-role-policy-document file://ec2-trust-policy.json Before creating the policy, update the msk-producer-policy.json file to reflect the required details including MSK cluster ARN etc.\naws iam put-role-policy --role-name Cloud9MSKRole --policy-name MSKProducerPolicy --policy-document file://msk-producer-policy.json Attach the IAM role to the Cloud9 EC2 instance:","prerequisites#Prerequisites":"You will need an AWS account, install AWS CLI as well a recent version of Go (1.18 or above).\nClone this GitHub repository and change to the right directory:\ngit clone https://github.com/abhirockzz/lambda-msk-serverless-trigger-golang cd lambda-msk-serverless-trigger-golang ","send-data-to-msk-serverless-using-producer-application#Send data to MSK Serverless using producer application":"Log into the Cloud9 instance and run the producer application (its a Docker image) from a terminal.\nexport MSK_BROKER=\u003center the MSK Serverless endpoint\u003e export MSK_TOPIC=test-topic docker run -p 8080:8080 -e MSK_BROKER=$MSK_BROKER -e MSK_TOPIC=$MSK_TOPIC public.ecr.aws/l0r2y6t0/msk-producer-app The application exposes a REST API endpoint using which you can send data to MSK.\ncurl -i -X POST -d 'test event 1' http://localhost:8080 This will create the specified topic (since it was missing to begin with) and also send the data to MSK.\nNow that the cluster and producer application are ready, we can move on to the consumer. Instead of creating a traditional consumer, we will deploy a Lambda function that will be automatically invoked in response to data being sent to the topic in MSK.","verify-the-integration#Verify the integration":"Go back to the Cloud9 terminal and send more data using the producer application\nI used a handy json utility called jo (sudo yum install jo)\nAPP_URL=http://localhost:8080 for i in {1..5}; do jo email=user${i}@foo.com name=user${i} | curl -i -X POST -d @- $APP_URL; done In the Lambda function logs, you should see the messages that you sent."},"title":"Getting started with MSK Serverless and AWS Lambda using Go"},"/blog/msk-to-dynamodb-with-kafka/":{"data":{"":"","conclusion--wrap-up#Conclusion \u0026amp; wrap up":"Use change data capture with MSK Connect to sync data between Aurora MySQL and DynamoDB\nThis is the second part of the blog series which provides a step-by-step walkthrough of data pipelines with Kafka and Kafka Connect. I will be using AWS for demonstration purposes, but the concepts apply to any equivalent options (e.g. running these locally in Docker).\nThis part will show Change Data Capture in action that let’s you track row-level changes in database tables in response to create, update and delete operations. For example, in MySQL, these change data events are exposed via the MySQL binary log (binlog).\nIn Part 1, we used the Datagen connector in the source part of the data pipeline - it helped us generate mock data to MSK topic and keep things simple. We will use Aurora MySQL as the source of data and leverage it’s Change Data Capture capability with the Debezium connector for MySQL to extract data in real-time from tables in Aurora MySQL, push that to MSK topics. Then, we will continue to use the DynamoDB sink connector just like we did before.\nIf you’re new to Debezium…\nIt is a distributed platform that builds on top of Change Data Capture features available in different databases. It provides a set of Kafka Connect connectors which tap into row-level changes (using CDC) in database table(s) and convert them into event streams. These are sent to Kafka and can be made available to all the downstream applications.\nHere is a high level diagram of the solution presented in this blog post.\nI am assuming that you are following along from Part 1 where the creation process for the base infrastructure and services required for this tutorial were already covered. If you haven’t already, refer to the Prepare infrastructure components and services section in part 1 section\nData pipeline part 1: Aurora MySQL to MSK Let’s start by creating the first half of the pipeline to synchronise data from Aurora MySQL table to a topic in MSK.\nIn this section, you will:\nDownload the Debezium connector artefacts Create Custom Plugin in MSK Deploy the Debezium source connector to MSK Connect At the end, you will have the first half of the data pipeline ready to go!\nCreate a Custom plugin and Connector Upload the Debezium connector to Amazon S3\nLog into the Kafka client EC2 instance and run these commands:\nsudo -u ec2-user -i mkdir debezium \u0026\u0026 cd debezium wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/1.9.0.Final/debezium-connector-mysql-1.9.0.Final-plugin.tar.gz tar xzf debezium-connector-mysql-1.9.0.Final-plugin.tar.gz cd debezium-connector-mysql zip -9 ../debezium-connector-mysql-1.9.0.Final-plugin.zip * cd .. aws s3 cp ./debezium-connector-mysql-1.9.0.Final-plugin.zip s3://msk-lab-\u003cENTER_YOUR_AWS_ACCOUNT_ID\u003e-plugins-bucket/ Create Custom Plugin\nFor step by step instructions on how to create a MSK Connect Plugin, refer to Creating a custom plugin using the AWS Management Console in the official documentation.\nWhile creating the Custom Plugin, make sure to choose the Debezium connector zip file you uploaded to S3 in the previous step.\nCreate the Debezium source connector\nFor step by step instructions on how to create a MSK Connect Connector, refer to Creating a connector in the official documentation.\nTo create a connector:\nChoose the plugin you just created.\nEnter the connector name and choose the MSK cluster along with IAM authentication\nYou can enter the content provided below in the connector configuration section. Make sure you replace the following configuration as per your setup:\ndatabase.history.kafka.bootstrap.servers - Enter the MSK cluster endpoint database.hostname - Enter Aurora RDS MySQL Endpoint Leave the rest of configuration unchanged\nconnector.class=io.debezium.connector.mysql.MySqlConnector database.user=master database.server.id=123456 tasks.max=1 database.history.kafka.topic=dbhistory.salesdb database.history.kafka.bootstrap.servers=\u003cENTER MSK CLUSTER ENDPOINT\u003e database.server.name=salesdb database.port=3306 include.schema.changes=true database.hostname=\u003cENTER RDS MySQL ENDPOINT\u003e database.password=S3cretPwd99 database.include.list=salesdb value.converter.schemas.enable=false key.converter.schemas.enable=false key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.json.JsonConverter database.history.consumer.security.protocol=SASL_SSL database.history.consumer.sasl.mechanism=AWS_MSK_IAM database.history.consumer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required; database.history.consumer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler database.history.producer.security.protocol=SASL_SSL database.history.producer.sasl.mechanism=AWS_MSK_IAM database.history.producer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required; database.history.producer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler transforms=unwrap transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState Under Access permissions, choose the correct IAM role (the one with AuroraConnectorIAMRole in its name) for the connector Click Next to move to the Security options - leave them unchanged Click Next. For Log delivery, choose Deliver to Amazon CloudWatch Logs. Locate and select /msk-connect-demo-cwlog-group Click Next - On the final page, scroll down and click Create connector to start the process and wait for the connector to start. Once that’s done and the connector has transitioned to Running state, proceed with the below steps.\nTest the pipeline We want to confirm whether records from the SALES_ORDER table in the salesdb database have been pushed to MSK topic. To do that, from the EC2 host, run the Kafka CLI consumer.\nNote the topic name salesdb.salesdb.SALES_ORDER - this is as per Debezium convention\nsudo -u ec2-user -i export MSK_BOOTSTRAP_ADDRESS=\u003cENTER MSK CLUSTER ENDPOINT\u003e /home/ec2-user/kafka/bin/kafka-console-consumer.sh --bootstrap-server $MSK_BOOTSTRAP_ADDRESS --consumer.config /home/ec2-user/kafka/config/client-config.properties --from-beginning --topic salesdb.salesdb.SALES_ORDER | jq --color-output . In another terminal, use MySQL client and connect to the Aurora database and insert few records:\nsudo -u ec2-user -i export RDS_AURORA_ENDPOINT=\u003cENTER RDS MySQL ENDPOINT\u003e mysql -f -u master -h $RDS_AURORA_ENDPOINT --password=S3cretPwd99 USE salesdb; select * from SALES_ORDER limit 5; INSERT INTO SALES_ORDER (ORDER_ID, SITE_ID, ORDER_DATE, SHIP_MODE) VALUES (29001, 2568, now(), 'STANDARD'); INSERT INTO SALES_ORDER (ORDER_ID, SITE_ID, ORDER_DATE, SHIP_MODE) VALUES (29002, 1649, now(), 'ONE-DAY'); INSERT INTO SALES_ORDER (ORDER_ID, SITE_ID, ORDER_DATE, SHIP_MODE) VALUES (29003, 3861, now(), 'TWO-DAY'); INSERT INTO SALES_ORDER (ORDER_ID, SITE_ID, ORDER_DATE, SHIP_MODE) VALUES (29004, 2568, now(), 'STANDARD'); INSERT INTO SALES_ORDER (ORDER_ID, SITE_ID, ORDER_DATE, SHIP_MODE) VALUES (29005, 1649, now(), 'ONE-DAY'); INSERT INTO SALES_ORDER (ORDER_ID, SITE_ID, ORDER_DATE, SHIP_MODE) VALUES (29006, 3861, now(), 'TWO-DAY'); If everything is setup correctly, you should see the records in the consumer terminal.\n{ \"ORDER_ID\": 29001, \"SITE_ID\": 2568, \"ORDER_DATE\": 1655279536000, \"SHIP_MODE\": \"STANDARD\" } { \"ORDER_ID\": 29002, \"SITE_ID\": 1649, \"ORDER_DATE\": 1655279536000, \"SHIP_MODE\": \"ONE-DAY\" } { \"ORDER_ID\": 29003, \"SITE_ID\": 3861, \"ORDER_DATE\": 1655279563000, \"SHIP_MODE\": \"TWO-DAY\" } ... The secret to compact change event payloads\nNotice how compact the change data capture event payload is. This is because we configured the connector to use io.debezium.transforms.ExtractNewRecordState which is a Kafka Single Message Transform (SMT). By default Debezium change event structure is quite complex - along with the change event, it also includes metadata such as schema, source database info etc. It looks something like this:\n{ \"before\": null, \"after\": { \"ORDER_ID\": 29003, \"SITE_ID\": 3861, \"ORDER_DATE\": 1655279563000, \"SHIP_MODE\": \"TWO-DAY\" }, \"source\": { \"version\": \"1.9.0.Final\", \"connector\": \"mysql\", \"name\": \"salesdb\", \"ts_ms\": 1634569283000, \"snapshot\": \"false\", \"db\": \"salesdb\", \"sequence\": null, \"table\": \"SALES_ORDER\", \"server_id\": 1733046080, \"gtid\": null, \"file\": \"mysql-bin-changelog.000003\", \"pos\": 43275145, \"row\": 0, \"thread\": null, \"query\": null }, \"op\": \"c\", \"ts_ms\": 1655279563000, \"transaction\": null ... Thanks to the Kafka SMT (specified using transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState), we can effectively flatten the event payload and customize it as per our requirements.\nFor details, refer to New Record State Extraction in the Debezium documentation.\nData pipeline part 2: MSK to DynamoDB We can now shift our focus to the second half of the pipeline that’s responsible for taking data from MSK topic to DynamoDB table with the help of the DynamoDB Sink connector.\nIf the DynamoDB table is not present, the connector automatically creates one for you, but it uses default settings i.e. its creates a table in Provisioned Mode, with 10 read capacity units (RCUs) and 10 write capacity units (WCUs).\nBut your use-case might need a configuration. For example, in order to handle high volume of data, you may want to configure Auto scaling, or even better, activate On-Demand mode for your table.\nThat’s exactly what we will do.\nBefore you proceed, create a DynamoDB table Use the following settings:\nTable name - kafka_salesdb.salesdb.SALES_ORDER (do not change the table name) Partition key - ORDER_ID (Number) Range key - SITE_ID (Number) Capacity mode - On-demand That’s it, you’re good to go!\nCreate a Custom plugin and Connector For step by step instructions on how to create a MSK Connect Plugin, refer to Creating a custom plugin using the AWS Management Console in the official documentation.\nWhile creating the Custom Plugin, make sure to choose the DynamoDB connector zip file you uploaded to S3 in the previous step.\nFor step by step instructions on how to create a MSK Connect Connector, refer to Creating a connector in the official documentation.\nTo create a connector:\nChoose the plugin you just created.\nEnter the connector name and choose the MSK cluster along with IAM authentication\nYou can enter the content provided below in the connector configuration section. Make sure you replace the following configuration as per your setup:\nUse the right topic name for topics attribute (we are using salesdb.salesdb.SALES_ORDER in this example, since that’s the topic name format that Debezium source connector adopts) For confluent.topic.bootstrap.servers, enter MSK cluster endpoint For aws.dynamodb.endpoint and aws.dynamodb.region, enter the region where you created the DynamoDB table e.g. us-east-1 Leave the rest of configuration unchanged\nconnector.class=io.confluent.connect.aws.dynamodb.DynamoDbSinkConnector tasks.max=2 aws.dynamodb.region=\u003cENTER AWS REGION e.g. us-east-1\u003e aws.dynamodb.endpoint=https://dynamodb.\u003cENTER AWS REGION\u003e.amazonaws.com topics=salesdb.salesdb.SALES_ORDER value.converter.schemas.enable=false key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.json.JsonConverter table.name.format=kafka_${topic} confluent.topic.bootstrap.servers=\u003cENTER MSK CLUSTER ENDPOINT\u003e confluent.topic.security.protocol=SASL_SSL confluent.topic.sasl.mechanism=AWS_MSK_IAM confluent.topic.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required; confluent.topic.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler aws.dynamodb.pk.hash=value.ORDER_ID aws.dynamodb.pk.sort=value.SITE_ID Under Access permissions, choose the correct IAM role (the one with DynamoDBConnectorIAMRole in its name) for the connector Click Next to move to the Security options - leave them unchanged Click Next. For Log delivery, choose Deliver to Amazon CloudWatch Logs. Locate and select /msk-connect-demo-cwlog-group Click Next - On the final page, scroll down and click Create connector to start the process and wait for the connector to start. Once that’s done and the connector has transitioned to Running state, proceed with the below steps.\nChoosing DynamoDB primary key\nIn the above configuration we set aws.dynamodb.pk.hash and aws.dynamodb.pk.sort to value.ORDER_ID and value.SITE_ID respectively. This implies that the ORDER_ID field from the Kafka topic event payload will be used as the partition key and the value for SITE_ID will we designated as the Range key (depending on your requirements, you can also leave aws.dynamodb.pk.sort empty).\nTest the end to end pipeline As part of the initial load process, the connector makes sure that all the existing records from Kafka topic are persisted to the DynamoDB table specified in the connector configuration. In this case, you should see more than 29000 records (as per SALES_ORDER table) in DynamoDB and you can run queries to explore the data.\nTo continue testing the end to end pipeline, you can insert more data in the SALES_ORDER table and confirm that they were synchronised to Kafka via the Debezium source connector and all the way to DynamoDB, thanks to the sink connector.\nDelete resources Once you’re done, delete the resources that you had created.\nDelete the contents of the S3 bucket (msk-lab-\u003cYOUR ACCOUNT_ID\u003e-plugins-bucket) Delete the CloudFormation stack Delete the DynamoDB table Delete the MSK Connect connectors, Plugins and Custom configuration Conclusion \u0026 wrap up Change Data Capture is a powerful tool, but we need a way to tap into these event logs and make it available to other services which depend on that data. In this part, you saw how we can leverage this capability to setup a streaming data pipeline between MySQL and DynamoDB using Kafka Connect.\nThis wraps up this series. Happy building!","data-pipeline-part-1-aurora-mysql-to-msk#Data pipeline part 1: Aurora MySQL to MSK":"","data-pipeline-part-2-msk-to-dynamodb#Data pipeline part 2: MSK to DynamoDB":"","delete-resources#Delete resources":""},"title":"MySQL to DynamoDB: Build a streaming data pipeline on AWS using Kafka"},"/blog/postgres-azure-data-explorer-cdc/":{"data":{"":"This blog post demonstrates how you can use Change Data Capture to stream database modifications from PostgreSQL to Azure Data Explorer (Kusto) using Apache Kafka.\nChange Data Capture (CDC) can be used to track row-level changes in database tables in response to create, update and delete operations. It is a powerful technique, but useful only when there is a way to leverage these events and make them available to other services.","additional-resources#Additional resources":"If you want to explore further, I would recommend\nExplore Debezium connectors Azure Data Explorer Ingestion overview Explore what you can do with Kusto Query Language Data Explorer connector features What is Azure Event Hubs? Use Change Data Capture with Kafka Connect support on Azure Event Hubs (Preview) ","azure-data-explorer-sink-connector-setup#Azure Data Explorer sink connector setup":"Copy the JSON contents below to a file (you can name it adx-sink-config.json). Replace the values for the following attributes as per your Azure Data Explorer setup - aad.auth.authority, aad.auth.appid, aad.auth.appkey, kusto.tables.topics.mapping (the database name) and kusto.url\n{ \"name\": \"adx-orders-sink\", \"config\": { \"connector.class\": \"com.microsoft.azure.kusto.kafka.connect.sink.KustoSinkConnector\", \"flush.size.bytes\": 10000, \"flush.interval.ms\": 30000, \"tasks.max\": 2, \"topics\": \"myserver.retail.orders_info\", \"kusto.tables.topics.mapping\": \"[{'topic': 'myserver.retail.orders_info','db': '\u003center database name\u003e', 'table': 'Orders','format': 'json', 'mapping':'OrdersEventMapping'}]\", \"aad.auth.authority\": \"\u003center tenant ID from service principal info\u003e\", \"kusto.url\": \"https://ingest-\u003center cluster name\u003e.\u003center region\u003e.kusto.windows.net\", \"aad.auth.appid\": \"\u003center app ID from service principal info\u003e\", \"aad.auth.appkey\": \"\u003center password from service principal info\u003e\", \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\", \"transforms\": \"unwrap\", \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\" } } Notice that Kafka Connect Single Message Transformation (SMT) have been used here - this is the ExtractNewRecordState transformation that Debezium provides. You can read up on it in the documentation\n\"transforms\": \"unwrap\", \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\" It removes the schema and other parts from the JSON payload and keeps it down to only what’s required. In this case, all we are looking for the order info from the after attribute (in the payload). For e.g.\n{ \"orderid\": 51, \"custid\": 306, \"amount\": 183, \"city\": \"Austin\", \"purchase_time\":\"2020-10-09 07:23:10\" } You could model this differently of course (apply transformation in the source connector itself), but there are a couple of benefits to this approach:\nOnly the relevant data sent to Azure Data Explorer The Kafka topic contains the entire change data event (along with the schema) which can be leveraged by any downstream service To install the connector, just use the Kafka Connect REST endpoint like before:\ncurl -X POST -H \"Content-Type: application/json\" --data @adx-sink-config.json http://localhost:8080/connectors # check status curl http://localhost:8080/connectors/adx-orders-sink/status Notice that port for the REST endpoint is 8080 - this is per service port mapping defined in docker-compose.yaml\nThe connector should spin into action, authenticate to Azure Data Explorer and start batching ingestion processes.\nNote that flush.size.bytes and flush.interval.ms are used to regulate the batching process. Please refer to the connector documentation for details on the individual properties.\nSince the flush configuration for the connector and the batching policy for the Orders table in Azure Data Explorer is pretty aggressive (for demonstration purposes), you should see data flowing into Data Explorer quickly.","clean-up#Clean up":"To stop the containers, you can:\ndocker-compose -p adx-kafka-cdc down -v To delete the Azure Data Explorer cluster/database, use az cluster delete or az kusto database delete. For PostgreSQL, simply use az postgres server delete\naz postgres server delete -g \u003cresource group name\u003e -n \u003cserver name\u003e az kusto cluster delete -n \u003ccluster name\u003e -g \u003cresource group name\u003e az kusto database delete -n \u003cdatabase name\u003e --cluster-name \u003ccluster name\u003e -g \u003cresource group name\u003e ","conclusion#Conclusion":"Kafka Connect helps you build scalable data pipelines without having to write custom plumbing code. You mostly need to setup, configure and of course operator the connectors. Remember that Kafka Connect worker instances are just JVM processes and depending on your scale and requirements you can use choose to operate them using Azure Kubernetes Service. Since Kafka Connect instances are stateless entities, you’ve a lot of freedom in terms of the topology and sizing of your cluster workloads!","debezium-postgresql-source-connector-setup#Debezium PostgreSQL source connector setup":"Copy the JSON contents below to a file (you can name it pg-source-config.json). Please ensure that you update the following attributes with the values corresponding to your PostgreSQL instance: database.hostname, database.user, database.password.\n{ \"name\": \"pg-orders-source\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"database.hostname\": \"\u003center database name\u003e.postgres.database.azure.com\", \"database.port\": \"5432\", \"database.user\": \"\u003center admin username\u003e@\u003center database name\u003e\", \"database.password\": \"\u003center admin password\u003e\", \"database.dbname\": \"postgres\", \"database.server.name\": \"myserver\", \"plugin.name\": \"wal2json\", \"table.whitelist\": \"retail.orders_info\", \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\" } } At the time of writing, Debezium supports the following plugins: decoderbufs, wal2json, wal2json_rds, wal2json_streaming, wal2json_rds_streaming and pgoutput. I have used wal2json in this example, and it’s supported on Azure as well.\nTo start the connector, simply use the Kafka Connect REST endpoint to submit the configuration.\ncurl -X POST -H \"Content-Type: application/json\" --data @pg-source-config.json http://localhost:9090/connectors # to confirm curl http://localhost:9090/connectors/pg-orders-source Notice that port for the REST endpoint is 9090 - this is per service port mapping defined in docker-compose.yaml\nLet’s peek into the Kafka topic and take a look at the change data capture events produced by the source connector.\ndocker exec -it adx-kafka-cdc_kafka_1 bash You will be dropped into a shell (inside the container). Execute the below command to consume the change data events from Kafka:\ncd bin \u0026\u0026 ./kafka-console-consumer.sh --topic myserver.retail.orders_info --bootstrap-server kafka:9092 --from-beginning Note that the topic name myserver.retail.orders_info is as a result of the convention used by the Debezium connector\nEach event in topic is corresponding to a specific order. It is in a JSON format that looks like what’s depicted below. Please note that the payload also contains the entire schema which has been removed for brevity.\n{ \"schema\": {....}, \"payload\": { \"before\": null, \"after\": { \"orderid\": 51, \"custid\": 306, \"amount\": 183, \"city\": \"Austin\", \"purchase_time\":\"2020-10-09 07:23:10\" }, \"source\": { \"version\": \"1.2.1.Final\", \"connector\": \"postgresql\", \"name\": \"myserver\", \"ts_ms\": 1602057392691, \"snapshot\": \"false\", \"db\": \"postgres\", \"schema\": \"retail\", \"table\": \"orders_info\", \"txId\": 653, \"lsn\": 34220200, \"xmin\": null }, \"op\": \"c\", \"ts_ms\": 1602057392818, \"transaction\": null } } So far, we have the first half of our pipeline. Let’s work on the second part!","introduction#Introduction":"Using Apache Kafka, it is possible to convert traditional batched ETL processes into real-time, streaming mode. You can do-it-yourself (DIY) and write good old Kafka producer/consumer using a client SDK of your choice. But why would you do that when you’ve Kafka Connect and it’s suite of ready-to-use connectors?\nOnce you opt for Kafka Connect, you have a couple of options. One is the JDBC connector which basically polls the target database table(s) to get the information. There is a better (albeit, a little more complex) way based on change data capture. Enter Debezium, which is a distributed platform that builds on top of Change Data Capture features available in different databases. It provides a set of Kafka Connect connectors which tap into row-level changes in database table(s) and convert them into event streams that are sent to Apache Kafka. Once the change log events are in Kafka, they will be available to all the downstream applications.\nHere is a high-level overview of the use-case presented in this post. It has been kept simplified for demonstration purposes.","overview#Overview":"Data related to Orders is stored in the PostgreSQL database and contains information such as order ID, customer ID, city, transaction amount. time etc. This data is picked up the Debezium connector for PostgreSQL and sent to a Kafka topic. Once the data is in Kafka, another (sink) connector sends them to Azure Data Explorer allow or further querying and analysis.\nThe individual components used in the end to end solution are as follows:\nSource and Destination Data pipelines can be pretty complex! This blog post provides a simplified example where a PostgreSQL database will be used as the source of data and a Big Data analytics engine acts as the final destination (sink). Both these components run in Azure: Azure Database for PostgreSQL (the Source) is a relational database service based on the open-source Postgres database engine and Azure Data Explorer (the Sink) is a fast and scalable data exploration service that lets you collect, store, and analyze large volumes of data from any diverse sources, such as websites, applications, IoT devices, and more.\nAlthough Azure PostgreSQL DB has been used in this blog, the instructions should work for any Postgres database. So feel free to use alternate options if you’d like!\nThe code and configuration associated with this blog post is available in this GitHub repository\nKafka and Kafka Connect Apache Kafka along with Kafka Connect acts as a scalable platform for streaming data pipeline - the key components here are the source and sink connectors.\nThe Debezium connector for PostgreSQL captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database, generates data change event records and streams them to Kafka topics. Behind the scenes, it uses a combination of a Postgres output plugin (e.g. wal2json, pgoutput etc.) and the (Java) connector itself reads the changes produced by the output plug-in using the PostgreSQL’s streaming replication protocol and the JDBC driver.\nThe Azure Data Explorer sink connector picks up data from the configured Kafka topic, batches and sends them to Azure Data Explorer where they are queued up ingestion and eventually written to a table in Azure Data Explorer. The connector leverages the Java SDK for Azure Data Explorer.\nMost of the components (except Azure Data Explorer and Azure PostgreSQL DB) run as Docker containers (using Docker Compose) - Kafka (and Zookeeper), Kafka Connect workers and the data generator application. Having said that, the instructions would work with any Kafka cluster and Kafka Connect workers, provided all the components are configured to access and communicate with each other as required. For example, you could have a Kafka cluster on Azure HD Insight or Confluent Cloud on Azure Marketplace.\nCheck out these hands-on labs if you’re interested in these scenarios\nHere is a breakdown of the components and their service definitions - you can refer to the complete docker-compose file in the GitHub repo\nDocker Compose services zookeeper: image: debezium/zookeeper:1.2 ports: - 2181:2181 kafka: image: debezium/kafka:1.2 ports: - 9092:9092 links: - zookeeper depends_on: - zookeeper environment: - ZOOKEEPER_CONNECT=zookeeper:2181 - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 The Kafka and Zookeeper run using the debezium images - they just work and are great for iterative development with quick feedback loop, demos etc.\ndataexplorer-connector: build: context: ./connector args: KUSTO_KAFKA_SINK_VERSION: 1.0.1 ports: - 8080:8083 links: - kafka depends_on: - kafka environment: - BOOTSTRAP_SERVERS=kafka:9092 - GROUP_ID=adx - CONFIG_STORAGE_TOPIC=adx_connect_configs - OFFSET_STORAGE_TOPIC=adx_connect_offsets - STATUS_STORAGE_TOPIC=adx_connect_statuses postgres-connector: image: debezium/connect:1.2 ports: - 9090:8083 links: - kafka depends_on: - kafka environment: - BOOTSTRAP_SERVERS=kafka:9092 - GROUP_ID=pg - CONFIG_STORAGE_TOPIC=pg_connect_configs - OFFSET_STORAGE_TOPIC=pg_connect_offsets - STATUS_STORAGE_TOPIC=pg_connect_statuses The Kafka Connect source and sink connectors run as separate containers, just to make it easier for you to understand and reason about them - it is possible to run both the connectors in a single container as well.\nNotice that, while the PostgreSQL connector is built into debezium/connect image, the Azure Data Explorer connector is setup using custom image. The Dockerfile is quite compact:\nFROM debezium/connect:1.2 WORKDIR $KAFKA_HOME/connect ARG KUSTO_KAFKA_SINK_VERSION RUN curl -L -O https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v$KUSTO_KAFKA_SINK_VERSION/kafka-sink-azure-kusto-$KUSTO_KAFKA_SINK_VERSION-jar-with-dependencies.jar Finally, the orders-gen service just Go application to seed random orders data into PostgreSQL. You can refer to the Dockerfile in the GitHub repo\norders-gen: build: context: ./orders-generator environment: - PG_HOST=\u003cpostgres host\u003e - PG_USER=\u003cpostgres username\u003e - PG_PASSWORD=\u003cpostgres password\u003e - PG_DB=\u003cpostgres db name\u003e Hopefully, by now you have a reasonable understanding of architecture and the components involved. Before diving into the practical aspects, you need take care of a few things.","pre-requisites#Pre-requisites":" You will need a Microsoft Azure account. Don’t worry, you can get it for free if you don’t have one already! Install Azure CLI Install Docker and Docker Compose Finally, clone this GitHub repo:\ngit clone https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo cd kafka-adx-postgres-cdc-demo To begin with, let’s make sure you have setup and configured Azure Data Explorer and PostgreSQL database.","query-data-explorer#Query Data Explorer":"You can query the Orders table in Data Explorer to slice and dice the data. Here are a few simple queries to start with.\nGet details for orders from New York city;\nOrders | where city == 'New York' Get only the purchase amount and time for orders from New York city sorted by amount\nOrders | where city == 'New York' | project amount, purchase_time | sort by amount Find out the average sales per city and represent that as a column chart:\nOrders | summarize avg_sales = avg(amount) by city | render columnchart The total purchase amount per city, represented as a pie chart:\nOrders | summarize total = sum(amount) by city | sort by total | render piechart Number of orders per city, represented as a line chart:\nOrders | summarize orders = count() by city | sort by orders | render linechart How do purchases vary over a day?\nOrders | extend hour = floor(purchase_time % 1d , 10m) | summarize event_count=count() by hour | sort by hour asc | render timechart How does it vary over a day across different cities?\nOrders | extend hour= floor( purchase_time % 1d , 10m) | where city in (\"New Delhi\", \"Seattle\", \"New York\", \"Austin\", \"Chicago\", \"Cleveland\") | summarize event_count=count() by hour, city | render columnchart Azure Data Explorer Dashboards Learn how to visualize data with Azure Data Explorer dashboards ","setup-and-configure-azure-data-explorer#Setup and configure Azure Data Explorer":" Create an Azure Data Explorer cluster and a database - this quickstart will guide you through the process.\nCreate a table (Orders) and the mapping (OrdersEventMapping) using the KQL queries below:\n.create table Orders (orderid: string, custid: string, city: string, amount: int, purchase_time: datetime) .create table Orders ingestion json mapping 'OrdersEventMapping' '[{\"column\":\"orderid\",\"Properties\":{\"path\":\"$.orderid\"}},{\"column\":\"custid\",\"Properties\":{\"path\":\"$.custid\"}},{\"column\":\"city\",\"Properties\":{\"path\":\"$.city\"}},{\"column\":\"amount\",\"Properties\":{\"path\":\"$.amount\"}},{\"column\":\"purchase_time\",\"Properties\":{\"path\":\"$.purchase_time\"}}]' During the ingestion process, Azure Data Explorer attempts to optimize for throughput by batching small ingress data chunks together as they await ingestion - the IngestionBatching policy can be used to fine tune this process. Optionally, for the purposes of this demo, you can update the policy as such:\n.alter table Orders policy ingestionbatching @'{\"MaximumBatchingTimeSpan\":\"00:00:30\", \"MaximumNumberOfItems\": 500, \"MaximumRawDataSizeMB\": 1024}' .show table \u003center database name\u003e.Orders policy ingestionbatching Refer to the IngestionBatching policy command reference for details\nCreate a Service Principal in order for the connector to authenticate and connect to Azure Data Explorer service. If you want to use the Azure Portal to do this, please refer to How to: Use the portal to create an Azure AD application and service principal that can access resources. The below example makes use of Azure CLI az ad sp create-for-rbac command. For example, to create a service principal with the name adx-sp: az ad sp create-for-rbac -n \"adx-sp\" You will get a JSON response:\n{ \"appId\": \"fe7280c7-5705-4789-b17f-71a472340429\", \"displayName\": \"kusto-sp\", \"name\": \"http://kusto-sp\", \"password\": \"29c719dd-f2b3-46de-b71c-4004fb6116ee\", \"tenant\": \"42f988bf-86f1-42af-91ab-2d7cd011db42\" } Please note down the appId, password and tenant as you will be using them in subsequent steps\nAdd permissions to your database Provide appropriate role to the Service principal you just created. To assign the admin role, follow this guide to use the Azure portal or use the following command in your Data Explorer cluster\n.add database \u003center database name\u003e admins ('aadapp=\u003center service principal appId\u003e;\u003center service principal tenant\u003e') 'AAD App' ","setup-and-configure-azure-postgresql-db#Setup and configure Azure PostgreSQL DB":"You can setup PostgreSQL on Azure using a variety of options including, the Azure Portal, Azure CLI, Azure PowerShell, ARM template. Once you’ve done that, you can easily connect to the database using you favourite programming language such as Java, .NET, Node.js, Python, Go etc.\nAlthough the above references are for Single Server deployment mode, please note that Hyperscale (Citus) is another deployment mode you can use for “workloads that are approaching – or already exceed – 100 GB of data.”\nPlease ensure that you keep the following PostgreSQL related information handy since you will need them to configure the Debezium Connector in the subsequent sections - database hostname (and port), username, password\nFor the end-to-end solution to work as expected, we need to:\nEnsure that the PostgreSQL instance in Azure is accessible from the local Kafka Connect workers (containers) Ensure appropriate PostrgeSQL replication setting (“Logical”) Create the Orders table which you will use to try out the change data capture feature If you’re using Azure DB for PostgreSQL, create a firewall rule using az postgres server firewall-rule create command to whitelist your host. Since we’re running Kafka Connect in Docker locally, simply navigate to the Azure portal (Connection security section of my PostrgreSQL instance) and choose Add current client IP address to make sure that your local IP is added to the firewall rule as such:\nTo change the replication mode for Azure DB for PostgreSQL, you can use the az postgres server configuration command:\naz postgres server configuration set --resource-group \u003cname of resource group\u003e --server-name \u003cname of server\u003e --name azure.replication_support --value logical .. or use the Replication menu of your PostgreSQL instance in the Azure Portal:\nAfter updating the configuration, you will need to re-start the server which you can do using the CLI (az postgres server restart) or the portal.\nOnce the database is up and running, create the table. I have used psql CLI in this example, but feel free to use any other tool. For example, to connect to your PostgreSQL database on Azure over SSL (you will be prompted for the password):\npsql -h \u003cPOSTGRESQL_INSTANCE_NAME\u003e.postgres.database.azure.com -p 5432 -U \u003cPOSTGRES_USER_NAME\u003e -W -d \u003cPOSTGRES_DB_NAME\u003e --set=sslmode=require //example psql -h my-pgsql.postgres.database.azure.com -p 5432 -U foo@my-pgsql -W -d postgres --set=sslmode=require Use the below SQL to create the table:\nCREATE SCHEMA retail; CREATE TABLE retail.orders_info ( orderid SERIAL NOT NULL PRIMARY KEY, custid INTEGER NOT NULL, amount INTEGER NOT NULL, city VARCHAR(255) NOT NULL, purchase_time VARCHAR(20) NOT NULL ); The purchase_time captures the time when the purchase was executed, but it uses VARCHAR instead of a TIMESTAMP type (ideally) to reduce the overall complexity. This is because of the way Debezium Postgres connector treats TIMESTAMP data type (and rightly so!)\nOver the course of the next few sections, you will setup the source (PostgreSQL), sink (Azure Data Explorer) connectors and validate the end to end pipeline.","start-docker-containers#Start Docker containers":"Starting up our local environment is very easy, thanks to Docker Compose - all we need is a single command:\ndocker-compose --project-name adx-kafka-cdc up --build This will build (and start) the order generator application container along with Kafka, Zookeeper and Kafka Connect workers.\nIt might take a while to download and start the containers: this is just a one time process.\nTo confirm whether all the containers have started:\ndocker-compose -p adx-kafka-cdc ps //output Name Command State Ports -------------------------------------------------------------------------------------------------------------------------- adx-kafka-cdc_dataexplorer-connector_1 /docker-entrypoint.sh start Up 0.0.0.0:8080-\u003e8083/tcp, 8778/tcp, 9092/tcp, 9779/tcp adx-kafka-cdc_kafka_1 /docker-entrypoint.sh start Up 8778/tcp, 0.0.0.0:9092-\u003e9092/tcp, 9779/tcp adx-kafka-cdc_orders-gen_1 /orders-gen Up adx-kafka-cdc_postgres-connector_1 /docker-entrypoint.sh start Up 0.0.0.0:9090-\u003e8083/tcp, 8778/tcp, 9092/tcp, 9779/tcp adx-kafka-cdc_zookeeper_1 /docker-entrypoint.sh start Up 0.0.0.0:2181-\u003e2181/tcp, 2888/tcp, 3888/tcp, 8778/tcp, 9779/tcp The orders generator app will start inserting random order events to the orders_info table in PostgreSQL. At this point you can also do quick sanity check to confirm that the order information is being persisted - I have used psql in the example below:\npsql -h \u003cPOSTGRESQL_INSTANCE_NAME\u003e.postgres.database.azure.com -p 5432 -U \u003cPOSTGRES_USER_NAME\u003e -W -d \u003cPOSTGRES_DB_NAME\u003e --set=sslmode=require select * from retail.orders_info order by orderid desc limit 5; This will give you the five most recent orders:\norderid | custid | amount | city | purchase_time ---------+--------+--------+-----------+--------------------- 10 | 77 | 140 | Seattle | 2020-10-09 07:10:49 9 | 541 | 186 | Cleveland | 2020-10-09 07:10:46 8 | 533 | 116 | Cleveland | 2020-10-09 07:10:42 7 | 225 | 147 | Chicago | 2020-10-09 07:10:39 6 | 819 | 184 | Austin | 2020-10-09 07:10:36 (5 rows) To stream the orders data to Kafka, we need to configure and start an instance of the Debezium PostgreSQL source connector."},"title":"Change Data Capture from PostgreSQL to Azure Data Explorer using Kafka Connect"},"/blog/postgres-debezium-pgoutput/":{"data":{"":"","#":"Set up a Change Data Capture architecture on Azure using Debezium, Postgres and Kafka was a tutorial on how to use Debezium for change data capture from Azure PostgreSQL and send them to Azure Event Hubs for Kafka - it used the wal2json output plugin.\nWhat about the pgoutput plugin? This blog will provide a quick walk through of how to pgoutput plugin and provide clarification on this point raised by Denis Arnaud (thank you for brining it up!)\nI will not be repeating a lot of details and use containerized versions (using Docker Compose) of Kafka connect, Kafka (and Zookeeper) to keep things simple. So, the only thing you need is Azure PostgreSQL, which you can setup using a variety of options including, the Azure Portal, Azure CLI, Azure PowerShell, ARM template.\nThe resources are available on GitHub - https://github.com/abhirockzz/debezium-postgres-pgoutput\nUsing the right publication.autocreate.mode With the pgoutput plugin, it’s important that you use the appropriate value for publication.autocreate.mode. If you’re using all_tables (which is the default), you need to ensure that the publication is created up-front for the specific table(s) you want to configure for change data capture. If the publication is not found, the connector will try to create one using CREATE PUBLICATION \u003cpublication_name\u003e FOR ALL TABLES; which will fail due to lack of permissions.\nThe other two options work as expected:\ndisabled: you need to ensure that the publication is created up-front. The connector will not attempt to create the publication if it isn’t found to exist upon startup - it will throw an exception and stop. filtered: you can (optionally) choose to create the publication up-front. If the publication is not found, the connector will create a new publication for all those tables matching the current filter configuration. This has been highlighted in the docs https://debezium.io/documentation/reference/1.3/connectors/postgresql.html#postgresql-on-azure\nLet’s try the different scenarios.. Before that:\ngit clone https://github.com/abhirockzz/debezium-postgres-pgoutput \u0026\u0026 cd debezium-postgres-pgoutput Start Kafka, Zookeeper and Kafka Connect containers:\nexport DEBEZIUM_VERSION=1.2 docker-compose up It might take a while to pull the containers for the first time\nOnce all the containers are up and running, connect to Azure PostgreSQL, create a table and insert some data:\npsql -h \u003cDBNAME\u003e.postgres.database.azure.com -p 5432 -U \u003cDBUSER\u003e@\u003cDBNAME\u003e -W -d postgres --set=sslmode=require psql -h abhishgu-pg.postgres.database.azure.com -p 5432 -U abhishgu@abhishgu-pg -W -d postgres --set=sslmode=require CREATE TABLE inventory (id SERIAL, item VARCHAR(30), qty INT, PRIMARY KEY(id)); When publication.autocreate.mode is set to filtered\nThis works well with Azure PostgreSQL - it does not require super user permissions because the connector creates the publication for a specific table(s) based on the filter/*list values\nUpdate the connector config file (pg-source-connector.json) with details of your Azure PostgreSQL instance and then create the connector\nTo create the connector:\ncurl -X POST -H \"Content-Type: application/json\" --data @pg-source-connector.json http://localhost:8083/connectors Notice the logs (in the docker compose terminal):\nCreating new publication 'mytestpub' for plugin 'PGOUTPUT' [io.debezium.connector.postgresql.connection.PostgresReplicationConnection] Once the connector starts, check the publications in PostgreSQL:\npubname | schemaname | tablename -----------+------------+----------- mytestpub | public | inventory Does it work?\nInsert a couple of records in the inventory table\npsql -h \u003cDBNAME\u003e.postgres.database.azure.com -p 5432 -U \u003cDBUSER\u003e@\u003cDBNAME\u003e -W -d postgres --set=sslmode=require INSERT INTO inventory (item, qty) VALUES ('apples', '100'); INSERT INTO inventory (item, qty) VALUES ('oranges', '42'); select * from inventory; The connector should push the change events from PostgreSQL WAL (write ahead log) to Kafka. Check the messages in the corresponding Kafka topic:\n//exec into the kafka docker container docker exec -it debezium-postgres-pgoutput_kafka_1 bash cd bin \u0026\u0026 ./kafka-console-consumer.sh --topic myserver.public.inventory --bootstrap-server kafka:9092 --from-beginning You should see a couple of change log event payloads (corresponding to the two INSERTs)\nyes they are verbose since the schema is included in the payload\nChange publication.autocreate.mode to disabled\nFor this mode, we need a publication created up-front. Since we already have one (mytestpub), just use it. All you need to do is update the publication.autocreate.mode in pg-source-connector.json to disabled.\nRe-create the connector:\n//delete curl -X DELETE localhost:8083/connectors/inventory-connector //create curl -X POST -H \"Content-Type: application/json\" --data @pg-source-connector.json http://localhost:8083/connectors Test it end to end using the same steps as in the previous section - everything should work just fine!\nJust to confirm, update the publication.name in connector config to one that does not exist. The connector will fail to start due to missing publication (as expected)\nTry publication.autocreate.mode = all_tables\nSet publication.autocreate.mode to all_tables, publication.name to one that does not exist (e.g. testpub1) and create the connector:\ncurl -X POST -H \"Content-Type: application/json\" --data @pg-source-connector.json http://localhost:8083/connectors (as expected) It will fail with an error similar to this:\n.... INFO Creating new publication 'testpub1' for plugin 'PGOUTPUT' (io.debezium.connector.postgresql.connection.PostgresReplicationConnection:127) ERROR WorkerSourceTask{id=inventory-connector-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:179) io.debezium.jdbc.JdbcConnectionException: ERROR: must be superuser to create FOR ALL TABLES publication .... Notice the part must be superuser to create FOR ALL TABLES publication - as previously mentioned, CREATE PUBLICATION \u003cpublication_name\u003e FOR ALL TABLES; failed due to lack of superuser permissions.\nAs I mentioned earlier, you need to work around this by creating the publication manually for specific tables only\nClean up To clean up, delete the Azure PostgreSQL instance using az postgres server delete and remove the containers\naz postgres server delete -g \u003cresource group\u003e -n \u003cserver name\u003e docker-compose down -v That’s it for this short blog post. Stay tuned for more!"},"title":"PostgreSQL pgoutput plugin for change data capture"},"/blog/postgres-kafka-cassandra/":{"data":{"":"","conclusion#Conclusion":"To summarize, you learnt how to use Kafka Connect for real-time data integration between PostgreSQL, Apache Kafka and Azure Cosmos DB. Since the sample adopts a Docker container based approach, you can easily customise this as per your own unique requirements, rinse and repeat!\nThe following topics might also be of interest… If you found this useful, you may also want to explore the following resources:\nMigrate data from Oracle to Azure Cosmos DB Cassandra API using Blitzz Migrate data from Cassandra to Azure Cosmos DB Cassandra API account using Azure Databricks Quickstart: Build a Java app to manage Azure Cosmos DB Cassandra API data (v4 Driver) Apache Cassandra features supported by Azure Cosmos DB Cassandra API Quickstart: Build a Cassandra app with Python SDK and Azure Cosmos DB ","here-is-a-high-level-overview-#Here is a high-level overview \u0026hellip;":"","query-azure-cosmos-db#Query Azure Cosmos DB":"Check the Cassandra tables in Azure Cosmos DB. If you have cqlsh installed locally, you can simply use it as such:\nexport SSL_VERSION=TLSv1_2 \u0026\u0026\\ export SSL_VALIDATE=false \u0026\u0026\\ cqlsh.py \u003ccosmosdb account name\u003e.cassandra.cosmos.azure.com 10350 -u kehsihba-cassandra -p \u003ccosmosdb password\u003e --ssl Here are some of the queries you can try:\nselect count(*) from retail.orders_by_customer; select count(*) from retail.orders_by_city; select * from retail.orders_by_customer; select * from retail.orders_by_city; select * from retail.orders_by_city where city='Seattle'; select * from retail.orders_by_customer where customer_id = 10; ","start-datastax-apache-kafka-connector-instance#Start DataStax Apache Kafka connector instance":"Save the connector configuration (JSON) to a file example, cassandra-sink-config.json and update the properties as per your environment.\n{ \"name\": \"kafka-cosmosdb-sink\", \"config\": { \"connector.class\": \"com.datastax.oss.kafka.sink.CassandraSinkConnector\", \"tasks.max\": \"1\", \"topics\": \"myserver.retail.orders_info\", \"contactPoints\": \"\u003cAzure Cosmos DB account name\u003e.cassandra.cosmos.azure.com\", \"loadBalancing.localDc\": \"\u003cAzure Cosmos DB region e.g. Southeast Asia\u003e\", \"datastax-java-driver.advanced.connection.init-query-timeout\": 5000, \"ssl.hostnameValidation\": true, \"ssl.provider\": \"JDK\", \"ssl.keystore.path\": \"\u003cpath to JDK keystore path e.g. \u003cJAVA_HOME\u003e/jre/lib/security/cacerts\u003e\", \"ssl.keystore.password\": \"\u003ckeystore password: it is 'changeit' by default\u003e\", \"port\": 10350, \"maxConcurrentRequests\": 500, \"maxNumberOfRecordsInBatch\": 32, \"queryExecutionTimeout\": 30, \"connectionPoolLocalSize\": 4, \"auth.username\": \"\u003cAzure Cosmos DB user name (same as account name)\u003e\", \"auth.password\": \"\u003cAzure Cosmos DB password\u003e\", \"topic.myserver.retail.orders_info.retail.orders_by_customer.mapping\": \"order_id=value.orderid, customer_id=value.custid, purchase_amount=value.amount, city=value.city, purchase_time=value.purchase_time\", \"topic.myserver.retail.orders_info.retail.orders_by_city.mapping\": \"order_id=value.orderid, customer_id=value.custid, purchase_amount=value.amount, city=value.city, purchase_time=value.purchase_time\", \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\", \"transforms\": \"unwrap\", \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\", \"offset.flush.interval.ms\": 10000 } } Start the connector:\ncurl -X POST -H \"Content-Type: application/json\" --data @cassandra-sink-config.json http://localhost:8080/connectors If everything has been configured correctly, connector will start pumping data from Kafka topci into Cassandra table(s) and our end to end pipeline will be operational.\nYou’d obviously want to …","start-off-by-creating-cassandra-keyspace-and-tables#Start off by creating Cassandra Keyspace and tables":" Use the same Keyspace and table names as below\nCREATE KEYSPACE retail WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'datacenter1' : 1}; CREATE TABLE retail.orders_by_customer (order_id int, customer_id int, purchase_amount int, city text, purchase_time timestamp, PRIMARY KEY (customer_id, purchase_time)) WITH CLUSTERING ORDER BY (purchase_time DESC) AND cosmosdb_cell_level_timestamp=true AND cosmosdb_cell_level_timestamp_tombstones=true AND cosmosdb_cell_level_timetolive=true; CREATE TABLE retail.orders_by_city (order_id int, customer_id int, purchase_amount int, city text, purchase_time timestamp, PRIMARY KEY (city,order_id)) WITH cosmosdb_cell_level_timestamp=true AND cosmosdb_cell_level_timestamp_tombstones=true AND cosmosdb_cell_level_timetolive=true; ","use-docker-compose-to-start-all-the-services#Use Docker Compose to start all the services":" git clone https://github.com/abhirockzz/postgres-kafka-cassandra cd postgres-kafka-cassandra As promised, use a single command to start all the services for the data pipeline:\ndocker-compose -p postgres-kafka-cassandra up --build It might take a while to download and start the containers: this is just a one time process.\nCheck whether all the containers have started. In a different terminal, run:\ndocker-compose -p postgres-kafka-cassandra ps The data generator application will start pumping data into the orders_info table in PostgreSQL. You can also do quick sanity check to confirm. Connect to your PostgreSQL instance using psql client…\npsql -h localhost -p 5432 -U postgres -W -d postgres when prompted for the password, enter postgres\n… and query the table:\nselect * from retail.orders_info; At this point, all you have is PostgreSQL, Kafka and an application writing random data to PostgreSQL. You need to start the Debezium postgreSQL connector to send the PostgreSQL data to a Kafka topic.\nStart PostgreSQL connector instance Save the connector configuration (JSON) to a file example pg-source-config.json\n{ \"name\": \"pg-orders-source\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"database.hostname\": \"localhost\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"password\", \"database.dbname\": \"postgres\", \"database.server.name\": \"myserver\", \"plugin.name\": \"wal2json\", \"table.include.list\": \"retail.orders_info\", \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\" } } To start the PostgreSQL connector instance:\ncurl -X POST -H \"Content-Type: application/json\" --data @pg-source-config.json http://localhost:9090/connectors To check the change data capture events in the Kafka topic, peek into the Docker container running the Kafka connect worker:\ndocker exec -it postgres-kafka-cassandra_cassandra-connector_1 bash Once you drop into the container shell, just start the usual Kafka console consumer process:\ncd ../bin ./kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic myserver.retail.orders_info --from-beginning Note that the topic name is myserver.retail.orders_info which as per the connector convention\nYou should see the change data events in JSON format.\nSo far so good! The first half of the data pipeline seems to be working as expected. For the second half, we need to…","you-will-need-to#You will need to\u0026hellip;":"Apache Kafka often serves as a central component in the overall data architecture with other systems pumping data into it. But, data in Kafka (topics) is only useful when consumed by other applications or ingested into other systems. Although, it is possible to build a solution using the Kafka Producer/Consumer APIs using a language and client SDK of your choice, there are other options in the Kafka ecosystem.\nOne of them is Kafka Connect, which is a platform to stream data between Apache Kafka and other systems in a scalable and reliable manner. It supports several off the shelf connectors, which means that you don’t need custom code to integrate external systems with Apache Kafka.\nThis article will demonstrate how to use a combination of Kafka connectors to set up a data pipeline to synchronize records from a relational database such as PostgreSQL in real-time to Azure Cosmos DB Cassandra API.\nThe code and config for this application is availablei in this GitHub repo - https://github.com/abhirockzz/postgres-kafka-cassandra\nHere is a high-level overview … … of the end to end flow presented in this article.\nOperations against the data in PostgreSQL table (applies to INSERTs for this example) will be pushed to a Kafka topic as change data events, thanks to the Debezium PostgreSQL connector that is a Kafka Connect source connector - this is achieved using a technique called Change Data Capture (also known as CDC).\nChange Data Capture: a quick primer It is a technique used to track row-level changes in database tables in response to create, update and delete operations. This is a powerful capability, but useful only if there is a way to tap into these event logs and make it available to other services which depend on that information.\nDebezium is an open-source platform that builds on top of Change Data Capture features available in different databases. It provides a set of Kafka Connect connectors which tap into row-level changes (using CDC) in database table(s) and convert them into event streams. These event streams are sent to Apache Kafka. Once the change log events are in Kafka, they will be available to all the downstream applications.\nThis is different compared to the “polling” technique adopted by the Kafka Connect JDBC connector\nThe diagram (from the debezium.io website) summarises it nicely!\nPart two In the second half of the pipeline, the DataStax Apache Kafka connector (Kafka Connect sink connector) synchronizes change data events from Kafka topic to Azure Cosmos DB Cassandra API tables.\nComponents This example provides a reusable setup using Docker Compose. This is quite convenient since it enables you to bootstrap all the components (PostgreSQL, Kafka, Zookeeper, Kafka Connect worker, and the sample data generator application) locally with a single command and allow for a simpler workflow for iterative development, experimentation etc.\nUsing specific features of the DataStax Apache Kafka connector allows us to push data to multiple tables. In this example, the connector will help us persist change data records to two Cassandra tables that can support different query requirements.\nHere is a breakdown of the components and their service definitions - you can refer to the complete docker-compose file in the GitHub repo.\nKafka and Zookeeper use debezium images. The Debezium PostgreSQL Kafka connector is available out of the box in the debezium/connect Docker image! To run as a Docker container, the DataStax Apache Kafka Connector is baked on top the debezium/connect image. This image includes an installation of Kafka and its Kafka Connect libraries, thus making it really convenient to add custom connectors. You can refer to the Dockerfile. The data-generator service seeds randomly generated (JSON) data into the orders_info table in PostgreSQL. You can refer to the code and Dockerfile in the GitHub repo You will need to… Install Docker and Docker Compose. Provision an Azure Cosmos DB Cassandra API account Use cqlsh or hosted shell for validation "},"title":"Learn how to setup data pipeline from PostgreSQL to Cassandra using Kafka Connect"},"/blog/postgres-kafka-debezium-azure-cdc/":{"data":{"":"","change-data-capture-in-action-#Change data capture in action \u0026hellip;":"Change Data Capture (CDC) is a technique used to track row-level changes in database tables in response to create, update and delete operations. Different databases use different techniques to expose these change data events - for example, logical decoding in PostgreSQL, MySQL binary log (binlog) etc. This is a powerful capability, but useful only if there is a way to tap into these event logs and make it available to other services which depend on that information.\nDebezium does just that! It is a distributed platform that builds on top of Change Data Capture features available in different databases. It provides a set of Kafka Connect connectors which tap into row-level changes (using CDC) in database table(s) and convert them into event streams. These event streams are sent to Apache Kafka which is a scalable event streaming platform - a perfect fit! Once the change log events are in Kafka, they will be available to all the downstream applications.\nThis is different compared to the “polling” technique adopted by the Kafka Connect JDBC connector\nThe diagram (from the debezium.io website) summarises it nicely!\nThis blog is a guide to getting started with setting up a change data capture based system on Azure using Debezium, Azure DB for PostgreSQL and Azure Event Hubs (for Kafka). It will use the Debezium PostgreSQL connector to stream database modifications from PostgreSQL to Kafka topics in Azure Event Hubs\nThe related config files are available in the GitHub repo https://github.com/abhirockzz/\nAlthough I have used managed Azure services for demonstration purposes these instructions should work for any other setup as well e.g. a local Kafka cluster and PostgreSQL instance.\nSetup PostgreSQL and Kafka on Azure This section will provide pointers on how to configure Azure Event Hubs and Azure DB for PostgreSQL. All you need is a Microsoft Azure account - go ahead and sign up for a free one!\nAzure DB for PostgreSQL Azure DB for PostgreSQL is a managed, relational database service based on the community version of open-source PostgreSQL database engine, and is available in two deployment modes.\nAt the time of writing, it supports PostgreSQL version 11.6\nYou can setup PostgreSQL on Azure using a variety of options including, the Azure Portal, Azure CLI, Azure PowerShell, ARM template. Once you’ve done that, you can easily connect to the database using you favourite programming language such as Java, .NET, Node.js, Python, Go etc.\nAlthough the above references are for Single Server deployment mode, please note that Hyperscale (Citus) is another deployment mode you can use for “workloads that are approaching – or already exceed – 100 GB of data.”\nPlease ensure that you keep the following PostgreSQL related information handy since you will need them to configure the Debezium Connector in the subsequent sections - database hostname (and port), username, password\nAzure Event Hubs Azure Event Hubs is a fully managed data streaming platform and event ingestion service. It also provides a Kafka endpoint that supports Apache Kafka protocol 1.0 and later and works with existing Kafka client applications and other tools in the Kafka ecosystem including Kafka Connect (demonstrated in this blog).\nYou can use the Azure Portal, Azure CLI, PowerShell or ARM template to create an Azure Event Hubs namespace and other resources. To ensure that the Kafka functionality is enabled, all you need to do is choose the Standard or Dedicated tier (since the Basic tier doesn’t support Kafka on Event Hubs.)\nAfter the setup, please ensure that you keep the Connection String handy since you will need it to configure Kafka Connect. You can do so using the Azure Portal or Azure CLI\nInstall Kafka To run Kafka Connect, I will be using a local Kafka installation just for convenience. Just download Apache Kafka, unzip its contents and you’re good to go!\nDownload Debezium connector and start Kafka Connect To start with, clone this Git repo:\ngit clone https://github.com/abhirockzz/debezium-azure-postgres-cdc cd debezium-azure-postgres-cdc Download Debezium PostgreSQL source connector JARs\n1.2.0 is the latest version at the time of writing\nDEBEZIUM_CONNECTOR_VERSION=1.2.0 curl https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/${DEBEZIUM_CONNECTOR_VERSION}.Final/debezium-connector-postgres-${DEBEZIUM_CONNECTOR_VERSION}.Final-plugin.tar.gz --output debezium-connector-postgres.tar.gz tar -xvzf debezium-connector-postgres.tar.gz You should now see a new folder named debezium-connector-postgres. Copy the connector JAR files to your Kafka installation:\nexport KAFKA_HOME=[path to kafka installation e.g. /Users/foo/work/kafka_2.12-2.3.0] cp debezium-connector-postgres/*.jar $KAFKA_HOME/libs //confirm ls -lrt $KAFKA_HOME/libs | grep debezium ls -lrt $KAFKA_HOME/libs | grep protobuf ls -lrt $KAFKA_HOME/libs | grep postgresql Before starting the Kafka Connect cluster, edit the connect.properties file to include appropriate values for the following attributes: bootstrap.servers, sasl.jaas.config, producer.sasl.jaas.config, consumer.sasl.jaas.config (just replace the placeholders)\nStart Kafka Connect cluster (I am running it in distributed mode):\nexport KAFKA_HOME=[path to kafka installation e.g. /Users/foo/work/kafka_2.12-2.3.0] $KAFKA_HOME/bin/connect-distributed.sh connect.properties Wait for the Kafka Connect instance to start - you should see Kafka Connect internal topics in Azure Event Hubs e.g.\nConfigure PostgreSQL Before installing the connector, we need to:\nEnsure that the PostgreSQL instance is accessible from your Kafka Connect cluster Ensure that the PostrgeSQL replication setting is set to “Logical” Create a table which you can use to try out the change data capture feature If you’re using Azure DB for PostgreSQL, create a firewall rule using az postgres server firewall-rule create command to whitelist your Kafka Connect host. In my case, it was a local Kafka Connect cluster, so I simply navigated to the Azure portal (Connection security section of my PostrgreSQL instance) and chose Add current client IP address to make sure that my local IP was added to the firewall rule as such:\nTo change the replication mode for Azure DB for PostgreSQL, you can use the az postgres server configuration command:\naz postgres server configuration set --resource-group \u003cname of resource group\u003e --server-name \u003cname of server\u003e --name azure.replication_support --value logical .. or use the Replication menu of your PostgreSQL instance in the Azure Portal:\nAfter updating the configuration, you will need to re-start the server which you can do using the CLI (az postgres server restart) or the portal.\nOnce the database is up and running, create the table - I have used psql CLI in this example, but feel free to use any other tool. For example, to connect to your PostgreSQL database on Azure over SSL (you will be prompted for the password):\npsql -h \u003cPOSTGRESQL_INSTANCE_NAME\u003e.postgres.database.azure.com -p 5432 -U \u003cPOSTGRES_USER_NAME\u003e -W -d \u003cPOSTGRES_DB_NAME\u003e --set=sslmode=require //example psql -h my-pgsql.postgres.database.azure.com -p 5432 -U foo@my-pgsql -W -d postgres --set=sslmode=require //to create the table CREATE TABLE todos (id SERIAL, description VARCHAR(30), todo_status VARCHAR(10), PRIMARY KEY(id)); Install Debezium PostgreSQL source connector Update the pg-source-connector.json file with the details for the Azure PostgreSQL instance. Here is an example:\n{ \"name\": \"todo-test-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"database.hostname\": \"\u003cPOSTGRES_INSTANCE_NAME\u003e.postgres.database.azure.com\", \"database.port\": \"5432\", \"database.user\": \"\u003cDB_USER_NAME\u003e\", \"database.password\": \"\u003cPASSWORD\u003e\", \"database.dbname\": \"\u003cDB_NAME e.g. postgres\u003e\", \"database.server.name\": \"\u003cLOGICAL_NAMESPACE e.g. todo-server\u003e\", \"plugin.name\": \"wal2json\", \"table.whitelist\": \"\u003cTABLE_NAMES e.g. public.todos\u003e\" } } Let’s go through the configuration:\nFor detailed info, check Debezium documentation\nconnector.class: name of the connector class (this is a static value) database.hostname and database.port: IP address or hostname for your PostgreSQL instance as well as the port (e.g. 5432) database.user and database.password: username and password for your PostgreSQL instance database.dbname: database name e.g. postgres database.server.name: Logical name that identifies and provides a namespace for the particular PostgreSQL database server/cluster being monitored. table.whitelist: comma-separated list of regex specifying which tables you want to monitor for change data capture plugin.name: name of the logical decoding plug-in e.g. wal2json At the time of writing, Debezium supports the following plugins: decoderbufs, wal2json, wal2json_rds, wal2json_streaming, wal2json_rds_streaming and pgoutput. I have used wal2json in this example, and it’s supported on Azure as well!\nFinally, install the connector!\ncurl -X POST -H \"Content-Type: application/json\" --data @pg-source-connector.json http://localhost:8083/connectors Kafka Connect will now start monitoring the todos table for create, update and delete events\nChange data capture in action … Insert records:\npsql -h \u003cPOSTGRES_INSTANCE_NAME\u003e.postgres.database.azure.com -p 5432 -U \u003cPOSTGRES_USER_NAME\u003e -W -d \u003cPOSTGRES_DB_NAME\u003e --set=sslmode=require INSERT INTO todos (description, todo_status) VALUES ('install postgresql', 'complete'); INSERT INTO todos (description, todo_status) VALUES ('install kafka', 'complete'); INSERT INTO todos (description, todo_status) VALUES ('setup source connector', 'pending'); The connector should now spring into action and send the CDC events to a Event Hubs topic named \u003cserver name in config\u003e.\u003ctable name\u003e e.g. todo-server.public.todos\nLet’s introspect the contents of the topic to make sure everything is working as expected. I am using kafkacat in this example, but you can also create a consumer app using any of these options listed here\nUpdate metadata.broker.list and sasl.password attributes in kafkacat.conf to include Kafka broker details. In a different terminal, use it to read the CDC payloads:\nexport KAFKACAT_CONFIG=kafkacat.conf export BROKER=\u003ckafka broker\u003e e.g. for event hubs - my-eventhubs-namespace.servicebus.windows.net:9093 export TOPIC=\u003cserver config\u003e.\u003ctable name\u003e e.g. todo-server.public.todos kafkacat -b $BROKER -t $TOPIC -o beginning You should see the JSON payloads representing the change data events generated in PostgreSQL in response to the rows you had just added to the todos table. Here is a snippet of the payload:\n{ \"schema\": {...}, \"payload\": { \"before\": null, \"after\": { \"id\": 1, \"description\": \"install postgresql\", \"todo_status\": \"complete\" }, \"source\": { \"version\": \"1.2.0.Final\", \"connector\": \"postgresql\", \"name\": \"fullfillment\", \"ts_ms\": 1593018069944, \"snapshot\": \"last\", \"db\": \"postgres\", \"schema\": \"public\", \"table\": \"todos\", \"txId\": 602, \"lsn\": 184579736, \"xmin\": null }, \"op\": \"c\", \"ts_ms\": 1593018069947, \"transaction\": null } The event consists of the payload along with its schema (omitted for brevity). In payload section, notice how the create operation (\"op\": \"c\") is represented - \"before\": null means that this was a newly INSERTed row, after provides values for the each columns in the row, source provides the PostgreSQL instance metadata from where this event was picked up etc.\nYou can try the same with update or delete operations as well and introspect the CDC events, e.g.\nUPDATE todos SET todo_status = 'complete' WHERE description = 'setup source connector'; ","conclusion#Conclusion":"If you’ve reached this far, thanks for reading (this rather lengthy tutorial)!\nChange Data Capture is a powerful technique which can help “unlock the database” by providing near real-time access to it’s changes. This was a “getting started” guide meant to help you get up and running quickly, experiment with and iterate further. Hope you found it useful!","configure-postgresql#Configure PostgreSQL":"","download-debezium-connector-and-start-kafka-connect#Download Debezium connector and start Kafka Connect":"","install-debezium-postgresql-source-connector#Install Debezium PostgreSQL source connector":"","optional-install-file-sink-connector#(Optional) Install File sink connector":"As bonus, you can quickly test this with a File Sink connector as well. It is available out of the box in the Kafka distribution - all you need to do is install the connector. Just replace the topics and file attribute in file-sink-connector.json file\n{ \"name\": \"cdc-file-sink\", \"config\": { \"connector.class\": \"org.apache.kafka.connect.file.FileStreamSinkConnector\", \"tasks.max\": \"1\", \"topics\": \"\u003cserver name\u003e.\u003ctable name\u003e e.g. todos-server.public.todos\", \"file\": \"\u003center full path to file e.g. /Users/foo/work/pg-cdc.txt\u003e\" } } To create the connector:\ncurl -X POST -H \"Content-Type: application/json\" --data @file-sink-connector.json http://localhost:8083/connectors Play around with the database records and monitor the records in the configured output sink file, e.g.\ntail -f /Users/foo/work/pg-cdc.txt ","setup-postgresql-and-kafka-on-azure#Setup PostgreSQL and Kafka on Azure":""},"title":"Change Data Capture architecture using Debezium, Postgres and Kafka"},"/blog/real-time-search-redisearch-kafka/":{"data":{"":"Redis has a versatile set of data structures ranging from simple Strings all the way to powerful abstractions such as Redis Streams. The native data types can take you a long way, but there are certain use cases that may require a workaround. One example is the requirement to use secondary indexes in Redis in order to go beyond the key-based search/lookup for richer query capabilities. Though you can use Sorted Sets, Lists, and so on to get the job done, you’ll need to factor in some trade-offs.\nEnter RediSearch! Available as a Redis module, RediSearch provides flexible search capabilities, thanks to a first-class secondary indexing engine. It offers powerful features such as full-text Search, auto completion, geographical indexing, and many more.\nTo demonstrate the power of RediSearch, this blog post offers a practical example of how to use RediSearch with Azure Cache for Redis with the help of a Go service built using the RediSearch Go client. It’s designed to give you a set of applications that let you ingest tweets in real-time and query them flexibly using RediSearch.\nSpecifically, you will learn how to:\nWork with RediSearch indexes\nUse different RediSearch data types, such as TEXT, NUMERIC, TAG, and others\nHow to build an application to show RediSearch capabilities\nHow to deploy the service components to Azure with just a few commands\nAnalyze tweet data by querying RediSearch","application-overview#Application overview":"As mentioned, the example service lets you consume tweets in real-time and makes them available for querying via RediSearch.\nIt has two components:\nConsumer/Indexer: Reads from the Twitter Streaming API, creates the index, and continuously adds tweet data (in Redis HASHes) as they arrive.\nSearch service: A REST API that allows you to search tweets using the RediSearch query syntax.\nAt this point, I am going to dive into how to get the solution up and running so that you can see it in action. However, if you’re interested in understanding how the individual components work, please refer to the Code walk through section below, and the GitHub repo for this blog: https://github.com/abhirockzz/redisearch-tweet-analysis.\nPrerequisites\nTo begin with, you will need a Microsoft Azure account: get one for free here!\nThe service components listed above will be deployed to Azure Container Instances using native Docker CLI commands. This capability is enabled by integration between Docker and Azure .\nYou will need Docker Desktop version 2.3.0.5 or later, for Windows, macOS, or install the Docker ACI Integration CLI for Linux. To use Twitter Streaming API, you will also need a Twitter developer account. If you don’t have one already, please follow these instructions.\nStart off by using this quick-start tutorial to set up a Redis Enterprise tier cache on Azure. Once you finish the set up, ensure that you have the the Redis host name and access key handy:\nBoth the components of our service are available as Docker containers: the Tweet indexing service and the Search API service. (If you need to build your own Docker images, please use the respective Dockerfile available on the GitHub repo.)\nYou will now see how convenient it is to deploy these to Azure Container Instances, which allows you to run Docker containers on-demand in a managed, serverless Azure environment.","clean-up#Clean up":" After you finish, don’t forget to stop the services and the respective containers in Azure Container Instances:\nUse the Azure Portal to delete the Azure Redis instance that you had created.","code-walk-through#Code walk through":"This section provides a high-level overview of the code for the individual components. This should make it easier to navigate the source code in the GitHub repo.\nTweets consumer/indexer:\ngo-twitter library has been used to interact with Twitter.\nIt authenticates to the Twitter Streaming API:\nconfig := oauth1.NewConfig(GetEnvOrFail(consumerKeyEnvVar), GetEnvOrFail(consumerSecretKeyEnvVar)) token := oauth1.NewToken(GetEnvOrFail(accessTokenEnvVar), GetEnvOrFail(accessSecretEnvVar)) httpClient := config.Client(oauth1.NoContext, token) client := twitter.NewClient(httpClient) And listens to a stream of tweets in a separate goroutine:\ndemux := twitter.NewSwitchDemux() demux.Tweet = func(tweet *twitter.Tweet) { if !tweet.PossiblySensitive { go index.AddData(tweetToMap(tweet)) time.Sleep(3 * time.Second) } } go func() { for tweet := range stream.Messages { demux.Handle(tweet) } }() Notice the go index.AddData(tweetToMap(tweet))- this is where the indexing component is invoked. It connects to Azure Cache for Redis:\nhost := GetEnvOrFail(redisHost) password := GetEnvOrFail(redisPassword) indexName = GetEnvOrFail(indexNameEnvVar) pool = \u0026redis.Pool{Dial: func() (redis.Conn, error) { return redis.Dial(\"tcp\", host, redis.DialPassword(password), redis.DialUseTLS(true), redis.DialTLSConfig(\u0026tls.Config{MinVersion: tls} } It then drops the index (and the existing documents as well) before re-creating it:\nrsClient := redisearch.NewClientFromPool(pool, indexName) err := rsClient.DropIndex(true) schema := redisearch.NewSchema(redisearch.DefaultOptions). AddField(redisearch.NewTextFieldOptions(\"id\", redisearch.TextFieldOptions{})). AddField(redisearch.NewTextFieldOptions(\"user\", redisearch.TextFieldOptions{})). AddField(redisearch.NewTextFieldOptions(\"text\", redisearch.TextFieldOptions{})). AddField(redisearch.NewTextFieldOptions(\"source\", redisearch.TextFieldOptions{})). //tags are comma-separated by default AddField(redisearch.NewTagFieldOptions(\"hashtags\", redisearch.TagFieldOptions{})). AddField(redisearch.NewTextFieldOptions(\"location\", redisearch.TextFieldOptions{})). AddField(redisearch.NewNumericFieldOptions(\"created\", redisearch.NumericFieldOptions{Sortable: true})). AddField(redisearch.NewGeoFieldOptions(\"coordinates\", redisearch.GeoFieldOptions{})) indexDefinition := redisearch.NewIndexDefinition().AddPrefix(indexDefinitionHashPrefix) err = rsClient.CreateIndexWithIndexDefinition(schema, indexDefinition) The index and its associated documents are dropped to allow you to start with a clean state, which makes it easier to experiment/demo. You can choose to comment out this part if you wish.\nInformation for each tweet is stored in a HASH (named tweet:[tweet ID]) using the HSET operation:\nfunc AddData(tweetData map[string]interface{}) { conn := pool.Get() hashName := fmt.Sprintf(\"tweet:%s\", tweetData[\"id\"]) val := redis.Args{hashName}.AddFlat(tweetData) _, err := conn.Do(\"HSET\", val...) } Tweets search exposes a REST API to query RediSearch. All the options (including query, etc.) are passed in the form of query parameters. For example, http://localhost:8080/search?q=@source:iphone. It extracts the required query parameters:\nqParams, err := url.ParseQuery(req.URL.RawQuery) if err != nil { log.Println(\"invalid query params\") http.Error(rw, err.Error(), http.StatusBadRequest) return } searchQuery := qParams.Get(queryParamQuery) query := redisearch.NewQuery(searchQuery) The q parameter is mandatory. However, you can also use the following parameters for search:\nfields : to specify which attributes you want to return in the result, and,\noffset_limit : if you want to specify the offset from where you want to search and the number of documents that you want to include in the result (by default, offset is 0 and limit is 10 — as per RediSearch Go client).\nFor example:\nhttp://localhost:8080/search?q=@source:Web\u0026fields=user,source\u0026offset_limit=5,100 fields := qParams.Get(queryParamFields) offsetAndLimit := qParams.Get(queryParamOffsetLimit) Finally, the results are iterated over and passed back as JSON (array of documents):\ndocs, total, err := rsClient.Search(query) response := []map[string]interface{}{} for _, doc := range docs { response = append(response, doc.Properties) } rw.Header().Add(responseHeaderSearchHits, strconv.Itoa(total)) err = json.NewEncoder(rw).Encode(response) That’s all for this section!","conclusion#Conclusion":"This end-to-end application demonstrates how to work with indexes, ingest real-time data to create documents (tweet information) which are indexed by RediSearch engine and then use the versatile query syntax to extract insights on those tweets.\nWant to understand what happens behind the scenes when you search for a topic on the Redis Labs documentation? Check out this blog post to learn how Redis Labs site incorporated full-text search with RediSearch! Or, perhaps you’re interested in exploring how to use RediSearch in a serverless application?\nIf you’re still getting started, visit the RediSearch Quick Start page.\nIf you want to learn more about the enterprise capabilities in Azure Cache for Redis, you can check out the following resources:\nOriginally published at https://redislabs.com on March 30, 2021.","deploy-to-azure#Deploy to Azure":"version: \"2\" services: tweets-search: image: abhirockzz/redisearch-tweets-search ports: - 80:80 environment: - REDIS_HOST=\u003cazure redis host name\u003e - REDIS_PASSWORD=\u003cazure redis access key\u003e - REDISEARCH_INDEX_NAME=tweets-index tweets-indexer: image: abhirockzz/redisearch-tweets-consumer environment: - TWITTER_CONSUMER_KEY=\u003ctwitter api consumer key\u003e - TWITTER_CONSUMER_SECRET_KEY=\u003ctwitter api consumer secret\u003e - TWITTER_ACCESS_TOKEN=\u003ctwitter api access token\u003e - TWITTER_ACCESS_SECRET_TOKEN=\u003ctwitter api access secret\u003e - REDIS_HOST=\u003cazure redis host name\u003e - REDIS_PASSWORD=\u003cazure redis access key\u003e - REDISEARCH_INDEX_NAME=tweets-index A docker-compose.yml file defines the individual components ( tweets-search and tweets-indexer). All you need to do is update it to replace the values for your Azure Redis instance as well as your Twitter developer account credentials. Here is the file in its entirety:\nCreate an Azure context:\ndocker login azure docker context create aci aci-context docker context use aci-context Clone the GitHub repo:\ngit clone https://github.com/abhirockzz/redisearch-tweet-analysis cd redisearch-tweet-analysis Deploy both the service components as part of a container group:\ndocker compose up -p azure-redisearch-app Note that Docker Compose commands currently available in an ACI context start with docker compose. That is NOT the same as docker-compose with a hyphen.\nYou will see an output similar to this:\n[+] Running 1/3 ⠿ Group azure-redisearch-app Created 8.3s ⠸ tweets-search Creating 6.3s ⠸ tweets-indexer Creating 6.3s Wait for services to start, you can also check the Azure portal. Once both the services are up and running, you can check their respective logs:\ndocker logs azure-redisearch-app_tweets-indexer docker logs azure-redisearch-app_tweets-search If all goes well, the tweet-consumer service should have kicked off. It will read a stream of tweets and persist them to Redis.","redis-enterprise-tiers-on-azure-cache-for-redis#Redis Enterprise tiers on Azure Cache for Redis":"Redis Enterprise is available as a native service on Azure in the form of two new tiers for Azure Cache for Redis which are operated and supported by Microsoft and Redis Labs. This service gives developers access to a rich set of Redis Enterprise features, including modules like RediSearch. For more information, see these resources:\nAzure Cache for Redis, Enterprise Tiers Are Now Generally Available\nDeepening Our Partnership with Microsoft to Grow Redis Enterprise in the Cloud\nMicrosoft and Redis Labs collaborate to give developers new Azure Cache for Redis capabilities\nRedis Enterprise features on Azure Cache for Redis","the-moment-of-truth#The moment of truth!":"It’s time to query the tweet data. To do so, you can access the REST API in Azure Container Instances with an IP address and a fully qualified domain name (FQDN) (read more in Container Access). To find the IP, run docker ps and check the PORTS section in the output\nYou can now run all sorts of queries! Before diving in, here is a quick idea of the indexed attributes that you can use in your search queries:\nid - this is a the Tweet ID ( TEXT attribute) user - the is the screen name ( TEXT attribute) text - tweet contents ( TEXT attribute) source - tweet source e.g. Twitter for Android, Twitter Web App, Twitter for iPhone ( TEXT attribute) hashtags - hashtags (if any) in the tweet (available in CSV format as a TAG attribute) location - tweet location (if available). this is a user defined location (not the exact location per se) created - timestamp (epoch) of the tweet. this is NUMERIC field and can be used for range queries coordinates - geographic location (longitude, latitude) if made available by the client ( GEO attribute) (Note, I use curl in the examples below)\nSet the base URL for the search service API:\nexport REDISEARCH_API_BASE_URL=\u003cfor example, http://20.197.96.54:80/search\u003e Start simple and query all the documents (using * ):\ncurl -i $REDISEARCH_API_BASE_URL?q=* You will see an output similar to this:\nHTTP/1.1 200 OK Page-Size: 10 Search-Hits: 12 Date: Mon, 25 Jan 2021 13:21:52 GMT Content-Type: text/plain; charset=utf-8 Transfer-Encoding: chunked //JSON array of documents (omitted) Notice the headers Page-Size and Search-Hits: these are custom headers being passed from the application, mainly to demonstrate pagination and limits. In response to our “get me all the documents” query, we found 12 results in Redis, but the JSON body returned 10 entries. This is because of the default behavior of the RediSearch Go API, which you can change using different query parameter, such as:\ncurl -i \"$REDISEARCH_API_BASE_URL?q=*\u0026offset_limit=0,100\" offset_limit=0,100 will return up to 100 documents ( limit ) starting with the first one ( offset = 0). Or, for example, search for tweets sent from an iPhone:\ncurl -i \"$REDISEARCH_API_BASE_URL?q=@source:iphone\" You may not always want all the attributes in the query result. For example, this is how to just get back the user (Twitter screen name) and the tweet text:\ncurl -i \"$REDISEARCH_API_BASE_URL?q=@location:india\u0026fields=user,text\" How about a query on the user name (e.g. starting with jo):\ncurl -i \"$REDISEARCH_API_BASE_URL?q=@user:jo*\" You can also use a combination of attributes in the query:\nbash curl -i $REDISEARCH_API_BASE_URL?q=@location:India @source:android How about we look for tweets with specific hashtags? It is possible to use multiple hashtags (separated by |)?\ncurl -i \"$REDISEARCH_API_BASE_URL?q=@hashtags:\\{potus|cov*\\}\" Want to find out how many tweets with the biden hashtag were created recently? Use a range query:\ncurl -i \"$REDISEARCH_API_BASE_URL?q=@hashtags:{biden} @created:[1611556920000000000 1711556930000000000]\" If you were lucky to grab some coordinates info on the tweets, you can try extracting them and then query on coordinates attribute:\ndocker compose down -p azure-redisearch-app These are just a few examples. Feel free to experiment further and try out other queries. This section in the RediSearch documentation might come in handy!"},"title":"RediSearch in Action"},"/blog/redis-grafana/":{"data":{"":"","#":"Recently, I discovered a nice way of plugging in monitoring for Redis using Grafana, thanks to this great Data Source plugin that works with any Redis database, including Azure Cache for Redis!\nIt’s really easy to setup and try Setup an Azure Cache for Redis instance\nStart Grafana in Docker:\ndocker run -d -p 3000:3000 --name=grafana -e \"GF_INSTALL_PLUGINS=redis-datasource\" grafana/grafana Access Grafana dashboard - browse to http://localhost:3000/\nEnter admin as the username and password\nAdd the Data Source\nChoose Redis\nEnter the host and access key for the Azure Cache for Redis instance.\nMake sure you enable TLS\nAll set! You can now run queries from the Explore section.\nThis is just an example of the INFO command to retrieve basic Server info.\nThat’s not all There is a nice dashboard which comes with the Data Source - https://grafana.com/grafana/dashboards/12776.\nTo import it, just go to the Dashboards section of the Data Source.\nTo see it in action, first navigate to Dashboards and choose Redis\n.. and here it is.\nThis is just the beginning… I am not a Grafana expert, but pretty sure there is lots more to explore. I will try playing around with custom dashboards and other supported Redis Data structures such as Streams etc."},"title":"An easy to use monitoring solution for Redis"},"/blog/redis-on-aws-getting-started-easy-way/":{"data":{"":"","clean-up#Clean up":"Once you’re done, don’t forget to:\nDelete the MemoryDB cluster, and the Cloud9 environment That’s all for this blog. I hope you were able to follow along - setup a MemoryDB cluster and a Cloud9 environment (along with a ready to use IDE and terminal!). After doing some initial connectivity testing using redis-cli in docker, you also ran a test program to experiment a little more.\nAll this with just your browser!\nI hope this was useful and you can re-use this for your particular setup, requirements and programming language. Happy coding!","setup-cloud9-and-memorydb#Setup Cloud9 and MemoryDB":"","youre-all-set#You\u0026rsquo;re all set!":"This quick-start uses AWS Cloud9 IDE to help you get up and running with MemoryDB for Redis, quickly\nWhen I was initially exploring some of the AWS services (such as MemoryDB, MSK, Elasticache for Redis etc.), I usually followed the documentation that involved setting up EC2, SSH-ing into the instance, install/copy stuff (language, runtime, code, client etc.) and then try things out. Most often, the first step is the hardest, and it’s important for developers to have the least amount of friction as possible to “get going”.\nAs I searched for simpler/faster ways, I discovered AWS Cloud9 and it turned out to be super useful. It was quick, predictable and had a bunch of useful tooling readily available. In this blog, I will provide step-by-step instructions on how you can easily and quickly get started (and continue experimenting/building/developing) with Amazon MemoryDB for Redis using Cloud9.\nAmazon MemoryDB for Redis is a durable, in-memory database service that is compatible with Redis, thus empowering you to build applications using the same flexible and friendly Redis data structures, APIs, and commands that they already use today. It is fully integrated with Amazon VPC and the cluster in always launched in a VPC.\nYou don’t need to install anything your local machine to work through this tutorial.\nThe only thing you need is an AWS account (of course!) - so make sure you have one (even a free tier might work)\nSetup Cloud9 and MemoryDB If you’re new to AWS in general (or MemoryDB/any other service), I would advise you to go through the setup manually using the AWS Console (as opposed to using CloudFormation or other tooling). This gives you an overview of the options available and will be helpful when you try to automate the same using AWS CLI, CDK, CloudFormation etc.\nCloud9 environment\nThis is quite simple - the documentation worked as expected.\nGo to the AWS console \u003e Cloud9:\nJust enter the name your environment:\nYou can safely choose the defaults on the second screen:\nYou will have a t2.micro node type (1 GiB RAM + 1 vCPU) with Amazon Linux 2 which will be auto-hibernated after 30 mins (if not used) The instance will be placed in the default VPC (any subnet in any AZ). - A security group will also be created This is good enough for now.\nOn the last page, review your settings, click Create environment and you should be off to the races!\nMemoryDB for Redis\nAgain, the documentation works as expected. there are a few config knobs, but i will advice you to keep it simple:\nSingle node cluster - select db.t4g.small node type (it’s sufficient for now) Place the default vpc - you will be choosing this (along with the subnets) while creating a subnet group (in MemoryDB) Make sure to setup the ACL and credentials (username and password to connect to MemoryDB) as well Be patient, the cluster should be ready in a few mins :)\nSecurity configuration\nYou need to add configuration to allow access from Cloud9 instance to your MemoryDB cluster.\nFirst, copy the the security group ID for your Cloud9 instance:\nThen, open the security group for your MemoryDB cluster:\nAdd an Inbound security rule:\nThe rule says: Allow instance associated with the source security group (Cloud9 in this case) to access TCP port 6379 of instance associated with target security group (MemoryDB in this case)\nYou’re all set! Navigate to your Cloud9 IDE:\nGo to AWS console \u003e Cloud9:\nYour Cloud9 env should open up - you should see a terminal.\nConnect to MemoryDB - the easy way The simplest way is to use redis-cli. You don’t need to install it separately - let’s just use Docker since it’s already pre-installed for us!\nredis-cli is available in the redis container itself, so you can start it up and use it from there. Pull the Redis Docker image from DockerHub - docker pull redis\nAdmin:~/environment $ docker pull redis Using default tag: latest latest: Pulling from library/redis 214ca5fb9032: Pull complete 9eeabf2ad250: Pull complete b8eb79a9f3c4: Pull complete 0ba9bf1b547e: Pull complete 2d2e2b28e876: Pull complete 3e45fcdfb831: Pull complete Digest: sha256:180582894be9a7d5f1201877744b912945a8f9a793a65cd66dc1af5ec3fff0fc Status: Downloaded newer image for redis:latest docker.io/library/redis:latest Run the container:\nAdmin:~/environment $ docker run --rm -it redis /bin/bash root@429f8fabaf09:/data# Now you are inside a terminal (in the container) of a terminal (Cloud9 IDE) ;)\nCopy the cluster endpoint of your MemoryDB cluster and set it as an environment variable\nMake sure you remove the port (:6379) from the cluster endpoint since redis-cli appends that automatically:\nexport MEMORYDB_CLUSTER_ENDPOINT=\u003cmemorydb cluster endpoint without the :6379 part) redis-cli -c --user \u003cmemorydb username\u003e --askpass -h $MEMORYDB_CLUSTER_ENDPOINT --tls --insecure --askpass will prompt you for the password - enter it.\nWohoo! You are now connected to your MemoryDB cluster from inside a Docker container in your Cloud9 instance.\nTime for the customary hello world dance!\nIn the terminal:\nSET hello world SET foo bar You should get an OK response from MemoryDB\nSo far so good! You were able to use standard tooling (redis-cli) to connect with your freshly minted MemoryDB cluster. This is good for sanity/connectivity testing, but you can also so some “lightweight” development and run some programs to execute operations on MemoryDB - this is the next logical step.\nSo let’s do that. The example below shows a Go program, but you could use a language of your choice. After all, most language runtimes (like Java, Python, Node.js, Go etc.) come pre-installed in Cloud9 environment! Check this out https://docs.aws.amazon.com/cloud9/latest/user-guide/language-support.html\nRun a program to connect with MemoryDB The code is on GitHub, so simply clone it and change to the right folder:\ngit clone https://github.com/abhirockzz/memorydb-cloud9-quickstart cd memorydb-cloud9-quickstart Set the environment variables and run the program!\nexport MEMORYDB_CLUSTER_ENDPOINT=\u003cmemorydb cluster endpoint (with the port)\u003e export MEMORYDB_USERNAME=\u003cmemorydb username\u003e export MEMORYDB_PASSWORD=\u003cmemorydb password\u003e go run main.go Here is the output when I ran it:\nAdmin:~/environment/memorydb-cloud9-quickstart (master) $ go run main.go go: downloading github.com/go-redis/redis/v8 v8.11.5 go: downloading github.com/gorilla/mux v1.8.0 go: downloading github.com/cespare/xxhash/v2 v2.1.2 go: downloading github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f 2022/05/12 04:53:46 connecting to cluster ****************(redacted) 2022/05/12 04:53:46 successfully connected to cluster 2022/05/12 04:53:46 started HTTP server.... This will start a HTTP server that exposes a few endpoints. Let’s try them out.\nOpen a separate terminal in Cloud9 to run the commands below\nFirst, take a look at the cluster info:\ncurl -i http://localhost:8080/ HTTP/1.1 200 OK Date: Thu, 12 May 2022 04:57:03 GMT Content-Length: 354 Content-Type: text/plain; charset=utf-8 [{\"Start\":0,\"End\":16383,\"Nodes\":[{\"ID\":\"3a0ef99406d4165fab450fde6c0a4eac3ee8f215\",\"Addr\":\"****************.amazonaws.com:6379\"},{\"ID\":\"2b5a4663a9183f7921517c6f14195e9d26a6ca79\",\"Addr\":\"****************.amazonaws.com:6379\"}]}] We got back info about the Shards in our cluster along with individual nodes.\nThe result will be different in your case\nRemember we had executed SET hello world with redis-cli before? Let’s GET that value now:\n# get the value for the key \"hello\" Admin:~/environment $ curl -i localhost:8080/hello HTTP/1.1 200 OK Date: Thu, 12 May 2022 04:54:45 GMT Content-Length: 32 Content-Type: text/plain; charset=utf-8 {\"Key\":\"hello\",\"Value\":\"world\"} Do the same for the key foo:\nAdmin:~/environment $ curl -i localhost:8080/foo HTTP/1.1 200 OK Date: Thu, 12 May 2022 04:55:44 GMT Content-Length: 28 Content-Type: text/plain; charset=utf-8 {\"Key\":\"foo\",\"Value\":\"bar\"} Works as expected - what about a key that does not exist?\nAdmin:~/environment $ curl -i localhost:8080/notthere HTTP/1.1 404 Not Found Date: Thu, 12 May 2022 04:56:23 GMT Content-Length: 0 HTTP 404 - fair enough. Finally, you can set your own key-value:\nAdmin:~/environment $ curl -i -X POST -d 'redis' localhost:8080/awsome HTTP/1.1 200 OK Date: Thu, 12 May 2022 04:59:25 GMT Content-Length: 0 Admin:~/environment $ curl -i localhost:8080/awsome HTTP/1.1 200 OK Date: Thu, 12 May 2022 05:00:51 GMT Content-Length: 33 Content-Type: text/plain; charset=utf-8 {\"Key\":\"awsome\",\"Value\":\"redis\"} Alright, everything works!\nThe Dockerfile for the sample app is also present in the Github repo in case you want to build a docker image and run that instead."},"title":"Getting started with Redis on AWS - the easy way!"},"/blog/redis-streams-in-action-part1/":{"data":{"":"","looking-ahead#Looking ahead\u0026hellip;":"Welcome to this series of blog posts which covers Redis Streams with the help of a practical example. We will use a sample application to make Twitter data available for search and query in real-time. RediSearch and Redis Streams serve as the backbone of this solution that consists of several co-operating components, each of which will we covered in a dedicated blog post.\nThe code is available in this GitHub repo - https://github.com/abhirockzz/redis-streams-in-action\nThis is the first part which explores the use case, motivations and provides a high level overview of the Redis features used in the solution.\nSolution Architecture The use case is relatively simple. As an end goal, we want to have a service that allows us to search for tweets based on some criteria such as hashtags, user, location etc. Of course, there are existing solutions for this. The one presented in this blog series is an example scenario and can be applied to similar problems.\nHere is a summary of the individual components:\nTwitter Stream Consumer: A Rust application to consume streaming Twitter data and pass them on to Redis Streams. I will demonstrate how to run this as a Docker container in Azure Container Instances Tweets Processor: The tweets from Redis Streams are processed by a Java application - this too will be deployed (and scaled) using Azure Container Instances. Monitoring service: The last part is a Go application to monitor the progress of the tweets processor service and ensure that any failed records are re-processed. This is a Serverless component which will be deployed to Azure Functions where you can run it based on a Timer trigger and only pay for the duration it runs for. I have used a few Azure services (including Enterprise tier of Azure Cache for Redis that supports Redis modules such as RediSearch, RedisTimeSeries and Redis Bloom) to run different parts of the solution, but you can tweak the instructions a little bit and apply them as per your environment e.g. you can use use Docker to run everything locally! Although the individual services have been written in different programming languages, the same concepts apply (in terms of Redis Streams, RediSearch, scalability etc.) and can be implemented in the language of your choice.\nThe “Need for scale” I had written a blog post RediSearch in Action that covered the same use case i.e. how to implement a set of applications for consuming tweets in real-time, index them in RediSearch and query them using a REST API. However, the solution presented here has been implemented with the help of Redis Streams along with other components in order to make the architecture scalable and fault-tolerant. In this specific example, it’s the ability to process large volume of tweets, but the same idea can be extended/applied to other use-cases which deal with high velocity data e.g. IoT, log analytics, etc. Such problems benefit from an architecture where you can horizontally scale out your applications to handle increasing data volumes. Typically, this involves introducing a Messaging system to act a buffer between producers and consumers. Since this is a common requirement and the problem space is well understood, there are lot of established solutions in the distributed messaging world ranging from JMS (Java Messaging Service), Apache Kafka, RabbitMQ, NATS, and of course Redis.\nLots of options in Redis! There is something unique about Redis though. From a messaging point of view, Redis is quite flexible since it provides multiple options to support different paradigms, hence serving a wide range of use cases. It’s features include Pub-Sub, Lists (worker queue approach) and Redis Streams. Since this blog series is focuses on Redis Streams, I will provide a quick over view of the other possibilities before moving on.\nPub-Sub: it follows a based broadcast paradigm where multiple receivers can consume messages sent to a specific channel. Producers and consumers are completely decoupled, but note that there is no concept of message persistence i.e. if a consumer app is not up and running, it does not get those messages when it comes back on later. Lists: they allow us to adopt a worker-queue based approach which can distribute load among worker apps. the messages are removed once they are consumed. it can provide some level of fault-tolerance and reliability using RPOPLPUSH (and BRPOPLPUSH) Redis Streams Introduced in Redis 5.0, Redis Streams provides the best of Pub/Sub and Lists along with reliable messaging, durability for messages replay, Consumer Groups for load balancing, Pending Entry List for monitoring and much more! What makes it different is that fact it is a append-only log data structure. In a nutshell, producers can add records (using XADD), consumers can subscribe to new items arriving to the stream (with XREAD). It supports range queries (XRANGE etc.) and thanks to consumer groups, a group of apps can distribute the processing load (XREADGROUP) and its possible to monitor its state (XPENDING etc).\nSince the magic of Redis lies in its powerful command system, let’s go over some of the Redis Streams commands, grouped by functionality for easier understanding:\nAdd entries\nThere is only one way you can add messages to a Redis Stream. XADD appends the specified stream entry to the stream at the specified key. If the key does not exist, as a side effect of running this command the key is created with a stream value.\nRead entries\nXRANGE returns the stream entries matching a given range of IDs (the - and + special IDs mean respectively the minimum ID possible and the maximum ID possible inside a stream) XREVRANGE is exactly like XRANGE, but with the difference of returning the entries in reverse order (use the end ID first and the start ID later) XREAD reads data from one or multiple streams, only returning entries with an ID greater than the last received ID reported by the caller. XREADGROUP is a special version of the XREAD command with support for consumer groups. You can create groups of clients that consume different parts of the messages arriving in a given stream Manage Redis Streams\nXACK removes one or multiple messages from the Pending Entries List (PEL) of a stream consumer group. XGROUP is used to manage the consumer groups associated with a Redis stream. XPENDING is the used to inspect the list of pending messages to observe and understand what is happening with a streams consumer groups. XCLAIM is used to acquire the ownership of the message and continue processing. XAUTOCLIAM transfers ownership of pending stream entries that match the specified criteria. Conceptually, XAUTOCLAIM is equivalent to calling XPENDING and then XCLAIM Delete\nXDEL removes the specified entries from a stream, and returns the number of entries deleted, that may be different from the number of IDs passed to the command in case certain IDs do not exist. XTRIM trims the stream by evicting older entries (entries with lower IDs) if needed. For a detailed, I would highly recommend reading “Introduction to Redis Streams” (from the official Redis docs).\nWhat about RediSearch? Redis has a versatile set of data structures ranging from simple Strings all the way to powerful abstractions such as Redis Streams. The native data types can take you a long way, but there are certain use cases that may require a workaround. One example is the requirement to use secondary indexes in Redis in order to go beyond the key-based search/lookup for richer query capabilities. Though you can use Sorted Sets, Lists, and so on to get the job done, you’ll need to factor in some trade-offs.\nAvailable as a Redis module, RediSearch provides flexible search capabilities, thanks to a first-class secondary indexing engine. Some of its key features include full-text search, auto completion, and geographical indexing. There are a bunch of other features whose detailed exploration is out of scope of this blog series. I would highly recommend you to go through the documentation to explore further. For now, here is a quick overview of some of the RediSearch commands. You will see them in action in subsequent blog posts.\nTwo of the most important commands include creating an index and executing search queries:\nFT.CREATE is used to create an index with a given schema and associated details. FT.SEARCH searches the index with a textual query, returning either documents or just ids. You can execute other operations on indices:\nFT.DROPINDEX deletes the index. Note that by default, it does not delete the document hashes associated with the index FT.INFO returns information and statistics on the index such as number of documents, number of distinct terms and more. FT.ALTER SCHEMA ADD adds a new field to the index. This causes future document updates to use the new field when indexing and re-indexing of existing documents. To work with auto-complete features, you can use “suggestions”:\nFT.SUGADD adds a suggestion string to an auto-complete suggestion dictionary. FT.SUGGET gets completion suggestions for a prefix. RediSearch supports synonyms which is a data structure comprised of a set of groups, each of which contains synonym terms. FT.SYNUPDATE can be used to create or update a synonym group with additional terms.\nIf you want query spell check correction (similar to “did you mean” feature), you can use FT.SPELLCHECK which performs spelling correction on a query, returning suggestions for misspelled terms.\nA dictionary is a set of terms. Dictionaries can be used to modify the behavior of RediSearch’s query spelling correction, by including or excluding their contents from potential spelling correction suggestions. You can use FT.DICTADD and FT.DICTDEL to add and delete terms, respectively.\nThat’s it for this part!\nLooking ahead… As I mentioned, this was just an introduction. Over the course of next three blog posts, you will learn about the details of the individual components used to build the solution. You will deploy, run and validate them on Azure as well as walk through the code to get a better understanding to what’s happening “behind the scenes”. Stay tuned!","lots-of-options-in-redis#Lots of options in Redis!":"","redis-streams#Redis Streams":"","solution-architecture#Solution Architecture":"","the-need-for-scale#The \u0026ldquo;Need for scale\u0026rdquo;":"","what-about-redisearch#What about RediSearch?":""},"title":"Redis Streams in Action: Part 1 (Intro and overview)"},"/blog/redis-streams-in-action-part2/":{"data":{"":"","code-walk-through#Code walk through":"","deploy-the-app-to-azure-container-instances#Deploy the app to Azure Container Instances":"","moving-on-to-the-next-one#Moving on to the next one\u0026hellip;":"Welcome to this series of blog posts which covers Redis Streams with the help of a practical example. We will use a sample application to make Twitter data available for search and query in real-time. RediSearch and Redis Streams serve as the backbone of this solution that consists of several co-operating components, each of which will we covered in a dedicated blog post.\nThe code is available in this GitHub repo - https://github.com/abhirockzz/redis-streams-in-action\nIn this part, we look at the service which interacts with the Twitter Streaming API to consume tweets and move them on to the next part in the processing pipeline.\nOur end goal is to be able to process tweets and make them available for search and queries via RediSearch. One could write a “do it all” service to consume tweets and directly store them in RediSearch. But, in order to scale to handle the volume of tweets, we need a service to act as a buffer and decouple our producer (the application we will focus in this blog) and consumer (covered in next blog).\nThis is exactly what our first component facilitates - it consumes streaming Twitter data and forwards it to Redis Streams. We will deploy it to Azure Container Instances, validate it’s functionality and also walk-through how it works along with the code.\nAs you will see in the later parts of this series, this also provides a foundation for scale-out architecture.\nAll in all, this blog post is short and simple! It lays down the ground work for other parts of the solution which will be covered in subsequent posts. Please don’t worry about the fact that the service is written in Rust (in case you don’t know it already). The logic can be easily ported over to your favorite programming language.\nPre-requisites Start by getting a free Azure account if you don’t have one already and install the Azure CLI as well.\nWe will be deploying the tweets consumer application to Azure Container Instances using regular Docker CLI commands. This capability is enabled by integration between Docker and Azure. Just ensure that you have Docker Desktop version 2.3.0.5 or later, for Windows, macOS, or install the Docker ACI Integration CLI for Linux.\nTo use the Twitter Streaming API, you will also need a Twitter developer account. If you don’t have one already, please follow these instructions on how to set it up.\nDeploy the app to Azure Container Instances To start off, setup the Enterprise tier of Azure Cache for Redis, using this quickstart.Once you finish this step, ensure that you save the following information: the Redis host name and Access key\nThe tweets consumer application is available as a Docker container - the easiest way is to simply re-use it. If you wish to build you own image, please use the Dockerfile available in the GitHub repo.\nYou will now see how convenient it is to deploy it to Azure Container Instances, that allows you to run Docker containers on-demand in a managed, serverless Azure environment.\nFirst, create an Azure context to associate Docker with an Azure subscription and resource group so you can create and manage container instances.\ndocker login azure docker context create aci aci-context docker context use aci-context Set the environment variables - make sure to update Redis host and credentials as per your account:\nexport REDIS_HOSTNAME=\u003credis host port e.g. my-redis-host:10000\u003e export IS_TLS=true export REDIS_PASSWORD=\u003credis access key (password)\u003e # don't forget your twitter api credentials export TWITTER_API_KEY=\u003capi key\u003e export TWITTER_API_KEY_SECRET=\u003capi key secret\u003e export TWITTER_ACCESS_TOKEN=\u003caccess token\u003e export TWITTER_ACCESS_TOKEN_SECRET=\u003caccess token secret\u003e Just execute the good old docker run:\ndocker run -d --name redis-streams-producer \\ -e REDIS_HOSTNAME=$REDIS_HOSTNAME \\ -e IS_TLS=$IS_TLS \\ -e REDIS_PASSWORD=$REDIS_PASSWORD \\ -e TWITTER_API_KEY=$TWITTER_API_KEY \\ -e TWITTER_API_KEY_SECRET=$TWITTER_API_KEY_SECRET \\ -e TWITTER_ACCESS_TOKEN=$TWITTER_ACCESS_TOKEN \\ -e TWITTER_ACCESS_TOKEN_SECRET=$TWITTER_ACCESS_TOKEN_SECRET \\ abhirockzz/tweets-redis-streams-producer-rust A container should now be created in Azure and you should see an output similar to this:\n[+] Running 2/2 ⠿ Group redis-streams-producer Created 4.2s ⠿ redis-streams-producer Created 15.8s Validate this using the Azure portal:\nTo check the container logs:\ndocker logs redis-streams-producer So, does it work? Well, it should! To confirm, connect to the Redis instance using redis-cli:\nredis-cli -h \u003credis cache host\u003e -p \u003credis port\u003e -a \u003caccess key\u003e --tls … and run the XRANGE command to introspect Redis Streams:\nXRANGE tweets_stream - + COUNT 5 This will return the first five tweets. You can change the COUNT as per your requirements.\nThe - and + special IDs mean respectively the minimum ID possible and the maximum ID possible inside the stream\nThat’s all you need to confirm that our application is able to consume tweets and add them to Redis Streams. As mentioned before, the rest of the components in our solution will build on top of this foundation.\nYou can either pause the app for now or delete it:\n#to pause docker stop redis-streams-producer #to delete docker rm redis-streams-producer Now that you’ve seen the application in action, let’s quickly walk through “how” things work. If you’re interested in exploring some Rust code, you will find it useful.\nCode walk through You can refer to the code here\nThe app uses the following libraries:\nA Rust library for accessing the Twitter Streaming API (uses tokio) redis-rs, a Rust library for Redis with both high and low-level APIs serde and serde json It starts by connecting to Redis and Twitter:\nfn connect_redis() -\u003e redis::Connection { println!(\"Connecting to Redis\"); let redis_host_name = env::var(\"REDIS_HOSTNAME\").expect(\"missing environment variable REDIS_HOSTNAME\"); let redis_password = env::var(\"REDIS_PASSWORD\").unwrap_or_default(); //if Redis server needs secure connection let uri_scheme = match env::var(\"IS_TLS\") { Ok(_) =\u003e \"rediss\", Err(_) =\u003e \"redis\", }; let redis_conn_url = format!(\"{}://:{}@{}\", uri_scheme, redis_password, redis_host_name); println!(\"redis_conn_url {}\", redis_conn_url); let client = redis::Client::open(redis_conn_url).expect(\"check Redis connection URL\"); client.get_connection().expect(\"failed to connect to Redis\") } Rather than follow a specific set of keywords or a user, we simply connect to the Twitter sample stream, which provides access to about 1% of all Tweets in real-time:\nlet token = twitter_token(); TwitterStream::sample(\u0026token) .try_flatten_stream() .try_for_each(|json| { let msg: model::StreamMessage = serde_json::from_str(\u0026json).expect(\"failed to convert tweet JSON to struct\"); process(msg, c.clone()); future::ok(()) }) .await .expect(\"error connecting to Twitter stream!\"); Bulk of the logic is encapsulated in the process function. Let’s go through that bit by bit. twitter-stream crate returns each tweet in raw JSON form. It is converted into a model::StreamMessage which is a struct that’s modeled as per the data we intend to extract from the raw tweet.\nWe use serde_json to get this done:\nserde_json::from_str(\u0026json).expect(\"json to struct conversion failed\"); It is then passed to the process function along with a redis::Connection.\nlet conn = connect_redis(); let c = Arc::new(Mutex::new(conn)); ... fn process(msg: model::StreamMessage, conn: Arc\u003cMutex\u003credis::Connection\u003e\u003e) { //omitted } But why wrap it within an Arc of Mutex?\nThat’s because we need to pass the redis::Connection to a FnMut closure. It moves the connection, thus we need to use a shared reference, which Arc provides. But Arc is not enough since we are not allowed to mutate the data. Thus, we use to use a Mutex to lock the connection object - the Rust compiler can be confident that only one thread can access it at a time (preserve immutability)\nThe processing part is relatively simple. It’s all about using the xadd_map function to add the tweet to a Redis Stream. It accepts a BTreeMap, which we create from info in model::StreamMessage - the tweet text, twitter user (screen) name, ID, location and hashtags (if any). Ultimately, the goal is to be able to index these in RediSearch and query them flexibly.\nlet mut stream_entry: BTreeMap\u003cString, String\u003e = BTreeMap::new(); stream_entry.insert(\"id\".to_string(), tweet.id.to_string()); stream_entry.insert(\"user\".to_string(), tweet.user.screen_name); stream_entry.insert(\"text\".to_string(), tweet.text); stream_entry.insert(\"location\".to_string(), tweet.user.location); That’s all for this part.\nMoving on to the next one… We’re just getting started! This was the first component in our service that lays the foundation for processing the tweets and making them queryable via RediSearch. In the upcoming blog, we will dive into how to consume and process tweets from Redis Streams using a Java based application. Stay tuned!","pre-requisites#Pre-requisites":"","so-does-it-work#So, does it work?":""},"title":"Redis Streams in Action — Part 2 (Tweets consumer app)"},"/blog/redis-streams-in-action-part3/":{"data":{"":"","deploy-the-app-to-azure-container-instances#Deploy the app to Azure Container Instances":"","how-are-things-looking#How are things looking?":"","interested-in-the-final-part#Interested in the final part?":"So far, we covered high level overview in part 1, the tweets consumer Rust app in part 2 and a Java app to process those tweets from Redis Streams. As promised, the final part of the series will cover an app to monitor the process and re-process abandoned messages in order to keep our overall system robust - this will a Serverless Go application deployed to Azure Functions. Stay tuned!","lets-dig-a-little-deeper#Let\u0026rsquo;s dig a little deeper":"Welcome to this series of blog posts which covers Redis Streams with the help of a practical example. We will use a sample application to make Twitter data available for search and query in real-time. RediSearch and Redis Streams serve as the backbone of this solution that consists of several co-operating components, each of which will we covered in a dedicated blog post.\nThe code is available in this GitHub repo - https://github.com/abhirockzz/redis-streams-in-action\nThis blog post will cover a Java based Tweets processor application whose role is to pick up tweets from Redis Streams and store them (as a HASH) so that they can be queried using RediSearch (the accurate term for this is “indexing documents” in RediSearch). You will deploy the application to Azure, validate it, run a few RediSearch queries to search tweets. Finally, there is a section where we will walk through the code to understand “how things work”.\nPre-requisites Please make sure that you read part 2 of this series and have the Tweets consumer application up and running. This application will read tweets from the Twitter Streaming API and push them to Redis Streams. Our tweets processor app (the one described in this blog) will then take over.\nYou will need an Azure account which you can get for free and the Azure CLI. Like the previous application, this one will also be deployed to Azure Container Instances using regular Docker CLI commands. This capability is enabled by integration between Docker and Azure. Just ensure that you have Docker Desktop version 2.3.0.5 or later, for Windows, macOS, or install the Docker ACI Integration CLI for Linux.\nDeploy the app to Azure Container Instances If you’ve been following along from the previous blog post, you should have setup the Enterprise tier of Azure Cache for Redis, using this quickstart. Once you finish this step, ensure that you save the following information: the Redis host name and Access key\nThe application is available as a Docker container - the easiest way is to simply re-use it. If you wish to build you own image, please use the Dockerfile available on the GitHub repo.\nIf you choose to build your own image, make sure to build the JAR file using Maven (mvn clean install) first\nIt’s really convenient to deploy it to Azure Container Instances, that allows you to run Docker containers on-demand in a managed, serverless Azure environment.\nMake sure you create an Azure context to associate Docker with an Azure subscription and resource group so you can create and manage container instances.\ndocker login azure docker context create aci aci-context docker context use aci-context Set the environment variables - make sure to update Redis host and credentials as per your account:\nexport STREAM_NAME=tweets_stream # don't change export STREAM_CONSUMER_GROUP_NAME=redisearch_app_group # don't change export REDIS_HOST=\u003credis host port e.g. my-redis-host\u003e export REDIS_PORT=\u003credis port\u003e export REDIS_PASSWORD=\u003credis access key (password)\u003e export SSL=true .. and then use docker run to deploy the container to Azure:\ndocker run -d --name redis-streams-consumer \\ -e STREAM_NAME=$STREAM_NAME \\ -e STREAM_CONSUMER_GROUP_NAME=$STREAM_CONSUMER_GROUP_NAME \\ -e REDIS_HOST=$REDIS_HOST \\ -e REDIS_PORT=$REDIS_PORT \\ -e REDIS_PASSWORD=$REDIS_PASSWORD \\ -e SSL=$SSL \\ abhirockzz/tweets-redis-streams-consumer-java As the container is being created, you should see an output similar to this:\n[+] Running 2/2 ⠿ Group redis-streams-consumer Created 5.2s ⠿ redis-streams-consumer Created 10.5s Validate this using the Azure portal:\nTo check the container logs, you can use the usual docker logs command:\ndocker logs redis-streams-consumer You should see an output similar to this:\nReading from stream tweets_stream with XREADGROUP saved tweet to hash tweet:1393089239324282880 Reading from stream tweets_stream with XREADGROUP saved tweet to hash tweet:1393089243539517441 Reading from stream tweets_stream with XREADGROUP not processed - tweet:1393089247721132033 Reading from stream tweets_stream with XREADGROUP saved tweet to hash tweet:1393089256105693184 Reading from stream tweets_stream with XREADGROUP saved tweet to hash tweet:1393089260304179200 .... Notice the not processed logs? We will discuss them in the next section\nOnce the app is up and running, it will start consuming from tweets_stream Redis Stream and store each tweet info in a separate HASH, which in turn will be indexed by RediSearch. Before moving on, login to the Redis instance using redis-cli:\nredis-cli -h \u003chostname\u003e -p 10000 -a \u003cpassword\u003e --tls How are things looking? If you see the logs carefully, you should be able to find the name of the HASH (which is based on the tweet ID) e.g. tweet:\u003ctweet id\u003e. Just inspect it’s contents with HGETALL:\nredis-cli\u003e TYPE tweet:1393089163856056320 redis-cli\u003e hash redis-cli\u003e HGETALL tweet:1393089163856056320 The result will look like any other HASH. For e.g.\n1) \"location\" 2) \"Nairobi, Kenya\" 3) \"text\" 4) \"RT @WanjaNjubi: #EidMubarak \\xf0\\x9f\\x99\\x8f\\nMay peace be upon you now and always.\\n#EidUlFitr https://t.co/MlL0DbM2aS\" 5) \"id\" 6) \"1393089163856056320\" 7) \"user\" 8) \"Hot_96Kenya\" 9) \"hashtags\" 10) \"EidMubarak,EidUlFitr\" Alright, its time to query tweets with RediSearch! Let’s use a few commands to search the tweets-index index:\nFT.SEARCH tweets-index hello - will return tweets which FT.SEARCH tweets-index hello|world - its the same as above, just that it’s applicable for “hello” OR “world” Use FT.SEARCH tweets-index \"@location:India\" if you’re interested in tweets from a specific location FT.SEARCH tweets-index \"@user:jo* @location:India\" - this combines location along with a criteria that the username should start with jo FT.SEARCH tweets-index \"@user:jo* | @location:India\" - this is subtle variant of the above. | signifies an OR criteria You can search using hash tags as well - FT.SEARCH tweets-index \"@hashtags:{cov*} Include multiple hash tags as such - FT.SEARCH tweets-index \"@hashtags:{cov*|Med*}\" These are just a few examples. I would highly recommend you to refer to the RediSearch documentation and try other other queries as well.\nLet’s scale out One of the key benefits of using Redis Streams is to leverage its Consumer Groups feature. This means that you can simply add more instances to the application (horizontal scale out) in order to improve the processing - the more number of instances, the faster the tweets gets processed. Each application will consume from a different part of the same Redis Stream (tweets_stream), thus the workload is distributed (almost) evenly amongst all the instances - this gives you the ability to scale linearly.\nLet’s try this out. To start another instance, use docker run - make sure to use a different name:\ndocker run -d --name redis-streams-consumer_2 \\ -e STREAM_NAME=$STREAM_NAME \\ -e STREAM_CONSUMER_GROUP_NAME=$STREAM_CONSUMER_GROUP_NAME \\ -e REDIS_HOST=$REDIS_HOST \\ -e REDIS_PORT=$REDIS_PORT \\ -e REDIS_PASSWORD=$REDIS_PASSWORD \\ -e SSL=$SSL \\ abhirockzz/tweets-redis-streams-consumer-java Notice that I used a different name --name redis-streams-consumer_2\nThings will continue like before - just a little faster since we have another helping hand. You can check the logs the new instance as well - docker logs redis-streams-consumer_2.\nYou can continue to experiment further and try scaling out to more instances.\nLet’s dig a little deeper We can introspect Redis Streams using the XPENDING command:\nXPENDING tweets_stream redisearch_app_group You will an output similar to this:\n1) (integer) 25 2) \"1618572598902-0\" 3) \"1618573768902-0\" 4) 1) 1) \"consumer-b6410cf9-8244-41ba-b0a5-d79b66d33d65\" 2) \"20\" 2) 1) \"consumer-e5a872d4-b488-416e-92ee-55d2902b338f\" 2) \"5\" If you’re new to Redis Streams, this output might not make a lot of sense. The call to XPENDING returns the no. of messages that were received by our processing application, but have not been processed (and acknowledged) yet. In this case, we have two application instances (they randomly generate UUIDs) and have 20 and 5 unprocessed messages respectively (of course, the numbers will differ in your case).\nIn production scenario, application failures could happen due to multiple reasons. However, in our sample app, the below code snippet was used to simulate this situation - it randomly chooses (about 20% probability) to not process the tweet received from Redis Streams:\nif (!(random.nextInt(5) == 0)) { conn.hset(hashName, entry.getFields()); conn.xack(streamName, consumerGroupName, entry.getID()); } That’s the reason you will see XPENDING count increasing slowly but surely. In production, if one (or more) instances crash, the XPENDING count for those instance(s) will stop increasing but remain constant. It implies that, these messages are now left unprocessed - in this specific example, it means that the tweet information will not be available in RediSearch for you to query.\nRedis Streams to the rescue Redis Streams provides reliable messaging. It stores the state for each consumer - that’s exactly what you see with XPENDING! If you start another consumer instance with the same group and consumer name, you will be able to replay the same messages and re-process them to ensure that tweets are stored in Redis. This does not involve doing anything different/additional on your part.\nAnother option is to have a dedicated application that can periodically check the consumer group state (XPENDING), claim messages that have been left abandoned, re-process and (most importantly) acknowledge (XACK) them. In the next (final) part of this series, we will explore how you can build an application to do exactly this!","lets-scale-out#Let\u0026rsquo;s scale out":"","pre-requisites#Pre-requisites":"","so-how-does-it-all-work#So, how does it all work?":"It’s a good time to walk through the code real quick.\nYou can refer to the code in the GitHub repo\nThe app uses JRediSearch which abstracts the API of the RediSearch module. The first thing we do is establish a connection to Redis:\nGenericObjectPoolConfig\u003cJedis\u003e jedisPoolConfig = new GenericObjectPoolConfig\u003c\u003e(); JedisPool pool = new JedisPool(jedisPoolConfig, redisHost, Integer.valueOf(redisPort), timeout, redisPassword, isSSL); Client redisearch = new Client(INDEX_NAME, pool); Then we create a Schema and the Index definition.\nSchema sc = new Schema().addTextField(SCHEMA_FIELD_ID, 1.0).addTextField(SCHEMA_FIELD_USER, 1.0) .addTextField(SCHEMA_FIELD_TWEET, 1.0).addTextField(SCHEMA_FIELD_LOCATION, 1.0) .addTagField(SCHEMA_FIELD_HASHTAGS); IndexDefinition def = new IndexDefinition().setPrefixes(new String[] { INDEX_PREFIX }); try { boolean indexCreated = redisearch.createIndex(sc, Client.IndexOptions.defaultOptions().setDefinition(def)); if (indexCreated) { System.out.println(\"Created RediSearch index \"); } } catch (Exception e) { System.out.println(\"Did not create RediSearch index - \" + e.getMessage()); } To explore the Redis Streams APIs (xgroupCreate, xreadGroup etc.) exposed by the Jedis library, take a look at it’s javadocs\nBefore moving on, we create a Redis Streams Consumer group (using xgroupCreate) - this is mandatory. A consumer group represents a set of applications that work “together” and co-operate with each other to share the processing load:\ntry { conn = pool.getResource(); String res = conn.xgroupCreate(streamName, consumerGroupName, StreamEntryID.LAST_ENTRY, true); } Each app in the consumer group needs to be uniquely identified. While it is possible to assign a name manually, we generate a random consumer name.\nString consumerName = \"consumer-\" + UUID.randomUUID().toString(); The main part of the consumer app is loop that uses xreadGroup to read from the Redis Stream. Notice the StreamEntryID.UNRECEIVED_ENTRY - this means that we will are asking Redis to return stream entries which has not been received by any other consumer in the group. Also, our invocation blocks for 15 seconds and we opt to get a maximum of 50 messages per call to XREADGROUP (of course, you can change this as per requirements).\nwhile (true) { List\u003cEntry\u003cString, List\u003cStreamEntry\u003e\u003e\u003e results = conn.xreadGroup(consumerGroupName, consumerName, 50, 15000, false, Map.entry(streamName, StreamEntryID.UNRECEIVED_ENTRY)); if (results == null) { continue; } .... } Each stream entry needs to be saved to a Redis HASH (using hset). The good thing is that reading a stream entry returns a HashMap and this is exactly what HSET API expects as well. So we are able to re-use the HashMap!\nThat’s not all though, notice the xack method - this is way to call XACK and communicate that we have indeed processed the message successfully:\nfor (Entry\u003cString, List\u003cStreamEntry\u003e\u003e result : results) { List\u003cStreamEntry\u003e entries = result.getValue(); for (StreamEntry entry : entries) { String tweetid = entry.getFields().get(\"id\"); String hashName = INDEX_PREFIX + tweetid; try { // simulate random failure/anomaly. ~ 20% will NOT be ACKed if (!(random.nextInt(5) == 0)) { conn.hset(hashName, entry.getFields()); conn.xack(streamName, consumerGroupName, entry.getID()); } } catch (Exception e) { continue; } } } There is a lot of scope for optimization here. For e.g. you can make this process multi-threaded by spawning a thread for each batch (say 50 messages)\nThat’s all for this blog!"},"title":"Redis Streams in Action - Part 3 (Java app to process tweets with Redis Streams)"},"/blog/redis-streams-in-action-part4/":{"data":{"":"","code-walk-through#Code walk through":"","deploy-the-monitoring-service-to-azure-functions#Deploy the monitoring service to Azure Functions":"","monitoring-the-monitoring-app#Monitoring the monitoring app!":"","pre-requisites#Pre-requisites":"","serverless-go-apps-on-azure-thanks-to-custom-handlers#Serverless Go apps on Azure, thanks to Custom Handlers":"","thats-a-wrap#That\u0026rsquo;s a wrap!":"Welcome to this series of blog posts which covers Redis Streams with the help of a practical example. We will use a sample application to make Twitter data available for search and query in real-time. RediSearch and Redis Streams serve as the backbone of this solution that consists of several co-operating components, each of which will we covered in a dedicated blog post.\nThe code is available in this GitHub repo - https://github.com/abhirockzz/redis-streams-in-action\nWe will continue from where we left off in the previous blog post and see how to build a monitoring app to make the overall system more robust in the face of high load or failure scenarios. This is because our very often, data processing applications either slow down (due to high data volumes) or may even crash/stop due to circumstances beyond our control. If this happens with our Tweets processing application, the messages that were assigned to a specific instance will be left unprocessed. The monitoring component covered in this blog post, checks pending Tweets (using XPENDING), claims (XCLAIM), processes (store them as HASH using HSET) and finally acknowledges them (XACK).\nThis is a Go application which will be deployed to Azure Functions - yes, we will be using a Serverless model, wherein the monitoring system will execute based on a pre-defined Timer trigger. As always, we will first configure and deploy it to Azure, see it working and finally walk through the code.\nBefore we move on, here is some background about Go support in Azure Functions.\nServerless Go apps on Azure, thanks to Custom Handlers Those who have worked with Azure Functions might recall that Go is not one of the language handlers that is supported by default. That’s where Custom Handlers come into the picture.\nIn a nutshell, a Custom Handler is a lightweight web server that receive events from the Functions host. The only thing you need to implement a Custom Handler in your favorite runtime/language is - HTTP support!\nAn event trigger (via HTTP, Storage, Event Hubs etc.) invokes the Functions host. The way Custom Handlers differ from traditional functions is that the Functions host acts as a middle man: it issues a request payload to the web server of the Custom Handler (the function) along with a payload that contains trigger, input binding data and other metadata for the function. The function returns a response back to the Functions host which passes data from the response to the function’s output bindings for processing.\nHere is a summary of how Custom Handlers work at a high level (the diagram below has been picked from the documentation)\nAlright, let’s move on to the practical bits now.\nPre-requisites Please make sure that you read part 2, 3 of this series and have the respective applications up and running. Our monitoring application will build on top of the Tweets producer and processor services that you deploy.\nYou will need an Azure account which you can get for free and the Azure CLI. Make sure to download and install Go if you don’t have it already and also install the Azure functions Core Tools - this will allow you to deploy the function using a CLI (and also run it test and debug it locally)\nThe upcoming sections will guide you how to deploy and configure the Azure Function.\nDeploy the monitoring service to Azure Functions You will:\nCreate the an Azure Functions app Configure it Deploy the Function to the app that you created Start by creating a Resource Group to host all the components of the solution.\nSearch for Function App in the Azure Portal and click Add\nEnter the required details: you should select Custom Handler as the Runtime stack\nIn the Hosting section, choose Linux and Consumption (Serverless) for Operating system and Plan type respectively.\nEnable Application Insights (if you need to) Review the final settings and click Create to proceed Once the process is complete, the following resource will also be created along with the Function App:\nApp Service plan (a Consumption/Serverless plan in this case) An Azure Storage account An Azure Application Insights function) Update the Function App configuration Our function needs a few environment variables to work properly - these can be added as Function Configuration using the Azure portal. Here is the list:\nRedis connectivity details:\nREDIS_HOST - host and port for Redis instance e.g. myredis:10000 REDIS_PASSWORD - access key (password) for Redis instance Redis Stream info:\nSTREAM_NAME - the name of the Redis Stream (use tweets_stream as the value) STREAM_CONSUMER_GROUP_NAME - name of the Redis Streams consumer group (use redisearch_app_group as the value) Monitoring app metadata:\nMONITORING_CONSUMER_NAME - name of the consumer instance represented by the monitoring app (it is part of the aforementioned consumer group) MIN_IDLE_TIME_SEC - only pending messages that are older than the specified time interval will be claimed We’re now ready to deploy the function First, clone the GitHub repo and build the function:\ngit clone https://github.com/abhirockzz/redis-streams-in-action cd redis-streams-in-action/monitoring-app GOOS=linux go build -o processor_monitor cmd/main.go GOOS=linux is used to build a Linux executable since we chose a Linux OS for our Function App\nTo deploy, use the Azure Functions core tools CLI:\nfunc azure functionapp publish \u003center name of the Azure Function app\u003e Once completed, you should see the following logs:\nGetting site publishing info... Uploading package... Uploading 3.71 MB [###############################################################################] Upload completed successfully. Deployment completed successfully. Syncing triggers... Functions in streams-monitor: monitor - [timerTrigger] You should see the function in the Azure portal as well:\nThe function is configured to trigger every 20 seconds (as per function.json):\n{ \"bindings\": [ { \"type\": \"timerTrigger\", \"direction\": \"in\", \"name\": \"req\", \"schedule\": \"*/20 * * * * *\" } ] } Monitoring the monitoring app! As before, can introspect the state of our system using redis-cli - execute the XPENDING command:\nXPENDING tweets_stream redisearch_app_group You will an output similar to this (the numbers will differ in your case depending on how many tweets processor instances you were running and for how long):\n1) (integer) 209 2) \"1620973121009-0\" 3) \"1621054539960-0\" 4) 1) 1) \"consumer-1f20d41d-e63e-40d2-bc0f-749f11f15026\" 2) \"3\" 2) 1) \"monitoring_app\" 2) \"206\" As explained before, the monitoring app will claim pending messages which haven’t been processed by the other consumers (active or inactive). In the output above, notice that the no. messages currently being processed by monitoring_app (name of our consumer) is 206 - it actually claimed these from another consumer instance(s). Once these messages have been claimed, their ownership moves from their original consumer to the monitoring_app consumer.\nYou can check the same using XPENDING tweets_stream redisearch_app_group again, but it might be hard to detect since the messages actually get processed pretty quickly.\nOut of the 206 messages that were claimed, only the ones that have not being processed in the last 10 seconds (this is the MIN_IDLE_TIME_SEC we had specified) will be processed - others will be ignored and picked up in the next run by XPENDING call (if they are still in an unprocessed state). This is because we want to give some time for our consumer application to finish their work - 10 seconds is a pretty generous time-frame for the processing that involves saving to HASH using HSET followed by XACK. .\nPlease note that the 10 second time interval used above has been used as example and you should determine these figures based on the end to end latencies required for your data pipelines/processing.\nYou have complete flexibility in terms of how you want to run/operate such a “monitoring” component. I chose a serverless function but you could run it as standalone program, as a scheduled Cron job or even as a Kubernetes Job!\nDon’t forget to execute RediSearch queries to validate that you can search for tweets based on multiple criteria:\nFT.SEARCH tweets-index hello FT.SEARCH tweets-index hello|world FT.SEARCH tweets-index \"@location:India\" FT.SEARCH tweets-index \"@user:jo* @location:India\" FT.SEARCH tweets-index \"@user:jo* | @location:India\" FT.SEARCH tweets-index \"@hashtags:{cov*}\" FT.SEARCH tweets-index \"@hashtags:{cov*|Med*}\" Now that we have seen things in action, let’s explore the code.\nCode walk through Please refer to the code on GitHub\nThe app uses the excellent go-redis client library. As usual, it all starts with connecting to Redis (note the usage of TLS):\nclient := redis.NewClient(\u0026redis.Options{Addr: host, Password: password, TLSConfig: \u0026tls.Config{MinVersion: tls.VersionTLS12}}) err = client.Ping(context.Background()).Err() if err != nil { log.Fatal(err) } Then comes the part where bulk of the processing happens - think of it as workflow with sub-parts:\nWe call XPENDING to detect no. of pending messages e.g. XPENDING tweets_stream group1\nnumPendingMessages := client.XPending(context.Background(), streamName, consumerGroupName).Val().Count To get the pending messages, we invoke a different variant of XPENDING, to which we pass on the no. of messages we obtained in previous call\nxpendingResult := client.XPendingExt(context.Background(), \u0026redis.XPendingExtArgs{Stream: streamName,Group: consumerGroupName, Start: \"-\", End: \"+\", Count: numPendingMessages}) We can now claim the pending messages - the ownership of these will be changes from the previous consumer to the new consumer (monitoringConsumerName) whose name we specified\nxclaim := client.XClaim(context.Background(), \u0026redis.XClaimArgs{Stream: streamName, Group: consumerGroupName, Consumer: monitoringConsumerName, MinIdle: time.Duration(minIdleTimeSec) * time.Second, Messages: toBeClaimed}) Once the ownership is transferred, we can process them. This involves, adding tweet info to HASH (using HSET) and acknowledging successful processing (XACK). goroutines are used to keep things efficient for e.g. if we get 100 claimed messages in a batch, a scatter-gather process is folloeed where a goroutine is spawned to process each of these message. A sync.WaitGroup is used to “wait” for the current batch to complete before looking for next set of pending messages (if any).\nfor _, claimed := range xclaim.Val() { if exitSignalled { return } waitGroup.Add(1) go func(tweetFromStream redis.XMessage) { hashName := fmt.Sprintf(\"%s%s\", indexDefinitionHashPrefix, tweetFromStream.Values[\"id\"]) processed := false defer func() { waitGroup.Done() }() err = client.HSet(context.Background(), hashName, claimed.Values).Err() if err != nil { return // don't proceed (ACK) if HSET fails } err = client.XAck(context.Background(), streamName, consumerGroupName, tweetFromStream.ID).Err() if err != nil { return } processed = true }(claimed) } waitGroup.Wait() Before we dive into the other areas, it might help to understand the nitty gritty by exploring the code (which is relatively simple by the way)\nQuick note on the application structure Here is how the app is setup (folder structure):\n. ├── cmd │ └── main.go ├── monitor │ └── function.json ├── go.mod ├── go.sum ├── host.json host.json tells the Functions host where to send requests by pointing to a web server capable of processing HTTP events. Notice the customHandler.description.defaultExecutablePath which defines that processor_monitor is the name of the executable that’ll be used to run the web server.\n{ \"version\": \"2.0\", \"extensionBundle\": { \"id\": \"Microsoft.Azure.Functions.ExtensionBundle\", \"version\": \"[1.*, 2.0.0)\" }, \"customHandler\": { \"description\": { \"defaultExecutablePath\": \"processor_monitor\" }, \"enableForwardingHttpRequest\": true }, \"logging\": { \"logLevel\": { \"default\": \"Trace\" } } } That’s a wrap! this brings us to the end of this blog series. let’s recap what we learnt:\nIn the first part you got an overview of the use case, architecture, it’s components, along with an introduction to Redis Streams and RediSearch. It setup the scene for rest of the series. Part two dealt with the specifics of the Rust based tweets consumer app that consumed from the Twitter Streaming API and queued up the tweets in Redis Streams for further processing. Third part was all about the Java app that processed these tweets using by leveraging the Redis Streams Consumer Group feature and scaling out processing across multiple instances. … and final part (this one) was all about the Go app to monitor tweets that have been abandoned (in the pending entry list) either due to processing failure or consumer instance failure. I hope you found this useful and apply it to building scalable solutions with Redis Streams. Happy coding!"},"title":"Redis Streams in Action - Part 4 (Serverless Go app to monitor tweets processor)"},"/blog/redis-timeseries-kafka/":{"data":{"":"RedisTimeSeries is a Redis Module that brings native Time Series data structure to Redis. Time Series solutions which were earlier built on top of Sorted Sets (or Redis Streams) can benefit from RedisTimeSeries features such as high volume inserts, low latency reads, flexible query language, down-sampling and much more!\nGenerally speaking, Time Series data is (relatively) simple. Having said that, we need to factor in other characteristics as well:\nData velocity: For e.g. think hundreds of metrics from thousands of devices per second Volume (Big data): Think data accumulation over months (even years) Thus, Time Series databases such as RedisTimeSeries are just a part of the overall solution. You also need to think about how to collect (ingest), process and send all your data to RedisTimeSeries. What you really need is a scalable Data Pipeline that can act as a buffer to decouple producers, consumers.\nThat’s where Apache Kafka comes in! In addition to the core broker, it has a rich ecosystem of components, including Kafka Connect (which is a part of the solution architecture presented in this blog post), client libraries in multiple languages, Kafka Streams, Mirror Maker etc.\nThis blog post provides a practical example of how to use RedisTimeSeries with Apache Kafka for analyzing time series data.\nGitHub repo - https://github.com/abhirockzz/redis-timeseries-kafka\nLet’s start off by exploring the use case first - please note that it has been kept simple for the purposes of the blog post, but the subsequent sections","additional-considerations#Additional considerations":"\nOptimising RedisTimeSeries\nRetention policy: Think about this since your time series data points do not get trimmed/deleted by default. Down-sampling/Aggregations Rules: You don’t want to store data forever, right? Make sure to configure appropriate rules to take care of this (e.g. TS.CREATERULE temp:1:2 temp:avg:30 AGGREGATION avg 30000) Duplicate data policy: How would you like to handle duplicate samples? Make sure that the default policy (BLOCK) is indeed what you need. If not, consider other options. This is not an exhaustive list. For other configuration options, please refer to the RedisTimeSeries documentation\nWhat about long term data retention?\nData is precious, including time series! You may want to process it further e.g. run Machine learning to extract insights, predictive maintenance etc. For this to be possible, you will need to retain this data for longer time frame and for this to be cost-effective and efficient, you would want to use a scalable Object storage service such Azure Data Lake Storage Gen2 (ADLS Gen2).\nThere is a connector for that! You could enhance you existing data pipeline by using the fully-managed Azure Data Lake Storage Gen2 Sink Connector for Confluent Cloud to process and store the data in ADLS and then run machine learning using Azure Synapse Analytics or Azure Databricks.\nScalability\nYour time series data volumes can only move one way – up! It’s critical for your solution to be scalable from variety of angles:\nCore infrastructure: Managed services allow teams to focus on the solution rather than setting up and maintaining infrastructure, specially when it comes to complex distributed systems such as databases and streaming platforms such as Redis and Kafka. Kafka Connect: As far as the data pipeline is concerned, you’re in good hands since Kafka Connect platform is inherently stateless and horizontally scalable. You’ve a lot of options in terms of how you want to architect and size your Kafka Connect worker clusters. Custom applications: As was the case in this solution, we built a custom application to process data in Kafka topics. Fortunately, the same scalability characteristics apply to them as well. In terms of horizontal scale - it is limited only by number of Kafka topic partitions you have. Integration: It’s not just Grafana! RedisTimeSeries also integrates with Prometheus and Telegraf. However, there is no Kafka connector at the time this blog post was written - this would a great add-on!","conclusion#Conclusion":"Sure, you can use Redis for (almost) everything, including time series workloads! Be sure to think about the end-to-end architecture for data pipeline and integration from time series data sources, all the way to Redis and beyond.","delete-resources#Delete resources":"Delete resources:\nFollow the steps in the documentation to delete the Confluent Cloud cluster - all you need is to delete the Confluent organisation. Similarly, you should delete the Azure Cache for Redis instance as well. On your local machine:\nStop the Kafka Connect cluster Stop the mosquitto broker (e.g. brew services stop mosquitto) Stop Grafana service (e.g. brew services stop grafana) We explored a data pipeline to ingest, process and query time series data using Redis and Kafka. When you think about next steps and move towards a production grade solution, you should consider a few more things.","deploy-the-device-data-processor-app#Deploy the device data processor app":"Build the application JAR file:\ncd consumer export JAVA_HOME=/Library/Java/JavaVirtualMachines/zulu-11.jdk/Contents/Home mvn clean package Create an Azure Spring Cloud application and deploy the JAR file:\naz spring-cloud app create -n device-data-processor -s \u003cname of Azure Spring Cloud instance\u003e -g \u003cname of resource group\u003e --runtime-version Java_11 az spring-cloud app deploy -n device-data-processor -s \u003cname of Azure Spring Cloud instance\u003e -g \u003cname of resource group\u003e --jar-path target/device-data-processor-0.0.1-SNAPSHOT.jar ","device-monitoring#Device monitoring":"Imagine there are many locations and each of them has multiple devices and you’re tasked with the responsibility to monitor device metrics - for now we will consider temperature and pressure. We will store these metrics in RedisTimeSeries (of course!) and use the following naming convention for the keys - \u003cmetric name\u003e:\u003clocation\u003e:\u003cdevice\u003e. For e.g. temperature for device 1 in location 5 will be represented as temp:5:1. each time series data point will also have the following labels (metadata) - metric, location, device. This is to allow for flexible querying as you will see later in the upcoming sections.\nHere are a couple of examples to give you an idea of how you would add data points using the TS.ADD command:\n# temperature for device 2 in location 3 along with labels TS.ADD temp:3:2 * 20 LABELS metric temp location 3 device 2 # pressure for device 2 in location 3 TS.ADD pressure:3:2 * 60 LABELS metric pressure location 3 device 2` ","enjoy-grafana-dashboards#Enjoy Grafana dashboards!":"Browse to the Grafana UI at localhost:3000.\nThe Redis Data Source plugin for Grafana works with any Redis database, including Azure Cache for Redis. Follow the instructions in this blog post to configure a data source.\nImport the dashboards in the grafana_dashboards folder in the GitHub repo you had cloned.\nRefer to the Grafana documentation if you need assistance on how to import dashboards.\nFor instance here is a dashboard that shows the average pressure (over 30 seconds) for device 5 in location 1 (uses TS.MRANGE)\nHere is another dashboard, that shows the maximum temperature (over 15 seconds) for multiple devices in location 3 (again, thanks to TS.MRANGE).","pre-reqs#Pre-reqs":" An Azure account - you can get one for free here Install Azure CLI JDK 11 for e.g. OpenJDK A recent version of Maven and Git ","set-up-the-infrastructure-components#Set up the infrastructure components":"Follow the documentation to provision Azure Cache for Redis (Enterprise Tier) which comes with the RedisTimeSeries module.\nProvision Confluent Cloud cluster on Azure Marketplace and also create a Kafka topic (e.g. mqtt.device-stats)\nYou can provision an instance of Azure Spring Cloud using the Azure portal or use the Azure CLI\naz spring-cloud create -n \u003cname of Azure Spring Cloud service\u003e -g \u003cresource group name\u003e -l \u003center location e.g southeastasia\u003e Before moving on, make sure to clone the GitHub repo:\ngit clone https://github.com/abhirockzz/redis-timeseries-kafka cd redis-timeseries-kafka ","setup-local-services#Setup local services":"The components include:\nMosquitto MQTT broker Grafana for tracking time series data in dashboard Kafka Connect with the MQTT source connector MQTT broker I installed and started the mosquitto broker locally on Mac.\nbrew install mosquitto brew services start mosquitto You can follow steps corresponding your OS or feel free to use the this Docker image.\nGrafana I installed and started Grafana locally on Mac.\nbrew install grafana brew services start grafana You can do the same for your OS or feel free to use this Docker image.\ndocker run -d -p 3000:3000 --name=grafana -e \"GF_INSTALL_PLUGINS=redis-datasource\" grafana/grafana Kafka Connect You should be able to find the connect-distributed.properties file in the repo that you just cloned. Replace the values for properties such as bootstrap.servers, sasl.jaas.config etc.\nFirst, download and unzip Apache Kafka locally.\nStart a local Kafka Connect cluster\nexport KAFKA_INSTALL_DIR=\u003ckafka installation directory e.g. /home/foo/kafka_2.12-2.5.0\u003e $KAFKA_INSTALL_DIR/bin/connect-distributed.sh connect-distributed.properties To install MQTT source connector manually:\nDownload the connector/plugin ZIP file from this link, and, Extract it into one of the directories that is listed on the Connect worker’s plugin.path configuration properties If you’re using Confluent Platform locally, simply use the CLI: confluent-hub install confluentinc/kafka-connect-mqtt:latest\nCreate MQTT source connector instance\nMake sure to check the mqtt-source-config.json file: make sure you enter the right topic name for kafka.topic and leave the mqtt.topics unchanged.\ncurl -X POST -H 'Content-Type: application/json' http://localhost:8083/connectors -d @mqtt-source-config.json # wait for a minute before checking the connector status curl http://localhost:8083/connectors/mqtt-source/status ","so-you-want-to-run-some-redistimeseries-commands#So, you want to run some RedisTimeSeries commands!":"Crank up the redis-cli and connect to the Azure Cache for Redis instance:\nredis-cli -h \u003cazure redis hostname e.g. redisdb.southeastasia.redisenterprise.cache.azure.net\u003e -p 10000 -a \u003cazure redis access key\u003e --tls Start with simple queries:\n# pressure in device 5 for location 1 TS.GET pressure:1:5 # temperature in device 5 for location 4 TS.GET temp:4:5 Filter by location and get temperature and pressure for all devices:\nTS.MGET WITHLABELS FILTER location=3 Extract temp and pressure for all devices in one or more locations within a specific time range:\nTS.MRANGE - + WITHLABELS FILTER location=3 TS.MRANGE - + WITHLABELS FILTER location=(3,5) - + refers to everything from beginning up until the latest timestamp, but you could be more specific\nMRANGE is what we need! We can get back multiple time series and use filter.\nWe can also filter by a specific device in a location and further drill down by either temperature or pressure:\nTS.MRANGE - + WITHLABELS FILTER location=3 device=2 TS.MRANGE - + WITHLABELS FILTER location=3 device=2 metric=temp All these can be combined with aggregations.\nTS.MRANGE - + WITHLABELS FILTER location=3 metric=temp # all the temp data points are not useful. how about an average (or max) instead of every temp data points? TS.MRANGE - + WITHLABELS AGGREGATION avg 10000 FILTER location=3 metric=temp TS.MRANGE - + WITHLABELS AGGREGATION max 10000 FILTER location=3 metric=temp It’s also possible to create a rule to do this aggregation and store in a different time series\nOnce you’re done, don’t forget to delete resources to avoid unwanted costs.","solution-architecture#Solution Architecture":"Here is what the solution looks like at a high level:\nTo summarise the end-to-end flow:\nSimulated device data is sent to a local MQTT broker. this data is picked up by the MQTT Kafka connect source connector and sent to a confluent cloud Kafka cluster in Azure. it is processed by a spring application in Azure Spring Cloud which is finally sent to a Redis database in Azure.\nIt’s time to dive in! Before that, make sure you have the following:","start-simulated-device-data-generator#Start simulated device data generator":"Use script to send data to local MQTT broker. You can use the script in the GitHub repo you just cloned:\n./gen-timeseries-data.sh All it does is use the mosquitto_pub CLI command to send data\nData is sent to the device-stats MQTT topic (this is not the Kafka topic). You can double check by using the CLI subscriber:\nmosquitto_sub -h localhost -t device-stats To validate the end-to-end pipeline\nCheck the Kafka topic in the Confluent Cloud portal. You should also check the logs for the device data processor app in Azure Spring Cloud:\naz spring-cloud app logs -f -n device-data-processor -s \u003cname of Azure Spring Cloud instance\u003e -g \u003cname of resource group\u003e "},"title":"Processing Time-Series Data with Redis and Apache Kafka"},"/blog/redisearch-in-action/":{"data":{"":"Self-managing a distributed system like Apache Kafka ®, along with building and operating Kafka connectors, is complex and resource intensive. It requires significant Kafka skills and expertise in the development and operations teams of your organization. Additionally, the higher the volumes of real-time data that you work with, the more challenging it becomes to ensure that all of the infrastructure scales efficiently and runs reliably.\nConfluent and Microsoft are working together to make the process of adopting event streaming easier than ever by alleviating the typical infrastructure management needs that often pull developers away from building critical applications. With Azure and Confluent seamlessly integrated, you can collect, store, process event streams in real-time and feed them to multiple Azure data services. The integration helps reduce the burden of managing resources across Azure and Confluent.\nThe unified integration with Confluent enables you to:\nProvision a new Confluent Cloud resource from Azure client interfaces like Azure Portal/CLI/SDKs with fully managed infrastructure Streamline single sign-on (SSO) from Azure to Confluent Cloud with your existing Azure Active Directory (AAD) identities Get unified billing of your Confluent Cloud service usage through Azure subscription invoicing with the option to draw down on Azure commits; Confluent Cloud consumption charges simply appear as a line item on monthly Azure bills Manage Confluent Cloud resources from the Azure portal and track them in the \"All Resources\" page, alongside your Azure resources Confluent has developed an extensive library of pre-built connectors that seamlessly integrate data from many different environments. With Confluent, Azure customers access fully managed connectors that stream data for low-latency, real-time analytics into Azure and Microsoft services like Azure Functions, Azure Blob Storage, Azure Event Hubs, Azure Data Lake Storage (ADLS) Gen2, and Microsoft SQL Server. More real-time data can now easily flow to applications for smarter analytics and more context-rich experiences.","build-and-deploy-applications-to-azure-spring-cloud#Build and deploy applications to Azure Spring Cloud":"Start by cloning the GitHub repository and go into the mysql-kafka-redis-integration directory:\ngit clone [https://github.com/Azure-Samples/mysql-kafka-redis-integration](https://github.com/Azure-Samples/mysql-kafka-redis-integration) cd mysql-kafka-redis-integration For both services, update the application.yaml file in the src/main/resources folder with the connection details for Azure Cache for Redis and the Confluent Cloud cluster.\nHere is a trimmed down version for the change events processor service:\nredis: host: \u003center redis host\u003e port: \u003center redis port\u003e password: \u003center redis access key\u003e topic: name: \u003ctopic name e.g. myserver.products\u003e partitions-num: 6 replication-factor: 3 spring: kafka: bootstrap-servers: - \u003center Confluent Cloud bootstrap server\u003e properties: ssl.endpoint.identification.algorithm: https sasl.mechanism: PLAIN request.timeout.ms: 20000 retry.backoff.ms: 500 sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\u003center Confluent Cloud API key\u003e\" password=\"\u003center Confluent Cloud API secret\u003e\"; security.protocol: SASL_SSL ... The config for the Search API service is quite compact:\nredis: host: \u003center redis host\u003e port: \u003center redis port\u003e password: \u003center redis access key\u003e Build JAR files for the Spring applications:\nexport JAVA_HOME=\u003center path to JDK e.g. /Library/Java/JavaVirtualMachines/zulu-11.jdk/Contents/Home\u003e # Change Events Processor service mvn clean package -f change-events-processor/pom.xml # Search API service mvn clean package -f search-api/pom.xml Install the Azure Spring Cloud extension for the Azure CLI:\naz extension add --name spring-cloud Create the Azure Spring Cloud applications corresponding to both of the services:\n# Change Events Processor service az spring-cloud app create -n change-events-processor -s \u003center the name of Azure Spring Cloud service instance\u003e -g \u003center azure resource group name\u003e --runtime-version Java_11 # Search API service az spring-cloud app create -n search-api -s \u003center the name of Azure Spring Cloud service instance\u003e -g \u003center azure resource group name\u003e --runtime-version Java_11 --is-public true Deploy the JAR files for the respective applications that you just created:\n# for the Change Events Processor service az spring-cloud app deploy -n change-events-processor -s \u003center the name of Azure Spring Cloud service instance\u003e -g \u003center azure resource group name\u003e --jar-path change-events-processor/target/change-events-processor-0.0.1-SNAPSHOT.jar # for the Search API service az spring-cloud app deploy -n search-api -s \u003center the name of Azure Spring Cloud service instance\u003e -g \u003center azure resource group name\u003e --jar-path search-api/target/search-api-0.0.1-SNAPSHOT.jar ","configure-mysql-and-confluent-cloud-on-azure#Configure MySQL and Confluent Cloud on Azure":"MySQL instance on Azure\nCreate an Azure Database for MySQL server using the Azure CLI (or the Azure portal if that's what you prefer):\nKafka cluster in Confluent Cloud\nSet up and subscribe for Apache Kafka on Confluent Cloud, which you can easily discover via Azure Marketplace Provide configuration details for creating a Confluent Cloud organization on Azure Provisioning in Azure: Seamlessly provision Confluent organizations through the Azure portal Single sign-on to Confluent Cloud: Log in directly to Confluent Cloud Create Confluent Cloud fully managed resources like clusters, topics, and connectors Create a topic (optional): The connector automatically creates a topic (based on the default convention); create the topic manually if you want to override its default settings, though make sure to use the same topic name while configuring the connector Configure and launch the MySQL source connector using the portal ","delete-azure-resources#Delete Azure resources":"Once you're done, delete the services so that you do not incur unwanted costs. If they are in the same resource group, simply deleting the resource group will suffice. You can also delete the resources (MySQL, Confluent Cloud organization, Redis, and Azure Spring Cloud instance) individually.","export-data-to-azure-data-lake#Export data to Azure Data Lake":"If you want to store this data in Azure Data Lake Storage longer term (cold storage), Confluent's ADLS Gen2 connector has you covered. In our scenario, we already have product data flowing into the Kafka topic in Confluent Cloud on Azure-all we need to do is configure the connector to get the job done.\nAnd guess what-that's available as a fully managed offering as well!\nHere is what you need to do:\nCreate a storage account Configure the connector and start it; make sure to use the same topic name as you did before (e.g., myserver.products) Confirm that the data was exported to the Azure storage container in the ADLS account For a step by step guide, please follow the documentation.","objectives#Objectives":"The data can be uploaded into a relational database on Azure Database for MySQL, in this case, through an application or a batch process. This data will be synchronised from Confluent Cloud on Azure to the RediSearch module available in the Azure Cache for Redis Enterprise service. This will enable you to perform real-time search with your data in a flexible way. The real-time data is also streamed to an ADLS store. All the service components can be deployed to one Azure region for low latency and performance. Additionally, these service components are deployed in a single Azure subscription to enable unified billing of your Confluent Cloud usage through Azure subscription invoicing.\nPrerequisites\nAn Azure account Install the Azure CLI to deploy and manage the infrastructure components JDK 11 or above for e.g. Open JDK A recent Maven release Install Git Set up the Azure cloud environment\nCreate an Azure Database for MySQL server Create an instance of Apache Kafka on Confluent Cloud Create a Redis Enterprise cache instance with the RediSearch module enabled Provision an instance of Azure Spring Cloud Build and deploy applications to Azure Spring Cloud\nSet up the consumer application to process events from Kafka topics to Redis Set up the search app to query records from RediSearch Build and deploy the application JAR file Use the search application to query data\nUse curl or an HTTP client to invoke the Search API Export data to Azure Data Lake\nSetup ADLS Gen 2 connector to export data from Kafka topics Clean up\nDelete the resources (MySQL, Confluent Cloud organization, Redis, and your Azure Spring Cloud instance) individually or delete the resource group ","real-time-search-use-case#Real-time search use case":"In today's rapidly evolving business ecosystem, organizations must create new business models, provide great customer experiences, and improve operational efficiencies to stay relevant and competitive. Technology plays a critical role in this journey with the new imperative being to build scalable, reliable, persistent real-time systems. Real-time infrastructure for processing large volumes of data with lower costs and reduced risk plays a key role in this evolution.\nApache Kafka often plays a key role in the modern data architecture with other systems producing/consuming data to/from it. These could be customer orders, financial transactions, clickstream events, logs, sensor data, and database change events. As you might imagine, there is a lot of data in Kafka (topics), but it's useful only when processed (e.g., with Azure Spring Cloud or ksqlDB) or when ingested into other systems.\nLet's investigate an architecture pattern that transforms an existing traditional transaction system into a real-time data processing system. We'll describe a data pipeline that synchronizes data between MySQL and RediSearch, powered by Confluent Cloud on Azure. This scenario is applicable to many use cases, but we'll specifically cover the scenario where batch data must be available to downstream systems in near real time to fulfill search requirements. The data can be further streamed to an ADLS store for correlation of real-time and historic data, analytics, and visualizations. This provides a foundation for other services through APIs to drive important parts of the business, such as a customer-facing website that can provide fresh, up-to-date information on products, availability, and more.\nBelow are the key elements and capabilities of the above-mentioned architecture:\nInfrastructure components that form the bedrock of the architecture ADLS Gen2 sink connector for exporting data from Kafka topics to ADLS ADLS datastore for correlation of real-time and historical data and further analytics and visualizations Application components: These are services running on Azure Spring Cloud A Java Spring Boot application that uses the Spring for Apache Kafka integration is a consumer application that processes events from Kafka topics to Redis by creating the required index definition; it adds records as RediSearch documents by creating the required index definition and adding new product information as RediSearch documents (currently represented as a Redis hash) A search application is another Spring Boot application that makes the RediSearch data available as a REST API; it allows you to execute queries per the RediSearch query syntax The above-mentioned services use the JRediSearch library to interface with RediSearch in order to create indexes, add documents, and query. Thanks to the JDBC source connector, data in MySQL (the products table) is sent to a Kafka topic. Here is what the JSON payload looks like:","summary#Summary":"The urgency for real-time applications will grow exponentially as more businesses undergo digital transformation. With the new integration between Confluent and Azure along with the fully managed Kafka connectors available to export and source data into Azure data and storage services, you will be able process huge volumes of data much faster, simplify integration, and avoid the challenges of setting up and maintaining complex distributed systems.\nThis complete guide showed you the high-level architecture on how to run this solution on Azure based on managed PaaS services. The benefit of this is that you don't have to set up and maintain complex distributed systems, such as a database, event streaming platform, and runtime infrastructure for your Spring Boot Java apps.\nBear in mind that this is just one part of a potentially larger use case. Thanks to Kafka, you can extend this solution to integrate with other systems as well, such as Azure Data Lake, using yet another fully managed ADLS Gen2 connector.\nWant to learn more?\nIf you'd like to learn more, Get started with Apache Kafka on Confluent Cloud via Azure Marketplace and follow the quick start. When you sign up, you'll receive $200 of free usage each month for your first three months. Use the promo code CL60BLOG to receive an additional $60 of free usage.*\nGet Started\nRamya Oruganti is a senior product manager at Microsoft. She works on the Azure Developer Experience team focused on building integration services between Azure and Confluent Cloud. She has been working in the cloud and data space for more than a decade. From engineering to solution architecture roles at organizations like IBM, Oracle, and now Microsoft, Ramya has a wealth of hands-on understanding when it comes to cloud products.\nAbhishek Gupta is a senior developer advocate at Microsoft where he helps developers be successful with the Azure platform. His key focus areas include Kafka, databases, and Kubernetes. Abhishek is also an open source contributor and a Confluent Community Catalyst. Previously, in his role as a product manager, he helped build and advocate for developer-focused PaaS products.","time-to-see-real-time-search-in-action#Time to see real-time search in action!":"Now that we have all the components in place, we can test the end-to-end functionality. We will start by adding new product data to the MySQL database and use the Search app to make sure it has propagated all the way to Redis.\nInsert the following sample data:\nINSERT INTO `products` VALUES (42, 'Outdoor chairs', NOW(), '{\"brand\": \"Mainstays\", \"description\": \"Mainstays Solid Turquoise 72 x 21 in. Outdoor Chaise Lounge Cushion\", \"tags\": [\"Self ties cushion\", \"outdoor chairs\"], \"categories\": [\"Garden\"]}'); INSERT INTO `products` VALUES (43, 'aPhone', NOW(), '{\"brand\": \"Orange\", \"description\": \"An inexpensive phone\", \"tags\": [\"electronics\", \"mobile phone\"], \"categories\": [\"Electronics\"]}'); Get the URL for the Search API service using the portal or the CLI:\naz spring-cloud app show -n search-api -s \u003center the name of Azure Spring Cloud service instance\u003e -g \u003center azure resource group name\u003e Use curl or another HTTP client to invoke the Search API. Each of these queries will return results in form of a JSON payload, like so:\n[ { \"created\": \"1614235666000\", \"name\": \"Outdoor chairs\", \"description\": \"Mainstays Solid Turquoise 72 x 21 in. Outdoor Chaise Lounge Cushion\", \"id\": \"42\", \"categories\": \"Garden\", \"brand\": \"Mainstays\", \"tags\": \"Self ties cushion, outdoor chairs\" }, { \"created\": \"1614234718000\", \"name\": \"aPhone\", \"description\": \"An inexpensive phone\", \"id\": \"43\", \"categories\": \"Electronics\", \"brand\": \"Orange\", \"tags\": \"electronics, mobile phone\" } ] Here are a few examples to get you started. Note that the query parameter q is used to specify the RediSearch query.\n# search for all records curl \u003csearch api URL\u003e/search?q=* # search for products by name curl \u003csearch api URL\u003e/search?q=@name:Outdoor chairs # search for products by category curl \u003csearch api URL\u003e/search?q=@categories:{Garden | Electronics} # search for products by brand curl \u003csearch api URL\u003e/search?q=@brand:Mainstays # apply multiple search criteria curl \u003csearch api URL\u003e/search?q=@categories:{Electronics} @brand:Orange You can continue to add more product information and check the pipeline. You may also want to try the following:\nConfirm that information is flowing to the Kafka topic in Confluent Cloud Check the logs for the consumer application deployed to Azure Spring Cloud—you will be able to see the events that are getting processed (use az spring-cloud app logs -n -s -g ) Take a look at the RediSearch query syntax and try other queries as well Connect to the Azure Cache for Redis instance and run the RediSearch queries directly just to double-check Connect to the Azure Cache for Redis instance using the redis-cli:\nredis-cli -h \u003center host name\u003e -p \u003center port i.e. 10000\u003e -a \u003center redis password/access key\u003e --tls "},"title":"Real-Time Search and Analytics with Confluent, Azure, Redis, and Spring Cloud"},"/blog/rust-kafka-getting-started-1/":{"data":{"":"This is a two-part series to help you get started with Rust and Kafka. We will be using the rust-rdkafka crate which itself is based on librdkafka (C library).\nIn this post we will cover the Kafka Producer API.","initial-setup#Initial setup":"Make sure you install a Kafka broker - a local setup should suffice. Of course you will need to have Rust installed as well - you will need version 1.45 or above\nBefore you begin, clone the GitHub repo:\ngit clone https://github.com/abhirockzz/rust-kafka-101 cd part1 Check the Cargo.toml file:\n... [dependencies] rdkafka = { version = \"0.25\", features = [\"cmake-build\",\"ssl\"] } ... Note on the cmake-build feature\nrust-rdkafka provides a couple of ways to resolve the librdkafka dependency. I chose static linking, wherein librdkafka was compiled. You could opt for dynamic linking to refer to a locally installed version though.\nFor more, please refer to this link\nOk, let’s start off with the basics.","is-there-a-better-way#Is there a better way?":"Yes! If you are used to the declarative serialization/de-serialization approach in the Kafka Java client (and probably others as well), you may not like this “explicit” approach. Just to put things in perspective, this is how you’d do it in Java:\nProperties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"io.confluent.kafka.serializers.json.KafkaJsonSchemaSerializer\"); .... ProducerRecord\u003cString, User\u003e record = new ProducerRecord\u003cString, User\u003e(topic, key, user); producer.send(record); Notice that you simply configure the Producer to use KafkaJsonSchemaSerializer and the User class is serialized to JSON\nrust-rdkafka provides something similar with the ToBytes trait. Here is what it looks like:\npub trait ToBytes { /// Converts the provided data to bytes. fn to_bytes(\u0026self) -\u003e \u0026[u8]; } Self-explanatory, right? There are existing implementations for String, Vec\u003cu8\u003e etc. So you can use these types as key or value without any additional work - this is exactly what we just did. But the problem is the way we did it was “explicit” i.e. we converted the User struct into a JSON String and passed it on.\nWhat if we could implement ToBytes for User?\nimpl ToBytes for User { fn to_bytes(\u0026self) -\u003e \u0026[u8] { let b = serde_json::to_vec_pretty(\u0026self).expect(\"json serialization failed\"); b.as_slice() } } You will see a compiler error:\ncannot return value referencing local variable `b` returns a value referencing data owned by the current function For additional background, please refer to this [GitHub issue] (https://github.com/fede1024/rust-rdkafka/issues/128). I would happy to see an example other which can work with ToBytes - please drop in a note if you’ve inputs on this!\nTL;DR is that it’s best to stick to the “explicit” way of doing things unless you have a ToBytes implementation that “does not involve an allocation and cannot fail”.","producer-callback#Producer callback":"We are flying blind right now! Unless we explicitly create a consumer to look at our messages, we have no clue whether they are being sent to Kafka. Let’s fix that by implementing a ProducerContext (trait) to hook into the produce event - it’s like a callback.\nStart by creating a struct and an empty implementation for the ClientContext trait (this is mandatory).\nstruct ProducerCallbackLogger; impl ClientContext for ProducerCallbackLogger {} Now comes the main part where we implement the delivery function in the ProducerContext trait.\nimpl ProducerContext for ProduceCallbackLogger { type DeliveryOpaque = (); fn delivery( \u0026self, delivery_result: \u0026rdkafka::producer::DeliveryResult\u003c'_\u003e, _delivery_opaque: Self::DeliveryOpaque, ) { let dr = delivery_result.as_ref(); match dr { Ok(msg) =\u003e { let key: \u0026str = msg.key_view().unwrap().unwrap(); println!( \"produced message with key {} in offset {} of partition {}\", key, msg.offset(), msg.partition() ) } Err(producer_err) =\u003e { let key: \u0026str = producer_err.1.key_view().unwrap().unwrap(); println!( \"failed to produce message with key {} - {}\", key, producer_err.0, ) } } } } We match against the DeliveryResult (which is a Result after all) to account for success (Ok) and failure (Err) scenarios. All we do is simply log the message in both cases, since this is just an example. You could do pretty much anything you wanted to here (don’t go crazy though!)\nWe’ve ignored DeliveryOpaque which is an associated type of the ProducerContext trait\nWe need to make sure that we plug in our ProducerContext implementation. We do this by using the create_with_context method (instead of create) and make sure by providing the correct type for BaseProducer as well.\nlet producer: BaseProducer\u003cProduceCallbackLogger\u003e = ClientConfig::new().set(....) ... .create_with_context(ProduceCallbackLogger {}) ... How does the “callback get called”? Ok, we have the implementation, but we need a way to trigger it! One of the ways is to call flush on the producer. So, we could write our producer as such:\nadd producer.flush(Duration::from_secs(3));, and comment the sleep (just for now) producer .send( BaseRecord::to(\"rust\") .key(\u0026format!(\"key-{}\", i)) .payload(\u0026format!(\"value-{}\", i)), ) .expect(\"failed to send message\"); producer.flush(Duration::from_secs(3)); println!(\"flushed message\"); //thread::sleep(Duration::from_secs(3)); Hold on, we can do better! The send method is non-blocking (be default) but by calling flush after each send, we have now converted this into a synchronous invocation - not recommended from a performance perspective.\nWe can improve the situation by using a ThreadedProducer. It takes care of invoking the poll method in a background thread to ensure that the delivery callback notifications are delivered. Doing this is very simple - just change the type from BaseProducer to ThreadedProducer!\n# before: BaseProducer\u003cProduceCallbackLogger\u003e # after: ThreadedProducer\u003cProduceCallbackLogger\u003e Also, we don’t need the call to flush anymore.\n... //producer.flush(Duration::from_secs(3)); //println!(\"flushed message\"); thread::sleep(Duration::from_secs(3)); ... The code is available in src/2_threaded_producer.rs\nRun the program again Rename the file src/2_threaded_producer.rs to main.rs and execute cargo run Output:\nsending message sending message produced message with key key-1 in offset 6 of partition 2 produced message with key key-2 in offset 3 of partition 0 sending message produced message with key key-3 in offset 7 of partition 2 As expected, you should be able to see the producer event callback, denoting that the messages were indeed sent to the Kafka topic. Of course, you can connect to the topic directly and double-check, just like before:\n\u0026KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rust --from-beginning To try a failure scenario, try using an incorrect topic name and notice how the Err variant of the delivery implementation gets invoked.","sending-json-messages#Sending JSON messages":"So far, we were just sending Strings as key and values. JSON is a commonly used message format, let’s see how to use that.\nAssume we want to send User info which will be represented using this struct:\nstruct User { id: i32, email: String, } We can then use serde_json library to serialize this as JSON. All we need is to use the custom derives in serde - Deserialize and Serialize\nuse serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize, Debug)] struct User { id: i32, email: String, } Change the producer loop:\nCreate a User instance Serialize it to a JSON string using to_string_pretty Include that in the payload ... let user_json = serde_json::to_string_pretty(\u0026user).expect(\"json serialization failed\"); producer .send( BaseRecord::to(\"rust\") .key(\u0026format!(\"user-{}\", i)) .payload(\u0026user_json), ) .expect(\"failed to send message\"); ... you can also use to_vec (instead of to_string()) to convert it into a Vec of bytes (Vec\u003cu8\u003e)\nTo run the program… Rename the file src/3_JSON_payload.rs to main.rs, and execute cargo run Consume from the topic:\n\u0026KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rust --from-beginning You should see messages with a String key (e.g. user-34) and JSON value:\n{ \"id\": 34, \"email\": \"user-34@foobar.com\" } ","simple-producer#Simple producer":"Here is a simple producer based on BaseProducer:\nlet producer: BaseProducer = ClientConfig::new() .set(\"bootstrap.servers\", \"localhost:9092\") .set(\"security.protocol\", \"SASL_SSL\") .set(\"sasl.mechanisms\", \"PLAIN\") .set(\"sasl.username\", \"\u003cupdate\u003e\") .set(\"sasl.password\", \"\u003cupdate\u003e\") .create() .expect(\"invalid producer config\"); The send method to start producing messages - it’s done in tight loop with a thread::sleep in between (not something you would do in production) to make it easier to track/follow the results. The key, value (payload) and the destination Kafka topic is represented in the form of a BaseRecord\nfor i in 1..100 { println!(\"sending message\"); producer .send( BaseRecord::to(\"rust\") .key(\u0026format!(\"key-{}\", i)) .payload(\u0026format!(\"value-{}\", i)), ) .expect(\"failed to send message\"); thread::sleep(Duration::from_secs(3)); } You can check the entire code in the file src/1_producer_simple.rs\nTo test if the producer is working … Run the program:\nsimply rename the file src/1_producer_simple.rs to main.rs execute cargo run You should see this output:\nsending message sending message sending message ... What’s going on? To figure it out - connect to your Kafka topic (I have used rust as the name of the Kafka topic in the above example) using the Kafka CLI consumer (or any other consumer client e.g. kafkacat). You should see the messages flowing in.\nFor example:\n\u0026KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rust --from-beginning ","wrap-up#Wrap up":"That’s it for the first part. Part 2 will cover topics around Kafka consumer."},"title":"Getting started with Kafka and Rust: Part 1"},"/blog/rust-kafka-getting-started-2/":{"data":{"":"This is a two-part series to help you get started with Rust and Kafka. We will be using the rust-rdkafka crate which itself is based on librdkafka (C library).\nIn this post we will cover the Kafka Consumer API.","initial-setup#Initial setup":"Make sure you install a Kafka broker - a local setup should suffice. Of course you will need to have Rust installed as well - you will need version 1.45 or above\nBefore you begin, clone the GitHub repo:\ngit clone https://github.com/abhirockzz/rust-kafka-101 cd part2 ","other-considerations#Other considerations":"This is by no means an exhaustive list or coverage of all the delivery semantics:\nWe did not cover at-most once and exactly once. You may want to choose the use Async commit mode - this has it’s own set of caveats. Committing each and every message (even asynchronously) carries overhead. You may want to commit messages/offsets in batches. As always, you need to take care of a lot of corner cases here as well. That’s all for this two part series on getting started with Rust and Kafka using the rust-rdkafka library. We covered:\nA simple producer Producer with delivery callback How to send JSON payloads A basic consumer Handle re-balance and offset commit callbacks Explore manual commit and at-least once delivery semantics ","simple-consumer#Simple Consumer":"Creating a low-level consumer (BaseConsumer) is strikingly similar to how you’d create the its counterpart - BaseProducer. The only difference is that you will have to cast the output to the right type (which in this case is BaseConsumer)\nlet consumer: BaseConsumer = ClientConfig::new() .set(\"bootstrap.servers\", \"localhost:9092\") .set(\"group.id\", \"my_consumer_group\") .create() .expect(\"invalid consumer config\"); consumer .subscribe(\u0026[\"rust\"]) .expect(\"topic subscribe failed\"); Notice that the group.id config has also been included .set(\"group.id\", \"my_consumer_group\") - its mandatory.\nOnce a BaseConsumer is created, one can subscribe to one or more topics (in this case, its just one topic with the name rust).\nTo fetch messages from the topic, we start (spawn) a new thread:\nthread::spawn(move || loop { for msg_result in consumer.iter() { let msg = msg_result.unwrap(); let key: \u0026str = msg.key_view().unwrap().unwrap(); let value = msg.payload().unwrap(); let user: User = serde_json::from_slice(value).expect(\"failed to deser JSON to User\"); println!( \"received key {} with value {:?} in offset {:?} from partition {}\", key, user, msg.offset(), msg.partition() ) } }); It accepts a closure which in this case happens to be a infinite loop that:\nReceives messages, and, Prints out the key, value along with offset and partition info Calling iter on the consumer is just a short-cut invoking poll without any timeout.\nOther variations are also possible. You can use poll directly:\nloop { let message = consumer.poll(Duration::from_secs(2)); ... } Or, use this format:\nfor message in \u0026consumer { ... } Run the program Rename the file src/1_consumer_simple.rs to main.rs, and execute cargo run Output:\nsending message sending message produced message with key user-1 in offset 25 of partition 2 produced message with key user-2 in offset 12 of partition 4 sending message produced message with key user-3 in offset 20 of partition 0 received key user-3 with value User { id: 3, email: \"user-3@foobar.com\" } in offset 20 from partition 0 sending message produced message with key user-4 in offset 24 of partition 3 received key user-4 with value User { id: 4, email: \"user-4@foobar.com\" } in offset 24 from partition 3 sending message produced message with key user-5 in offset 25 of partition 3 received key user-5 with value User { id: 5, email: \"user-5@foobar.com\" } in offset 25 from partition 3 sending message produced message with key user-6 in offset 26 of partition 3 received key user-6 with value User { id: 6, email: \"user-6@foobar.com\" } in offset 26 from partition 3 sending message produced message with key user-7 in offset 27 of partition 3 received key user-7 with value User { id: 7, email: \"user-7@foobar.com\" } in offset 27 from partition 3 As expected:\nYou see the producer callbacks - confirms that the message was sent to Kafka Consumer received the message as well - as confirmed by the log ","switching-to-manual-commit#Switching to Manual commit":"By default, the offset commit process is taken care of by the library itself. But we can exercise more control over it by switching to manual mode.\nFirst thing would be do set enable.auto.commit to false - set(\"enable.auto.commit\", \"false\");\nAt-least once delivery To achieve this, we need to make sure we indeed process the message successfully before committing the offset. To simulate this, let’s write a function (named process) that can fail randomly. We will then use this in our consumer loop and commit only when this functions returns successfully.\nfn process(u: User) -\u003e Result\u003c(), ()\u003e { let mut rnd = rand::thread_rng(); let ok = rnd.gen_bool(1.0 / 2.0); //50% probability of returning true match ok { true =\u003e { println!(\"SUCCESSFULLY processed User info {:?}\", u); Ok(()) } false =\u003e { println!(\"FAILED to process User info {:?}\", u); Err(()) } } } We will need to modify our consumer loop\"\nAdd manual offset commit based on response from the process function Add a label ('consumer_thread) to our thread loop thread::spawn(move || 'consumer_thread: loop { for msg_result in consumer.iter() { //..... omitted println!( \"received key {} with value {:?} in offset {:?} from partition {}\", key, user, msg.offset(), msg.partition() ); let processed = process(user); match processed { Ok(_) =\u003e { consumer.commit_message(\u0026msg, CommitMode::Sync); } Err(_) =\u003e { println!(\"loop encountered processing error\"); break 'consumer_thread; } } } }); We call process - this is to simulate processing of each record received by the consumer. In case the processing succeeds (returns Ok), we commit the record using commit_message.\nNote that the commit itself may fail. This should ideally be handled in the commit_callback implementation of ConsumerContext\nRun the program Rename the file src/3_manual_commit.rs to main.rs, and Execute cargo run The program output is lengthy, but bear with me.\nOutput:\nproduced message with key user-1 in offset 22 of partition 2 produced message with key user-2 in offset 28 of partition 4 post_rebalance callback rebalanced partition 0 rebalanced partition 1 rebalanced partition 2 rebalanced partition 3 rebalanced partition 4 rebalanced partition 5 received key user-5 with value User { id: 5, email: \"user-5@foobar.com\" } in offset 52 from partition 3 SUCCESSFULLY processed User info User { id: 5, email: \"user-5@foobar.com\" } committed offset Offset(53) in partition 3 received key user-2 with value User { id: 2, email: \"user-2@foobar.com\" } in offset 28 from partition 4 SUCCESSFULLY processed User info User { id: 2, email: \"user-2@foobar.com\" } produced message with key user-3 in offset 35 of partition 0 committed offset Offset(29) in partition 4 received key user-1 with value User { id: 1, email: \"user-1@foobar.com\" } in offset 22 from partition 2 FAILED to process User info User { id: 1, email: \"user-1@foobar.com\" } loop encountered processing error. closing consumer... post_rebalance callback ALL partitions have been REVOKED Notice these logs messages when process returns successfully:\nreceived key user-5 with value User { id: 5, email: “user-5@foobar.com” } in offset 52 from partition 3 SUCCESSFULLY processed User info User { id: 5, email: “user-5@foobar.com” } committed offset Offset(52) in partition 3 For a failure scenario:\nreceived key user-1 with value User { id: 1, email: “user-1@foobar.com” } in offset 22 from partition 2 FAILED to process User info User { id: 1, email: “user-1@foobar.com” } loop encountered processing error. closing consumer… We ended up stopping the consumer when processing failed? The question here is:\nHow to handle messages that did not get processed? Note that failure could happen due to many reasons. A couple of them are:\nProcessing failed (this is what we simulated in this example), or, Processing was successful, but the commit failed If we continue with our consumer loop after a failed message, we could end up losing messages (data loss). Why? It’s because the commit_message method also marks smaller offsets (less that the one being handled) as committed. For e.g. if you had a scenario where offset 20 from partition 5 failed to get processed (and committed), you continue processing and offset 21 from partition 5 was processed and committed successfully, you will end up missing data from offset 20 - this is because committing offset 21 will also commit offsets 20 and below. Even after you re-start the application, this will not be detected.\nTo prevent this… You can either:\nHalt the consumer process after detecting the first failure. In this example, we do this by exiting our consumer thread itself (this is not acceptable for real-world applications though). When you restart the application, the processing will begin from the last committed offset and the failed message will be picked up and re-processed. Even better - You can handle this in commit_callback by sending this data to another Kafka topic (also know as a “dead letter topic”) which can be processed separately. ","what-about-consumer-callbacks#What about Consumer callbacks?":"Yes, just like the producer, the consumer API also has callbacks for:\nRe-balancing Offset commit To do this, we need to implement the ConsumerContext trait. We will:\nDefine a struct Provide an empty implementation for ClientContext Override the following methods from ConsumerContext trait - pre_rebalance, post_rebalance, commit_callback struct ConsumerCallbackLogger; impl ClientContext for ConsumerCallbackLogger {} impl ConsumerContext for ConsumerCallbackLogger { ... } We will skip the pre_rebalance method and focus on post_rebalance in this example:\nfn post_rebalance\u003c'a\u003e(\u0026self, rebalance: \u0026rdkafka::consumer::Rebalance\u003c'a\u003e) { println!(\"post_rebalance callback\"); match rebalance { Rebalance::Assign(tpl) =\u003e { for e in tpl.elements() { println!(\"rebalanced partition {}\", e.partition()) } } Rebalance::Revoke =\u003e { println!(\"ALL partitions are REVOKED\") } Rebalance::Error(err_info) =\u003e { println!(\"Post Rebalance error {}\", err_info) } } } Rebalance is an enum. As a part of the implementation, we match it against all the possible options (partitions assigned, partitions revoked, rebalance error) and simply log it.\nfn commit_callback( \u0026self, result: rdkafka::error::KafkaResult\u003c()\u003e, offsets: \u0026rdkafka::TopicPartitionList, ) { match result { Ok(_) =\u003e { for e in offsets.elements() { println!( \"committed offset {:?} in partition {}\", e.offset(), e.partition() ) } } Err(err) =\u003e { println!(\"error committing offset - {}\", err) } } } For commit callback events, we match on the KafkaResult (available in the commit_callback parameter) to check whether the commit was successful. If it was, we simply print out the committed offset in the partition or log the error that occurred during the commit process.\nOnce this is done, we simply need to plug-in our new implementation:\nlet consumer: BaseConsumer\u003cConsumerCallbackLogger\u003e = ClientConfig::new() .set(\"bootstrap.servers\", \"localhost:9092\",) .... .create_with_context(ConsumerCallbackLogger {}) .expect(\"invalid consumer config\"); To do this, we made a couple of changes:\nUse create_with_context (instead of create) use BaseConsumer\u003cConsumerCallbackLogger\u003e Run the program Rename the file src/2_consumer_callback.rs to main.rs, and execute cargo run sending message sending message produced message with key user-1 in offset 0 of partition 2 post_rebalance callback rebalanced partition 0 rebalanced partition 1 rebalanced partition 2 rebalanced partition 3 rebalanced partition 4 rebalanced partition 5 produced message with key user-2 in offset 0 of partition 4 sending message produced message with key user-3 in offset 0 of partition 0 received key user-3 with value User { id: 3, email: \"user-3@foobar.com\" } in offset 0 from partition 0 sending message committed offset Offset(1) in partition 0 committed offset Offset(1) in partition 4 produced message with key user-4 in offset 0 of partition 3 received key user-4 with value User { id: 4, email: \"user-4@foobar.com\" } in offset 0 from partition 3 As expected, the re-balance events were logged along with the successful commits.\nTrigger a Re-balance Partition assignment happens the first time when you start the application and you’re able to witness this, thanks to our ConsumerContext implementation. You can also trigger the rebalance again by starting the new instance of the application. Since there are two instances in the same consumer group, the topic partitions will be rebalanced. For e.g. if you had 6 partitions in the topic, they will be equally split up amongst these two instances.\nYou should see log messages similar to this:\n.... # instance 1 post_rebalance callback rebalanced partition 0 rebalanced partition 1 rebalanced partition 2 ... # instance 2 post_rebalance callback rebalanced partition 3 rebalanced partition 4 rebalanced partition 5 "},"title":"Getting started with Kafka and Rust: Part 2"},"/blog/rust-redis-getting-started/":{"data":{"":"Are you learning Rust and looking for ways to get some hands-on practice with concrete examples? A good approach might be to try and integrate Rust with external systems. Why not try to use it with Redis? It is a powerful, versatile database but dead simple to get started with!\nIn this blog post, you will learn how to use the Rust programming language to interact with Redis using the redis-rs client. We will walk through commonly used Redis data structures such as String, Hash, List etc. The Redis client used in the sample code exposes both high and low-level APIs and you will see both these styles in action.\nCode is available on GitHub - https://github.com/abhirockzz/rust-redis-101\nRedis is an in-memory data structure store which is often used as a database, cache, and message broker. It provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams.\nIn this blog post, I have included instructions for Azure Cache for Redis, but the sample application should work with any Redis instance. Azure Cache for Redis offers both the Redis open-source and a commercial product from Redis Labs as a managed service.","code-walk-through#Code walk through":"For you to get a better understanding, this section covers a step-by-step walk through of the code. It covers all the functions, each of which covers a specific Redis data structure. It is followed by the Run the sample application section.\nConnect to Redis The connect function is used to establish a connection to Redis. It expects host name and the password to be passed in as environment variables REDIS_HOSTNAME and REDIS_PASSWORD respectively. The format for the connection URL is \u003curi scheme\u003e://\u003cusername\u003e:\u003cpassword\u003e@\u003chostname\u003e.\nAzure Cache for Redis only accepts secure connections with TLS 1.2 as the minimum required version. The URI scheme would be rediss in case of a TLS connection, otherwise it’s redis.\nThe call to redis::Client::open performs basic validation while get_connection() actually initiates the connection - the program exits if the connectivity fails due to any reason such as an incorrect password.\nfn connect() -\u003e redis::Connection { //format - host:port let redis_host_name = env::var(\"REDIS_HOSTNAME\").expect(\"missing environment variable REDIS_HOSTNAME\"); let redis_password = env::var(\"REDIS_PASSWORD\").unwrap_or_default(); //if Redis server needs secure connection let uri_scheme = match env::var(\"IS_TLS\") { Ok(_) =\u003e \"rediss\", Err(_) =\u003e \"redis\", }; let redis_conn_url = format!(\"{}://:{}@{}\", uri_scheme, redis_password, redis_host_name); redis::Client::open(redis_conn_url) .expect(\"Invalid connection URL\") .get_connection() .expect(\"failed to connect to Redis\") } Basic operations on Strings and Integers This function covers SET, GET, and INCR commands. The low-level API is used for SET and GET, which sets and retrieves the value for a key named foo. The INCRBY command is executed using a high-level API i.e. incr increments the value of a key (named counter) by 2 followed by a call to get to retrieve it.\nfn basics() { let mut conn = connect(); let _: () = redis::cmd(\"SET\") .arg(\"foo\") .arg(\"bar\") .query(\u0026mut conn) .expect(\"failed to execute SET for 'foo'\"); let bar: String = redis::cmd(\"GET\") .arg(\"foo\") .query(\u0026mut conn) .expect(\"failed to execute GET for 'foo'\"); println!(\"value for 'foo' = {}\", bar); let _: () = conn .incr(\"counter\", 2) .expect(\"failed to execute INCR for 'counter'\"); let val: i32 = conn .get(\"counter\") .expect(\"failed to execute GET for 'counter'\"); println!(\"counter = {}\", val); } How to use a Hash data structure? The below code snippet demonstrates the functionality of a Redis HASH data structure. HSET is invoked using the low-level API to store information (name, version, repo) about Redis drivers (clients). For example, details for the Rust driver (one being used in this sample code!) is captured in form of a BTreeMap and then passed on to the low-level API. It is then retrieved using HGETALL.\nHSET can also be executed using a high-level API using hset_multiple that accepts an array of tuples. hget is then executed to fetch the value for a single attribute (the repo in this case).\nfn hash() { let mut conn = connect(); let mut driver: BTreeMap\u003cString, String\u003e = BTreeMap::new(); let prefix = \"redis-driver\"; driver.insert(String::from(\"name\"), String::from(\"redis-rs\")); driver.insert(String::from(\"version\"), String::from(\"0.19.0\")); driver.insert( String::from(\"repo\"), String::from(\"https://github.com/mitsuhiko/redis-rs\"), ); let _: () = redis::cmd(\"HSET\") .arg(format!(\"{}:{}\", prefix, \"rust\")) .arg(driver) .query(\u0026mut conn) .expect(\"failed to execute HSET\"); let info: BTreeMap\u003cString, String\u003e = redis::cmd(\"HGETALL\") .arg(format!(\"{}:{}\", prefix, \"rust\")) .query(\u0026mut conn) .expect(\"failed to execute HGETALL\"); println!(\"info for rust redis driver: {:?}\", info); let _: () = conn .hset_multiple( format!(\"{}:{}\", prefix, \"go\"), \u0026[ (\"name\", \"go-redis\"), (\"version\", \"8.4.6\"), (\"repo\", \"https://github.com/go-redis/redis\"), ], ) .expect(\"failed to execute HSET\"); let repo_name: String = conn .hget(format!(\"{}:{}\", prefix, \"go\"), \"repo\") .expect(\"HGET failed\"); println!(\"go redis driver repo name: {:?}\", repo_name); } Using Redis Lists In the function below, you can see how to use a LIST data structure. LPUSH is executed (with the low-level API) to add an entry to the list and the high-level lpop method is used to retrieve that from the list. Then, the rpush method is used to add a couple of entries to the list which are then fetched using the low-level lrange method.\nfn list() { let mut conn = connect(); let list_name = \"items\"; let _: () = redis::cmd(\"LPUSH\") .arg(list_name) .arg(\"item-1\") .query(\u0026mut conn) .expect(\"failed to execute LPUSH for 'items'\"); let item: String = conn .lpop(list_name) .expect(\"failed to execute LPOP for 'items'\"); println!(\"first item: {}\", item); let _: () = conn.rpush(list_name, \"item-2\").expect(\"RPUSH failed\"); let _: () = conn.rpush(list_name, \"item-3\").expect(\"RPUSH failed\"); let len: isize = conn .llen(list_name) .expect(\"failed to execute LLEN for 'items'\"); println!(\"no. of items in list = {}\", len); let items: Vec\u003cString\u003e = conn .lrange(list_name, 0, len - 1) .expect(\"failed to execute LRANGE for 'items'\"); println!(\"listing items in list\"); for item in items { println!(\"item: {}\", item) } } Store unique items in a Redis Set Here you can see some of the SET operations. The sadd (high-level API) method is used to add couple of entries to a SET named users. SISMEMBER is then executed (low-level API) to check whether user1 exists. Finally, smembers is used to fetch and iterate over all the set entries in the form of a Vector (Vec).\nfn set() { let mut conn = connect(); let set_name = \"users\"; let _: () = conn .sadd(set_name, \"user1\") .expect(\"failed to execute SADD for 'users'\"); let _: () = conn .sadd(set_name, \"user2\") .expect(\"failed to execute SADD for 'users'\"); let ismember: bool = redis::cmd(\"SISMEMBER\") .arg(set_name) .arg(\"user1\") .query(\u0026mut conn) .expect(\"failed to execute SISMEMBER for 'users'\"); println!(\"does user1 exist in the set? {}\", ismember); let users: Vec\u003cString\u003e = conn.smembers(set_name).expect(\"failed to execute SMEMBERS\"); println!(\"listing users in set\"); for user in users { println!(\"user: {}\", user) } } Take advantage of Redis Sorted Sets sorted_set function below demonstrates the Sorted Set data structure. ZADD is invoked (with the low-level API) to add a random integer score for a player (player-1). Next, the zadd method (high-level API) is used to add more players (player-2 to player-5) and their respective (randomly generated) scores. The number of entries in the sorted set is figured out using ZCARD and that’s used as the limit to the ZRANGE command (invoked with the low-level API) to list out the players with their scores in ascending order.\nfn sorted_set() { let mut conn = connect(); let sorted_set = \"leaderboard\"; let _: () = redis::cmd(\"ZADD\") .arg(sorted_set) .arg(rand::thread_rng().gen_range(1..10)) .arg(\"player-1\") .query(\u0026mut conn) .expect(\"failed to execute ZADD for 'leaderboard'\"); for num in 2..=5 { let _: () = conn .zadd( sorted_set, String::from(\"player-\") + \u0026num.to_string(), rand::thread_rng().gen_range(1..10), ) .expect(\"failed to execute ZADD for 'leaderboard'\"); } let count: isize = conn .zcard(sorted_set) .expect(\"failed to execute ZCARD for 'leaderboard'\"); let leaderboard: Vec\u003c(String, isize)\u003e = conn .zrange_withscores(sorted_set, 0, count - 1) .expect(\"ZRANGE failed\"); println!(\"listing players and scores in ascending order\"); for item in leaderboard { println!(\"{} = {}\", item.0, item.1) } } Now that you’ve gone through the code, it’s time to run the application and check the output.","conclusion#Conclusion":"In this tutorial, you learned how to use the Rust driver for Redis to connect and execute operations in Azure Cache for Redis. If you found this helpful, you may want to explore these additional resources:\nAn introduction to Redis data types and abstractions Read The Rust Book Try small exercises to get you used to reading and writing Rust code Considerations for designing and using a cache Best practices for Azure Cache for Redis High availability for Azure Cache for Redis ","pre-requisites#Pre-requisites":"You will need Rust (version 1.39 or above) installed on your computer. If you intend to use Azure Cache for Redis, simply create a free Azure subscription, and setup an Azure Cache for Redis instance using the Azure portal.\nFor the purposes of this tutorial, I would recommend setting up a Basic tier instance which is ideal for development/test and non-critical workloads.\nYou can also choose to simply use the Redis Docker container as such:\ndocker run --rm -p 6379:6379 redis That’s it, you’re ready to get started!","run-the-sample-application#Run the sample application":"Start by cloning the GitHub repo:\ngit clone https://github.com/abhirockzz/rust-redis-101.git cd rust-redis-101 If you’re using a local Redis server (e.g. with Docker) over an insecure connection without any password, simply use:\nexport REDIS_HOSTNAME=localhost:6379 If you’re using Azure Cache for Redis, fetch the Host name and Access Keys from the Azure portal. Set the respective environment variables:\nexport REDIS_HOSTNAME=\u003cHost name\u003e:\u003cport\u003e (e.g. \u003cname of cache\u003e.redis.cache.windows.net:6380) export REDIS_PASSWORD=\u003cPrimary Access Key\u003e export IS_TLS=true To run the application:\ncargo run You will see an output as such:\n******* Running SET, GET, INCR commands ******* value for 'foo' = bar counter = 29 ******* Running HASH commands ******* info for rust redis driver: {\"name\": \"redis-rs\", \"repo\": \"https://github.com/mitsuhiko/redis-rs\", \"version\": \"0.19.0\"} go redis driver repo name: \"https://github.com/go-redis/redis\" ******* Running LIST commands ******* first item: item-1 no. of items in list = 2 listing items in list item: item-2 item: item-3 ******* Running SET commands ******* does user1 exist in the set? true listing users in set user: user2 user: user1 user: user3 ******* Running SORTED SET commands ******* listing players and scores player-4 = 3 player-3 = 7 player-1 = 8 player-2 = 8 player-5 = 8 Important: If you were using Azure Cache for Redis, please ensure that you delete the Redis instance."},"title":"Getting started with Rust and Redis"},"/blog/scaling-multi-tenant-go-applications-choosing-the-right-database-partitioning-approach/":{"data":{"":"\nMulti-tenant applications face a fundamental challenge: how to efficiently store and query data for tenants of vastly different sizes? Consider the typical scenario where your platform serves both enterprise clients with hundreds of thousands of users, as well as small businesses with just a handful. With traditional database partitioning strategies you are likely to run into these common issues:\nPartition imbalance: Large tenants create oversized partitions while small tenants waste allocated resources Hot partitions: High-activity tenants overwhelm individual database partitions, creating performance bottlenecks Inefficient queries: User-specific lookups require scanning entire tenant datasets Resource contention: Mixed workloads compete for the same database resources Azure Cosmos DB has been a go-to solution for multi-tenant applications due to its global distribution, automatic scaling, and flexible data models. Its partition-based architecture naturally aligns with tenant isolation requirements, making it attractive for SaaS platforms, IoT applications, and content management systems.\nHowever, even with these capabilities, the fundamental multi-tenant partitioning challenges persist. Let’s examine how these issues manifest specifically in a Cosmos DB context.\nThis blog post explores an approach to solving multi-tenant scaling challenges in Go applications using Azure Cosmos DB. You’ll learn how to implement this using the Go SDK for Azure Cosmos DB, focusing on how to achieve efficient data distribution and query performance.\nCheck the GitHub repository for the code examples used in this blog","challenges-with-a-multi-tenant-saas-solution#Challenges with a multi-tenant SaaS solution":"Imagine you’re building a multi-tenant SaaS platform that manages user sessions and activities across different organizations using Cosmos DB. In such a setup, tenant variability is a significant challenge. Enterprise clients may have over 50,000 users generating millions of session events, while small businesses might only have 10 to 50 users with minimal activity. Mid-market companies typically fall in between, with 500 to 5,000 users and moderate usage. This wide range of tenant sizes and activity levels creates unique challenges for data partitioning and resource allocation in the database.\nThis is how you might define your user session data model using a single partition key:\ntype UserSession struct { ID string `json:\"id\"` TenantID string `json:\"tenantId\"` // Single partition key UserID string `json:\"userId\"` SessionID string `json:\"sessionId\"` Activity string `json:\"activity\"` Timestamp time.Time `json:\"timestamp\"` } This approach has several challenges. First, partition size imbalance occurs as enterprise tenants generate massive 20GB+ partitions, while small tenants use minimal storage, resulting in uneven resource utilization across physical partitions. Second, hot partition bottlenecks can develop when large tenants reach the 10,000 RU/s physical partition limit during peak usage periods. Third, user queries become inefficient because looking up individual user sessions requires scanning entire tenant partitions, which consumes unnecessary Request Units. Cross-tenant analytics also suffer, as queries spanning multiple tenants become expensive cross-partition operations.","conclusion#Conclusion":"Multi-tenant applications face inherent scaling challenges with traditional single-level partitioning: tenant size variability, hot partitions, and inefficient query patterns that impact both performance and cost. Hierarchical partition keys in Azure Cosmos DB address these issues by enabling intelligent data distribution across multiple partition levels, maintaining tenant isolation while achieving better resource utilization. By aligning your partition strategy with actual access patterns, you can build applications that scale naturally with tenant growth while maintaining predictable performance characteristics.\nCheck out the documentation for the azcosmos package (Go SDK). For more information on hierarchical partition keys, refer to the official documentation.","hierarchical-partition-keys-in-action-with-the-go-sdk-for-azure-cosmos-db#Hierarchical Partition Keys in action with the Go SDK for Azure Cosmos DB":"To explore the concepts, we will use a Go application that loads sample user session data into Azure Cosmos DB and queries it using the hierarchical partition keys.\nLoad data into Cosmos DB Run the loader to populate the database with sample data that uses hierarchical partition keys. Its a CLI application that generates user session data for users in different tenant types (Enterprise, Mid-market, Small business) and inserts it into the Cosmos DB container.\nClone the repository and change into the load directory:\ngit clone https://github.com/abhirockzz/cosmosdb-go-hierarchical-partition-keys cd cosmosdb-go-hierarchical-partition-keys/load Build the data loader application and run it. The database and container will be created automatically if they do not exist.\ngo build -o data-loader main.go ./data-loader -rows 100 -database \u003cinsert database name\u003e -container \u003cinsert container name\u003e -endpoint \"https://your-account.documents.azure.com:443/\" Here is how the container is created with hierarchical partition keys:\n//... partitionKeyDef := azcosmos.PartitionKeyDefinition{ Kind: azcosmos.PartitionKeyKindMultiHash, Version: 2, // Version 2 is required for hierarchical partition keys Paths: []string{ \"/tenantId\", // Level 1: Tenant isolation \"/userId\", // Level 2: User distribution \"/sessionId\", // Level 3: Session granularity }, } // Create container properties containerProperties := azcosmos.ContainerProperties{ ID: containerName, PartitionKeyDefinition: partitionKeyDef, } // Create container with 400 RU/s throughput throughputProperties := azcosmos.NewManualThroughputProperties(400) _, err = databaseClient.CreateContainer(ctx, containerProperties, \u0026azcosmos.CreateContainerOptions{ ThroughputProperties: \u0026throughputProperties, }) //.... … and this is how sample data is added:\n// Create hierarchical partition key (TenantID, UserID, SessionID) partitionKey := azcosmos.NewPartitionKeyString(session.TenantID). AppendString(session.UserID). AppendString(session.SessionID) // Insert the record using UpsertItem (insert or update if exists) _, err = containerClient.UpsertItem(ctx, partitionKey, sessionJSON, nil) Lets dive into the queries that demonstrate how to retrieve data using hierarchical partition keys.\nQuery patterns Let’s examine how different query patterns perform with hierarchical partition keys. To execute these examples, you can comment out the relevant sections in the main function of the query/main.go file, set the required environment variables, and run the application.\nexport COSMOS_DB_ENDPOINT=https://your-account.documents.azure.com:443/ export COSMOS_DB_DATABASE_NAME=\u003cinsert database name\u003e export COSMOS_DB_CONTAINER_NAME=\u003cinsert container name\u003e cd cosmosdb-go-hierarchical-partition-keys/query go run main.go 1. Point Read (Most Efficient) This is the most efficient query type, where you retrieve a single item using its unique ID and full partition key path. This avoids any cross-partition overhead.\nTake a look at the executePointRead function that performs a point read operation:\nfunc executePointRead(id, tenantId, userId, sessionId string) { // Create a partition key using the full partition key values pk := azcosmos.NewPartitionKeyString(tenantId).AppendString(userId).AppendString(sessionId) // Perform a point read operation resp, err := container.ReadItem(context.Background(), pk, id, nil) if err != nil { log.Fatalf(\"Failed to read item: %v\", err) } var queryResult QueryResult err = json.Unmarshal(resp.Value, \u0026queryResult) //..... } 2. Session-Specific Data This is routed to the single logical and physical partition that contains the data for the specified values of tenantId, tenantId, and sessionId.\nfunc queryWithFullPartitionKey(tenantID, userID, sessionID string) { query := \"SELECT * FROM c WHERE c.tenantId = @tenantId AND c.userId = @userId AND c.sessionId = @sessionId\" pkFull := azcosmos.NewPartitionKeyString(tenantID).AppendString(userID).AppendString(sessionID) pager := container.NewQueryItemsPager(query, pkFull, \u0026azcosmos.QueryOptions{ QueryParameters: []azcosmos.QueryParameter{ {Name: \"@tenantId\", Value: tenantID}, {Name: \"@userId\", Value: userID}, {Name: \"@sessionId\", Value: sessionID}, }, }) for pager.More() { page, err := pager.NextPage(context.Background()) if err != nil { log.Fatal(err) } for _, _item := range page.Items { var queryResult QueryResult err = json.Unmarshal(_item, \u0026queryResult) // log the results } } } 3. User-Specific Data (Targeted Cross-Partition) This query is a targeted cross-partition query that returns data for a specific user in the tenant and routed to specific subset of logical and physical partition(s) that contain data for the specified values of tenantId and userId.\nfunc queryWithTenantAndUserID(tenantID, userID string) { query := \"SELECT * FROM c WHERE c.tenantId = @tenantId AND c.userId = @userId\" // since we don't have the full partition key, we use an empty partition key emptyPartitionKey := azcosmos.NewPartitionKey() pager := container.NewQueryItemsPager(query, emptyPartitionKey, \u0026azcosmos.QueryOptions{ QueryParameters: []azcosmos.QueryParameter{ {Name: \"@tenantId\", Value: tenantID}, {Name: \"@userId\", Value: userID}, }, }) for pager.More() { page, err := pager.NextPage(context.Background()) if err != nil { log.Fatal(err) } fmt.Println(\"==========================================\") for _, _item := range page.Items { var queryResult QueryResult err = json.Unmarshal(_item, \u0026queryResult) // log the results } } } 4. Tenant-Wide Data (Efficient Cross-Partition) This query is a targeted cross-partition query that returns data for all users in a tenant and routed to a specific subset of logical and physical partition(s) that contain data for the specified value of tenantId.\nThe queryWithSinglePKParameter is a function that lets you query with a single partition key parameter - this can be either tenantId, userId, or sessionId.\nfunc queryWithSinglePKParameter(paramType, paramValue string) { if paramType != \"tenantId\" \u0026\u0026 paramType != \"userId\" \u0026\u0026 paramType != \"sessionId\" { log.Fatalf(\"Invalid parameter type: %s\", paramType) } query := fmt.Sprintf(\"SELECT * FROM c WHERE c.%s = @param\", paramType) emptyPartitionKey := azcosmos.NewPartitionKey() pager := container.NewQueryItemsPager(query, emptyPartitionKey, \u0026azcosmos.QueryOptions{ QueryParameters: []azcosmos.QueryParameter{ {Name: \"@param\", Value: paramValue}, }, }) for pager.More() { page, err := pager.NextPage(context.Background()) if err != nil { log.Fatal(err) } fmt.Printf(\"Results for %s: %s\\n\", paramType, paramValue) fmt.Println(\"==========================================\") for _, _item := range page.Items { var queryResult QueryResult err = json.Unmarshal(_item, \u0026queryResult) // log the results } } } 5. User or Session Across All Tenants (Fan-Out) Both types of queries will be routed to all physical partitions, resulting in a fan-out cross-partition query.\nSELECT * FROM c WHERE c.userId = 'user-1001' SELECT * FROM c WHERE c.sessionId = 'session-abc123' This type of query is not efficient and should be avoided in production scenarios. It is included here for completeness, but you should design your application to avoid such queries whenever possible.","hierarchical-partition-keys-to-the-rescue#Hierarchical Partition Keys to the rescue":"Hierarchical partition keys (HPKs) help implement subpartitioning that allows you to define up to three levels of partition key hierarchy. This leads to better data distribution and query routing compared to traditional single-level partitioning. Instead of forcing all tenant data into a single partition boundary, you are able to create logical subdivisions that align with your actual access patterns.\nMapping this to the the multi-tenant solution challenges, hierarchical partition keys allow you to define a three-level partitioning scheme:\nLevel 1: Primary partition key (e.g., tenantId) - provides tenant isolation Level 2: Secondary partition key (e.g., userId) - distributes data within tenants Level 3: Tertiary partition key (e.g., sessionId) - provides fine-grained distribution This creates a logical partition path like instead of just [\"Enterprise-Corp\"]. Large tenants can be subdivided by user and session, eliminating hot partitions. Instead of one massive “Enterprise-Corp” partition, you get manageable partitions like: [\"Enterprise-Corp\", \"user-1001\", \"session-abc123\"], [\"Enterprise-Corp\", \"user-1002\", \"session-def456\"], etc.\nNow, we can refactor the user session data model as such:\ntype UserSession struct { ID string `json:\"id\"` TenantID string `json:\"tenantId\"` // Level 1: Tenant isolation UserID string `json:\"userId\"` // Level 2: User distribution SessionID string `json:\"sessionId\"` // Level 3: Session granularity Activity string `json:\"activity\"` Timestamp time.Time `json:\"timestamp\"` } Your queries can now be efficiently routed to only the subset of physical partitions that contain the relevant data. Specifying the full or partial subpartitioned partition key path effectively avoids a cross-partition query across all the parititions, which is a common problem with single partition keys.\nSession details: WHERE tenantId = 'Enterprise-Corp' AND userId = 'user-1001' AND sessionId = 'session-abc123' provides single-partition access User-specific queries: WHERE tenantId = 'Enterprise-Corp' AND userId = 'user-1001' pinpoints exact data location Tenant-wide queries: WHERE tenantId = 'Enterprise-Corp' only targets relevant partitions Each logical partition (tenant-user-session combination) can scale independently, allowing tenant data to exceed the traditional 20GB limit and maintain optimal performance. Targeted queries consume fewer Request Units by avoiding unnecessary cross-partition scans, directly reducing operational expenses."},"title":"Scaling multi-tenant Go applications: Choosing the right database partitioning approach"},"/blog/serverless-talk-oracle-code/":{"data":{"":"This talk covers a bunch of common Serverless use cases like Backend APIs, Real-Time systems, Integration and Automation (uses Fn and Oracle Functions as the serverless platform).\nThe GitHub repo for the talk contains the ppt and links to other demos (source code etc.) - https://github.com/abhirockzz/oraclecode2019\nCheers!"},"title":"Serverless Architectures and Patterns"},"/blog/serverless-tensorflow-image-classification/":{"data":{"":"Image classification is a canonical example used to demonstrate machine learning techniques. This post shows you how to run a TensorFlow-based image classification application on the recently announced cloud service Oracle Functions.\nPhoto by Franck V. on Unsplash","deploying-to-oracle-functions#Deploying to Oracle Functions":"As mentioned previously, you can use the open source Fn CLI to deploy to Oracle Functions. Ensure that you have the latest version.\ncurl -LSs [https://raw.githubusercontent.com/fnproject/cli/master/install](https://raw.githubusercontent.com/fnproject/cli/master/install) | sh ​You can also download it directly from https://github.com/fnproject/cli/releases.\nOracle Functions Context Before using Oracle Functions, you have to configure the Fn Project CLI to connect to your Oracle Cloud Infrastructure tenancy.\nWhen the Fn Project CLI is initially installed, it’s configured for a local development context. To configure the Fn Project CLI to connect to your Oracle Cloud Infrastructure tenancy instead, you have to create a new context. The context information is stored in a .yaml file in the ~/.fn/contexts directory. It specifies Oracle Functions endpoints, the OCID of the compartment to which deployed functions belong, the Oracle Cloud Infrastructure configuration file, and the address of the Docker registry to push images to and pull images from.\nThis is what a context file looks like:\napi-url: [https://functions.us-phoenix-1.oraclecloud.com](https://functions.us-phoenix-1.oraclecloud.com) oracle.compartment-id: \u003cOCI_compartment_OCID\u003e oracle.profile: \u003cprofile_name_in_OCI_config\u003e provider: oracle registry: \u003cOCI_docker_registry\u003e Oracle Cloud Infrastructure Configuration The Oracle Cloud Infrastructure configuration file contains information about user credentials and the tenancy OCID. You can create multiple profiles with different values for these entries. Then, you can define the profile to be used by the CLI by using the oracle.profile attribute.\nHere is an example configuration file:\n[DEFAULT] user=ocid1.user.oc1..exampleuniqueID fingerprint=20:3b:97:13:55:1c:5b:0d:d3:37:d8:50:4e:c5:3a:34 key_file=~/.oci/oci_api_key.pem tenancy=ocid1.tenancy.oc1..exampleuniqueID pass_phrase=tops3cr3t region=us-ashburn-1 [ORACLE_FUNCTIONS_USER] user=ocid1.user.oc1..exampleuniqueID fingerprint=72:00:22:7f:d3:8b:47:a4:58:05:b8:95:84:31:dd:0e key_file=/.oci/admin_key.pem tenancy=ocid1.tenancy.oc1..exampleuniqueID pass_phrase=s3cr3t region=us-phoenix-1 You can define multiple contexts, each stored in a different context file. Switch to the correct context according to your Functions development environment:\nfn use context \u003ccontext_name\u003e Create the Application Start by cloning the contents of the GitHub repository:\ngit clone [https://github.com/abhirockzz/fn-hello-tensorflow](https://github.com/abhirockzz/fn-hello-tensorflow) Here is the command required to deploy an application:\nfn create app \u003capp_name\u003e --annotation oracle.com/oci/subnetIds='[\"\u003csubnet_ocid\u003e\"]' \u003capp_name\u003e is the name of the new application.\n\u003csubnet_ocid\u003e is the OCID of the subnet in which to run your function.\nFor example:\nfn create app fn-tensorflow-app --annotation oracle.com/oci/subnetIds='[\"ocid1.subnet.oc1.phx.exampleuniqueID\",\"ocid1.subnet.oc1.phx.exampleuniqueID\",\"ocid1.subnet.oc1.phx.exampleuniqueID\"]' Deploy the Function After you create the application, you can deploy your function with the following command:\nfn deploy --app \u003capp_name\u003e \u003capp_name\u003e is the name of the application in Oracle Functions to which you want to add the function.\nIf you want to use TensorFlow version 1.12.0 (for Java SDK and corresponding native libraries), use the following command:\nfn -v deploy --app fn-tensorflow-app You can also choose a specific version. Ensure that you specify it in pom.xml file before you build the function. For example, if you want to use version 1.11.0:\n\u003cdependency\u003e \u003cgroupId\u003eorg.tensorflow\u003c/groupId\u003e \u003cartifactId\u003etensorflow\u003c/artifactId\u003e **\u003cversion\u003e1.11.0\u003c/version\u003e** \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e To specify the version during function deployment, you can use — build-arg (build argument) as follows:\nfn -v deploy --app fn-tensorflow-app --build-arg TENSORFLOW_VERSION=\u003cversion\u003e For example, if you want to use 1.11.0:\nfn -v deploy --app fn-tensorflow-app **--build-arg TENSORFLOW_VERSION=1.11.0** When the deployment completes successfully, your function is ready to use. Use the fn ls apps command to list down the applications currently deployed. fn-tensorflow-app should be listed.","oracle-functions#Oracle Functions":"Oracle Functions which is a fully managed, highly scalable, on-demand, function-as-a-service platform built on enterprise-grade Oracle Cloud Infrastructure. It’s a serverless offering that enables you to focus on writing code to meet business needs without worrying about the underlying infrastructure, and get billed only for the resources consumed during the execution. You can deploy your code and call it directly or in response to triggers — Oracle Functions does all the work required to ensure that your application is highly available, scalable, secure, and monitored.\nOracle Functions is powered by the Fn Project, which is an open source, container native, serverless platform that can be run anywhere — in any cloud or on-premises. You can download and install the open source distribution of Fn Project, develop and test a function locally, and then use the same tooling to deploy that function to Oracle Functions.","summary#Summary":"We just deployed a simple yet fully functional machine learning application in the cloud! Eager to try this out?\nOracle Functions will be generally available in 2019, but we are currently providing access to selected customers through our Cloud Native Limited Availability Program. To learn more about Oracle Functions or to request access, please register. You can also learn more about the underlying open source technology used in Oracle Functions at FnProject.io.\nOriginally published at blogs.oracle.com on January 7, 2019.","the-code#The Code":"The image classification function is based on an existing TensorFlow example. It leverages the TensorFlow Java SDK, which in turn uses the native C++ implementation using JNI (Java Native Interface).\nFunction Image Input The image classification function leverages the Fn Java FDK, which simplifies the process of developing and running Java functions. One of its benefits is that it can seamlessly convert the input sent to your functions into Java objects and types. This includes:\nSimple data binding, like handling string input.\nBinding JSON data types to POJOs. You can customize this because it’s internally implemented using Jackson.\nWorking with raw inputs, enabled by an abstraction of the raw Fn Java FDK events received or returned by the function.\nThe binding can be further extended if you want to customize the way your input and output data is marshaled.\nThe existing TensorFlow example expects a list of image names (which must be present on the machine from which the code is being executed) as input. The function behaves similarly, but with an important difference — it uses the flexible binding capability provided by the Fn Java FDK. The classify method serves as the entry point to the function and accepts a Java byte array (byte[]), which represents the raw bytes of the image that is passed into the function. This byte array is then used to create the Tensor object using the static Tensor.create(byte[]) method:\npublic class LabelImageFunction { public String classify(byte[] **image**) { ... Tensor\u003cString\u003e input = Tensors.create(**image**); ... } } The full source code is available on GitHub.\nMachine Learning Model Typically, a machine-learning-based system consists of the following phases:\nTraining: An algorithm is fed with past (historical) data in order to learn from it (derive patterns) and build a model. Very often, this process is ongoing.\nPredicting: The generated model is then used to generate predictions or outputs in response to new inputs based on the facts that were learned during the training phase\nThis application uses a pregenerated model. As an added convenience, the model (and labels) required by the classification logic are packaged with the function itself (part of the Docker image). These can be found in the resources folder of the source code.\nThis means that you don’t have to set up a dedicated model serving component (like TensorFlow Serving).\nFunction Metadata The func.yaml file contains function metadata, including attributes like memory and timeout (for this function, they are 1024 MB and 120 seconds, respectively). This metadata is required because of the (fairly) demanding nature of the image classification algorithm (as opposed to simpler computations).\nschema_version: 20180708 name: classify version: 0.0.1 runtime: java memory: 1024 timeout: 120 triggers: - name: classify type: http source: /classify Here is a summary of the attributes used:\nschema_version represents the version of the specification for this file.\nname is the name and tag to which this function is pushed.\nversion represents the current version of the function. When deploying, it is appended to the image as a tag.\nruntime represents the programming language runtime, which is java in this case.\nmemory (optional) is the maximum memory threshold for this function. If this function exceeds this limit during execution, it’s stopped and an error message is logged.\ntimeout (optional) is the maximum time that a function is allowed to run.\ntriggers (optional) is an array of trigger entities that specify triggers for the function. In this case, we’re using an HTTP trigger.\nFunction Dockerfile Oracle Functions uses a set of prebuilt, language-specific Docker images for build and runtime phases. For example, for Java functions, fn-java-fdk-build is used for the build phase and fn-java-fdk is used at runtime.\nHere is the default Dockerfile that is used to create Docker images for your functions:\nFROM fnproject/fn-java-fdk-build:jdk9-1.0.75 as build-stage WORKDIR /function ENV MAVEN_OPTS -Dhttp.proxyHost= -Dhttp.proxyPort= -Dhttps.proxyHost= -Dhttps.proxyPort= -Dhttp.nonProxyHosts= -Dmaven.repo.local=/usr/share/maven/ref/repository ADD pom.xml /function/pom.xml RUN [\"mvn\", \"package\", \"dependency:copy-dependencies\", \"-DincludeScope=runtime\", \"-DskipTests=true\", \"-Dmdep.prependGroupId=true\", \"-DoutputDirectory=target\", \"--fail-never\"] ADD src /function/src RUN [\"mvn\", \"package\"] FROM fnproject/fn-java-fdk:jdk9-1.0.75 WORKDIR /function COPY --from=build-stage /function/target/*.jar /function/app/ CMD [\"com.example.fn.HelloFunction::handleRequest\"] It’s a multiple-stage Docker build that performs the following actions (out-of-the-box):\nMaven package and build\nCopying (using COPY) the function JAR and dependencies to the runtime image\nSetting the command to be executed (using CMD) when the function container is spawned\nBut there are times when you need more control over the creation of the Docker image, for example, to incorporate native third-party libraries. In such cases, you want to use a custom Dockerfile. It’s powerful because it gives you the freedom to define the recipe for your function. All you need to do is extend from the base Docker images.\nFollowing is the Dockerfile used for this function:\nFROM fnproject/fn-java-fdk-build:jdk9-1.0.75 as build-stage WORKDIR /function ENV MAVEN_OPTS -Dhttp.proxyHost= -Dhttp.proxyPort= -Dhttps.proxyHost= -Dhttps.proxyPort= -Dhttp.nonProxyHosts= -Dmaven.repo.local=/usr/share/maven/ref/repository ADD pom.xml /function/pom.xml RUN [\"mvn\", \"package\", \"dependency:copy-dependencies\", \"-DincludeScope=runtime\", \"-DskipTests=true\", \"-Dmdep.prependGroupId=true\", \"-DoutputDirectory=target\", \"--fail-never\"]' ARG TENSORFLOW_VERSION=1.12.0 RUN echo \"using tensorflow version \" $TENSORFLOW_VERSION RUN curl -LJO [https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-$TENSORFLOW_VERSION.jar](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-$TENSORFLOW_VERSION.jar) RUN curl -LJO [https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-linux-x86_64-$TENSORFLOW_VERSION.tar.gz](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-linux-x86_64-$TENSORFLOW_VERSION.tar.gz) RUN tar -xvzf libtensorflow_jni-cpu-linux-x86_64-$TENSORFLOW_VERSION.tar.gz ADD src /function/src RUN [\"mvn\", \"package\"] FROM fnproject/fn-java-fdk:jdk9-1.0.75 ARG TENSORFLOW_VERSION=1.12.0 WORKDIR /function COPY --from=build-stage /function/libtensorflow_jni.so /function/runtime/lib COPY --from=build-stage /function/libtensorflow_framework.so /function/runtime/lib COPY --from=build-stage /function/libtensorflow-$TENSORFLOW_VERSION.jar /function/app/ COPY --from=build-stage /function/target/*.jar /function/app/ CMD [\"com.example.fn.LabelImageFunction::classify\"] Notice the additional customization that it incorporates, in addition to the default steps like Maven build:\nAutomates TensorFlow setup (per the instructions), extracts the TensorFlow Java SDK and the native JNI (.so) libraries\n(as part of the second stage of the Docker build) Copies the JNI libraries to /function/runtime/lib and the SDK JAR to /function/app so that they are available to the function at runtime","time-to-classify-images#Time to Classify Images!":"As mentioned earlier, the function can accept an image as input and tell you what it is, along with the percentage accuracy.\nYou can start by downloading some of the recommended images or use images that you already have on your computer. All you need to do is pass them to the function while invoking it:\ncat \u003cpath to image\u003e | fn invoke fn-tensorflow-app classify Ok, let’s try this. Can it detect the sombrero in this image?\ncat /Users/abhishek/manwithhat.jpg | fn invoke fn-tensorflow-app classify **“366 • 9 • Gringo” (CC BY-NC-ND 2.0) by Pragmagraphr**\nResult:\nThis is a ‘sombrero’ Accuracy — 92% How about a terrier?\ncat /Users/abhishek/terrier.jpg | fn invoke fn-tensorflow-app classify **“Terrier” (CC BY-NC 2.0) by No_Water**\nResult:\nThis is a 'West Highland white terrier' Accuracy - 88% What will you classify? :-)","what-to-expect#What to Expect":"Before we dive into the details, let’s see what you can expect from your serverless machine learning function. After it’s set up and running, you can point the app to images and it will return an estimate of what it thinks the image is, along with the accuracy of the estimate.\nFor example, when passed to the classification function, this image returned — This is a ‘pizza’ Accuracy — 100%.\nPhoto by Alan Hardman on Unsplash"},"title":"Serverless Image Classification with TensorFlow"},"/blog/twitter-leaderboard-app-redis-lambda-part1/":{"data":{"":"Hello and welcome 👋🏼 to this two-part blog series that uses a practical application to demonstrate how to integrate Redis with AWS Lambda. The first part (this one) covers the application - by the end of this blog, you should have the solution deployed, played around with it and in the process, have a good overview of the solution.\nThe second part is about the Infrastructure (IaaC to be specific) aspects - its mostly centred around AWS CDK along with some code walkthrough.\nOver the course of this blog series, you will learn:\nHow to use Lambda and Redis together - including VPC and other configuration to make things work How to use Lambda Function URL How to deploy your Lambda function as a Docker container (not a zip file) Use AWS CDK to create all the components of the solution - Infrastructure (VPC, subnets etc.), database as well as Lambda functions (this includes multiple stacks in the context of a single CDK app) I have used Go for the Lambda functions (aws-lambda-go) as well as infrastructure (with CKD Go support), but you should be easily able to apply the concepts to the programming language of your choice.\nAs always, the code is available on Github\nHere is a quick overview of the services involved in the solution:\nAmazon MemoryDB for Redis - It is a durable, in-memory database service that is compatible with Redis, thus empowering you to build applications using the same flexible and friendly Redis data structures, APIs, and commands that they already use today. Lambda Function URL is a relatively new feature (at the time of writing this blog) that provides dedicated HTTP(S) endpoint for your Lambda function. It is really useful when all you need is a single endpoint for your function (e.g. to serve as a webhook) and don’t want to setup and configure an API Gateway. AWS Cloud Development Kit (CDK) is all about IaaC (Infrastructure-as-code). It is a framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. You can choose from a list of supported programming languages (at the time of writing - TypeScript, JavaScript, Python, Java, C#/.Net, and Go (in developer preview)) to define your infrastructure components as code, just like you would with any other application! ","deploy-the-application---one-cdk-stack-at-a-time#Deploy the application - One CDK Stack at a time":"We will cover the details in the second part. For now, just know that the infrastructure part of this solution is comprised of three (CDK) Stacks (in the context of a single CDK App). Although its possible to deploy all of them together (with cdk deploy --all), we will do it one by one. This way, you can review what’s happening at each stage, introspect the components that have been created and better understand how everything is wired together.\nFirst, clone the Github repo:\ngit clone https://github.com/abhirockzz/twitter-leaderboard-app Starting with the infrastructure… The first stack deploys a VPC (and also subnets, NAT gateway etc.), a MemoryDB for Redis cluster and a few security groups.\nChoose a password of your choice for MemoryDB and export it as a environment variable (this is just for demonstration purposes - for production, you will have specific processes in place to handle sensitive info)\nBe mindful of the password requirements. From the documentation:\n“In particular, be aware of these user password constraints when using ACLs for MemoryDB:\nPasswords must be 16–128 printable characters. The following non-alphanumeric characters are not allowed: , \"” / @.\" Change to the correct folder and kick off the stack deployment:\nexport MEMORYDB_PASSWORD=\u003center a password e.g. P@ssw0rd12345678\u003e cd cdk # stack1 is the name of the stack - used for simplicity cdk deploy stack1 There is lots to be done. While CDK is hard at work for us, you need to wait patiently :) This is probably a good time to navigate to the CloudFormation in AWS console and see what’s going on behind the scenes.\nOnce the stack creation is complete, go the AWS console and check out your freshly minted VPC, MemoryDB cluster and other components!\nHere is the stack output for your reference:\nThe next two stacks deploy separate Lambda functions. Before moving on though, make sure you build the Go binaries for both these functions.\nGo, build!\nMove to the respective folders and just invoke go build for each function:\ncd tweet-ingest \u0026\u0026 GOOS=linux go build -o app cd leaderboard-function \u0026\u0026 GOOS=linux go build -o app To package the Lambda function as a Docker container, I used the Go:1.x base image. But, you can explore other options as well. During deployment (via cdk deploy), the Docker image is built locally, pushed to a private ECR registry and finally the Lambda function is created - all this, with a few lines of code!\nThe second stack - For the first Lambda function The function requires your Twitter API credentials (along with the MemoryDB password) - seed them as environment variables. Then, initiate stack creation:\nexport MEMORYDB_PASSWORD=\u003center the password you had previously chosen e.g. P@ssw0rd12345678\u003e export TWITTER_API_KEY=\u003center twitter API key\u003e export TWITTER_API_SECRET=\u003center twitter API secret\u003e export TWITTER_ACCESS_TOKEN=\u003center twitter access token\u003e export TWITTER_ACCESS_TOKEN_SECRET=\u003center twitter API access token secret\u003e # note the name of the stack is stack2 cdk deploy stack2 This one will be faster (compared to stack1), I promise!\nOnce the stack creation is complete, to make things work, there is one manual step required. Go to the AWS console, open the Lambda function (named tweet-ingest-function), that was just created, click Add Trigger and manually add the CloudWatch trigger configuration.\nNow your function will be automatically triggered once every minute!\nTo check how things are going, check out the logs for your Lambda function (AWS console \u003e Lambda \u003e Monitor):\nI would also encourage you to check the VPC configuration for your Lambda function:\nAnd finally, deploy the Leaderboard function export MEMORYDB_PASSWORD=\u003center the password you had previously chosen e.g. P@ssw0rd12345678\u003e # note the name of the stack is stack3 cdk deploy stack3 After successful deployment, you should have a Lambda Function URL ready for you to access - you can simply copy it from the stack output!\nJust access the endpoint (I have used curl CLI, but a browser should work just fine):\ncurl -i \u003center lambda function URL from the stack output\u003e You should get back a JSON payload with info about top 10 hashtags along with their names and number of times they were mentioned - something similar to this:\n[ { \"Score\": 121, \"Member\": \"AWS\" }, { \"Score\": 56, \"Member\": \"gaming\" }, { \"Score\": 56, \"Member\": \"RESTOCK\" }, { \"Score\": 56, \"Member\": \"ALERT\" }, { \"Score\": 35, \"Member\": \"aws\" }, { \"Score\": 26, \"Member\": \"rtx3080\" }, { \"Score\": 26, \"Member\": \"geforce3080\" }, { \"Score\": 24, \"Member\": \"Infographic\" }, { \"Score\": 23, \"Member\": \"箱マスク\" }, { \"Score\": 23, \"Member\": \"startups\" } ] This concludes the first part. In the second one, we dive into the CDK aspects and look at some Go code - see you there!","requirements#Requirements":" Create an AWS account (if you do not already have one) and log in. The IAM user that you use must have sufficient permissions to make necessary AWS service calls and manage AWS resources. Install and configure AWS CLI Install and bootstrap AWS CDK Setup Docker Install Go Get your Twitter API credentials ","the-twitter-hashtag-leaderboard-app#The Twitter Hashtag Leaderboard app":"Don’t worry, it’s simpler than it sounds! Here is the high level architecture:\nThe solution can be divided into two logical parts:\nThe first part handles tweet ingestion: A Lambda function fetches tweets (from Twitter), extracts hashtags for each tweet, and stores them in MemoryDB (in a Redis Sorted Set). This function gets invoked based on a schedule based on a rule in CloudWatch trigger The second part provides the leaderboard functionality: This is yet another Lambda function that provides a HTTP(s) endpoint (thanks to Lambda Function URL) to query the sorted set and extract top 10 hashtags (leaderboard) I told you, it’s quite simple!\nAlright, with the intro out of the way, we can move on to the fun part - which is deploying the application. Before that, make sure you have the following ready:"},"title":"Build a Twitter Leaderboard app with Redis and AWS Lambda (part 1)"},"/blog/twitter-leaderboard-app-redis-lambda-part2/":{"data":{"":"This is the second blog post of this two-part series that uses a practical application to demonstrate how to integrate Redis with AWS Lambda. The first part was about the solution overview, deployment and hopefully you were able to try it out end to end. As promised, the second part (this one) will cover the Infrastructure aspects (IaaC to be specific) which is comprised of three (CDK) Stacks (in the context of a single CDK App).\nI will provide a walk through of the CDK code which is written in Go, thanks to the CDK Go support (which is Developer Preview at the time of writing).","cdk-code-walk-through#CDK code walk through":"Let’s take it one stack at a time\nPlease note that some of the code has been redacted/omitted for brevity - you can always refer to complete code in the GitHub repo\nStart with the infrastructure stack\nstack := awscdk.NewStack(scope, \u0026id, \u0026sprops) vpc = awsec2.NewVpc(stack, jsii.String(\"demo-vpc\"), nil) authInfo := map[string]interface{}{\"Type\": \"password\", \"Passwords\": []string{memorydbPassword}} user = awsmemorydb.NewCfnUser(stack, jsii.String(\"demo-memorydb-user\"), \u0026awsmemorydb.CfnUserProps{UserName: jsii.String(\"demo-user\"), AccessString: jsii.String(accessString), AuthenticationMode: authInfo}) acl := awsmemorydb.NewCfnACL(stack, jsii.String(\"demo-memorydb-acl\"), \u0026awsmemorydb.CfnACLProps{AclName: jsii.String(\"demo-memorydb-acl\"), UserNames: \u0026[]*string{user.UserName()}}) //snip ..... subnetGroup := awsmemorydb.NewCfnSubnetGroup(stack, jsii.String(\"demo-memorydb-subnetgroup\"), \u0026awsmemorydb.CfnSubnetGroupProps{SubnetGroupName: jsii.String(\"demo-memorydb-subnetgroup\"), SubnetIds: \u0026subnetIDsForSubnetGroup}) memorydbSecurityGroup = awsec2.NewSecurityGroup(stack, jsii.String(\"memorydb-demo-sg\"), \u0026awsec2.SecurityGroupProps{Vpc: vpc, SecurityGroupName: jsii.String(\"memorydb-demo-sg\"), AllowAllOutbound: jsii.Bool(true)}) memorydbCluster = awsmemorydb.NewCfnCluster(//... details omitted) //...snip twitterIngestFunctionSecurityGroup = awsec2.NewSecurityGroup(//... details omitted) twitterLeaderboardFunctionSecurityGroup = awsec2.NewSecurityGroup(//... details omitted) memorydbSecurityGroup.AddIngressRule(//... details omitted) memorydbSecurityGroup.AddIngressRule(//... details omitted) To summarise:\nA single line of code to create VPC and related components! We create ACL, User, Subnet group for MemoryDB cluster and refer to them when during cluster creation with awsmemorydb.NewCfnCluster We also create required security groups - their main role is to allow Lambda functions to access MemoryDB (we specify explicit Inbound rules to make that possible) One for MemoryDB cluster One each for both the Lambda functions The next stack deploys the tweets ingestion Lambda Function\n//.... memoryDBEndpointURL := fmt.Sprintf(\"%s:%s\", *memorydbCluster.AttrClusterEndpointAddress(), strconv.Itoa(int(*memorydbCluster.Port()))) lambdaEnvVars := \u0026map[string]*string{\"MEMORYDB_ENDPOINT\": jsii.String(memoryDBEndpointURL), \"MEMORYDB_USER\": user.UserName(), \"MEMORYDB_PASSWORD\": jsii.String(getMemorydbPassword()), \"TWITTER_API_KEY\": jsii.String(getTwitterAPIKey()), \"TWITTER_API_SECRET\": jsii.String(getTwitterAPISecret()), \"TWITTER_ACCESS_TOKEN\": jsii.String(getTwitterAccessToken()), \"TWITTER_ACCESS_TOKEN_SECRET\": jsii.String(getTwitterAccessTokenSecret())} awslambda.NewDockerImageFunction(stack, jsii.String(\"lambda-memorydb-func\"), \u0026awslambda.DockerImageFunctionProps{FunctionName: jsii.String(tweetIngestionFunctionName), Environment: lambdaEnvVars, Timeout: awscdk.Duration_Seconds(jsii.Number(20)), Code: awslambda.DockerImageCode_FromImageAsset(jsii.String(tweetIngestionFunctionPath), nil), Vpc: vpc, VpcSubnets: \u0026awsec2.SubnetSelection{Subnets: vpc.PrivateSubnets()}, SecurityGroups: \u0026[]awsec2.ISecurityGroup{twitterIngestFunctionSecurityGroup}}) //.... It’s quite simple compared to the previous stack. We define the environment variables required by our Lambda function (including Twitter API credentials) and deploy it as a Docker image.\nFor the function to be packaged as a Docker image, I used the Go:1.x base image. But, you can explore other options as well. During deployment, the Docker image is built locally, pushed to a private ECR registry and finally the Lambda function is created - all this, with a few lines of code!\nNotice that the MemoryDB cluster and security group are automatically referred/looked-up from the previous stack (not re-created!).\nFinally, the third stack takes care of the leaderboard Lambda function\nIt’s quite similar to the previous one, except for the addition of the Lambda Function URL (awslambda.NewFunctionUrl) which we use the output for the stack:\n//.... memoryDBEndpointURL := fmt.Sprintf(\"%s:%s\", *memorydbCluster.AttrClusterEndpointAddress(), strconv.Itoa(int(*memorydbCluster.Port()))) lambdaEnvVars := \u0026map[string]*string{\"MEMORYDB_ENDPOINT\": jsii.String(memoryDBEndpointURL), \"MEMORYDB_USERNAME\": user.UserName(), \"MEMORYDB_PASSWORD\": jsii.String(getMemorydbPassword())} function := awslambda.NewDockerImageFunction(stack, jsii.String(\"twitter-hashtag-leaderboard\"), \u0026awslambda.DockerImageFunctionProps{FunctionName: jsii.String(hashtagLeaderboardFunctionName), Environment: lambdaEnvVars, Code: awslambda.DockerImageCode_FromImageAsset(jsii.String(hashtagLeaderboardFunctionPath), nil), Timeout: awscdk.Duration_Seconds(jsii.Number(5)), Vpc: vpc, VpcSubnets: \u0026awsec2.SubnetSelection{Subnets: vpc.PrivateSubnets()}, SecurityGroups: \u0026[]awsec2.ISecurityGroup{twitterLeaderboardFunctionSecurityGroup}}) funcURL := awslambda.NewFunctionUrl(stack, jsii.String(\"func-url\"), \u0026awslambda.FunctionUrlProps{AuthType: awslambda.FunctionUrlAuthType_NONE, Function: function}) awscdk.NewCfnOutput(stack, jsii.String(\"Function URL\"), \u0026awscdk.CfnOutputProps{Value: funcURL.Url()}) That’s all for this blog post. Closing off with links to AWS Go CDK v2 references:\nFor MemoryDB - https://pkg.go.dev/github.com/aws/aws-cdk-go/awscdk/v2/awsmemorydb For Lambda - https://pkg.go.dev/github.com/aws/aws-cdk-go/awscdk/v2/awslambda For VPC etc. - https://pkg.go.dev/github.com/aws/aws-cdk-go/awscdk/v2/awsec2 CDK V2 https://pkg.go.dev/github.com/aws/aws-cdk-go/awscdk/v2 This concludes the two-part series. Stay tuned for more and as always, Happy Coding!"},"title":"Build a Twitter Leaderboard app with Redis and AWS Lambda (part 2)"},"/blog/url-shortener-dynamodb-apprunner/":{"data":{"":"","auto-scaling-in-action#Auto-scaling in action":"","aws-cdk-code-walk-through#AWS CDK code walk through\u0026hellip;":"Earlier, I wrote about a Serverless URL shortener application on AWS using DynamoDB, AWS Lambda and API Gateway.\nIn this blog post, we will deploy that as a REST API on AWS App Runner and continue to use DynamoDB as the database. AWS App Runner is a compute service that makes it easy to deploy applications from a container image (or source code), manage their scalability, deployment pipelines and more.\nWith the help of a practical example presented in this blog, you will:\nLearn about AWS App Runner, how to integrate it with DynamoDB Run simple benchmarks to explore the scalability characteristics of your App Runner service as well as DynamoDB Apply “Infrastructure-as-Code” with AWS CDK Go and deploy the entire stack, including the database, application and other AWS resources. Also see the DynamoDB Go SDK (v2) in action and some of the basic operations such as PutItem, GetItem. Let’s start by deploying the URL shortener application Before you begin, make sure you have the Go programming language (v1.16 or higher) and AWS CDK installed.\nClone the project and change to the right directory:\ngit clone https://github.com/abhirockzz/apprunner-dynamodb-golang cd cdk To start the deployment…\nRun cdk deploy and provide your confirmation to proceed. The subsequent sections will provide a walk through of the CDK code for you to better understand what’s going on.\ncdk deploy This will start creating the AWS resources required for our application.\nIf you want to see the AWS CloudFormation template which will be used behind the scenes, run cdk synth and check the cdk.out folder\nYou can keep track of the progress in the terminal or navigate to AWS console: CloudFormation \u003e Stacks \u003e DynamoDBAppRunnerStack\nOnce all the resources are created, you should have the DynamoDB table, the App Runner Service (along with the related IAM roles etc.).\nURL shortener service on App Runner\nYou should see the landing page of the App Runner service that was just deployed.\nAlso look at the Service Settings under Configuration which shows the environment variables (configured at runtime by CDK) as well as the compute resources (1 VCPU and 2 GB) that we specified\nOur URL shortener is ready! The application is relatively simple and exposes two endpoints:\nTo create a short link for a URL Access the original URL via the short link To try out the application, you need to get the endpoint URL provider by the App Runner service. It’s available in the stack output (in the terminal or the Outputs tab in the AWS CloudFormation console for your Stack):\nFirst, export the App Runner service endpoint as an environment variable,\nexport APP_URL=\u003center App Runner service URL\u003e # example export APP_URL=https://jt6jjprtyi.us-east-1.awsapprunner.com Invoke it with a URL that you want to access via a short link.\ncurl -i -X POST -d 'https://abhirockzz.github.io/' $APP_URL # output HTTP/1.1 200 OK Date: Thu, 21 Jul 2022 11:03:40 GMT Content-Length: 25 Content-Type: text/plain; charset=utf-8 {\"ShortCode\":\"ae1e31a6\"} You should get a JSON response with a short code and see an item in the DynamoDB table as well:\nYou can continue to test the application with a few other URLs.\nTo access the URL associated with the short code\n… enter the following in your browser http://\u003center APP_URL\u003e/\u003cshortcode\u003e\nFor example, when you enter https://jt6jjprtyi.us-east-1.awsapprunner.com/ae1e31a6, you will be re-directed to the original URL.\nYou can also use curl. Here is an example:\nexport APP_URL=https://jt6jjprtyi.us-east-1.awsapprunner.com curl -i $APP_URL/ae1e31a6 # output HTTP/1.1 302 Found Location: https://abhirockzz.github.io/ Date: Thu, 21 Jul 2022 11:07:58 GMT Content-Length: 0 Auto-scaling in action Both App Runner and DynamoDB are capable of scaling up (and down) according to workload.\nAWS App Runner\nAWS App Runner automatically scales up the number of instances in response to an increase in traffic and scales them back when the traffic decreases.\nThis is based on AutoScalingConfiguration which is driven by the following user-defined properties - Max concurrency, Max size and Min size. For details, refer to Managing App Runner automatic scaling\nHere is the auto-scale configuration for the URL shortener App Runner Service:\nDynamoDB\nIn case of On-demand mode, DynamoDB instantly accommodates your workloads as they ramp up or down to any previously reached traffic level. Provisioned mode requires us to specify the number of reads and writes per second that you require for your application, but you can use auto scaling to adjust your table’s provisioned capacity automatically in response to traffic changes.\nLets run some tests We can run a simple benchmarks and witness how our service reacts. I will be using a load testing tool called hey but you can also do use Apache Bench etc.\nHere is what we’ll do:\nStart off with a simple test and examine the response. Ramp up the load such that it breaches the provisioned capacity for the DynamoDB table. Update the DynamoDB table capacity and repeat. Install hey and execute a basic test - 200 requests with 50 workers concurrently (as per default settings):\nhey $APP_URL/\u003center the short code\u003e #example hey https://jt6jjprtyi.us-east-1.awsapprunner.com/ae1e31a6 This should be well within the capacity of our stack. Let’s bump it to 500 concurrent workers to execute requests for a sustained period of 4 minutes.\nhey -c 500 -z 4m $APP_URL/\u003center the short code\u003e #example hey -c 500 -z 4m https://jt6jjprtyi.us-east-1.awsapprunner.com/ae1e31a6 How is DynamoDB doing?\nIn DynamoDB console under Table capacity metrics, check Read usage (average units/second):\nMore importantly, check Read throttled events (count):\nSince our table was in Provisioned capacity mode (with 5 RCU and WCU), the requests got throttled and some of them failed.\nEdit the table to change its mode to On-demand, re-run the load test. You should not see throttling errors now since DynamoDB will auto-scale in response to the load.\nWhat about App Runner??\nIn the Metrics seton in App Runner console, check the Active Instances count.\nYou can also track the other metrics and experiment with various load capacities\nAlright, now that you’ve actually seen what the application does and examined the basic scalability characteristics of the stack, let’s move on to the how.\nBut, before that….\nDon’t forget to delete resources\nOnce you’re done, to delete all the services, simply use:\ncdk destroy AWS CDK code walk through… We will go through the keys parts of the NewDynamoDBAppRunnerStack function which defines the entire stack required by the URL shortener application (I’ve omitted some code for brevity).\nYou can refer to the complete code on GitHub\nWe start by defining a DynamoDB table with shorturl as the Partition key (Range/Sort key is not required for our case). Note that the BillingMode attribute decides the table capacity mode, which is Provisioned in this case (with 5 RCU and WCU). As demonstrated in the previous section, this was chosen on purpose.\nfunc NewDynamoDBAppRunnerStack(scope constructs.Construct, id string, props *DynamoDBAppRunnerStackProps) awscdk.Stack { //.... dynamoDBTable := awsdynamodb.NewTable(stack, jsii.String(\"dynamodb-short-urls-table\"), \u0026awsdynamodb.TableProps{ PartitionKey: \u0026awsdynamodb.Attribute{ Name: jsii.String(shortCodeDynamoDBAttributeName), Type: awsdynamodb.AttributeType_STRING, }, BillingMode: awsdynamodb.BillingMode_PROVISIONED, ReadCapacity: jsii.Number(5), WriteCapacity: jsii.Number(5), RemovalPolicy: awscdk.RemovalPolicy_DESTROY, }) //... Then, we use awsiam.NewRole to define a new IAM role and also add a policy that allows App Runner to execute actions in DynamoDB. In this case we provide granular permissions - GetItem and PutItem.\n//... apprunnerDynamoDBIAMrole := awsiam.NewRole(stack, jsii.String(\"role-apprunner-dynamodb\"), \u0026awsiam.RoleProps{ AssumedBy: awsiam.NewServicePrincipal(jsii.String(\"tasks.apprunner.amazonaws.com\"), nil), }) apprunnerDynamoDBIAMrole.AddToPolicy(awsiam.NewPolicyStatement(\u0026awsiam.PolicyStatementProps{ Effect: awsiam.Effect_ALLOW, Actions: jsii.Strings(\"dynamodb:GetItem\", \"dynamodb:PutItem\"), Resources: jsii.Strings(*dynamoDBTable.TableArn())})) awsecrassets.NewDockerImageAsset allows to create and push our application Docker image to ECR - with a single line of code.\n//... appDockerImage := awsecrassets.NewDockerImageAsset(stack, jsii.String(\"app-image\"), \u0026awsecrassets.DockerImageAssetProps{ Directory: jsii.String(appDirectory)}) Once all the pieces ready, we define the App Runner Service. Notice how it references the information required by the application:\nThe name of the DynamoDB table (defined previously) is seeded as TABLE_NAME env var (required by the application) The Docker image that we defined is directly used by the Asset attribute The IAM role that we defined is attached to the App Runner service as Instance Role The instance role is an optional role that App Runner uses to provide permissions to AWS service actions that your service’s compute instances need.\nNote that that an alpha version (at the time of writing) of the L2 App Runner CDK construct has been used and this is much simple compared to the CloudFormation based L1 construct. It offers a convenient NewService function with which you can define the App Runner Service including the source (locally available in this case), the IAM roles (Instance and Access) etc.\n//... app := awscdkapprunneralpha.NewService(stack, jsii.String(\"apprunner-url-shortener\"), \u0026awscdkapprunneralpha.ServiceProps{ Source: awscdkapprunneralpha.NewAssetSource( \u0026awscdkapprunneralpha.AssetProps{ ImageConfiguration: \u0026awscdkapprunneralpha.ImageConfiguration{Environment: \u0026map[string]*string{ \"TABLE_NAME\": dynamoDBTable.TableName(), \"AWS_REGION\": dynamoDBTable.Env().Region}, Port: jsii.Number(appPort)}, Asset: appDockerImage}), InstanceRole: apprunnerDynamoDBIAMrole, Memory: awscdkapprunneralpha.Memory_TWO_GB(), Cpu: awscdkapprunneralpha.Cpu_ONE_VCPU(), }) app.ApplyRemovalPolicy(awscdk.RemovalPolicy_DESTROY) ","lets-start-by-deploying-the-url-shortener-application#Let\u0026rsquo;s start by deploying the URL shortener application":"","wrap-up#Wrap up":"This brings us to the end of this blog post! You explored a URL shortener application that exposed REST APIs, used DynamoDB as its persistent store and deployed it to AWS App Runner. Then we looked at how the individual services scaled elastically in response to the workload. Finally, we also explored the AWS CDK code that made is possible to define the application and its infrastructure as (Go) code.\nHappy building!"},"title":"Use AWS App Runner, DynamoDB and CDK to deploy and run a Cloud native Go app"},"/blog/url-shortener-dynamodb-lambda-go/":{"data":{"":"Using AWS Lambda, DynamoDB and API Gateway\nThis blog post covers how to build a Serverless URL shortener application using Go. It leverages AWS Lambda for business logic, DynamoDB for persistence and API Gateway to provide the HTTP endpoints to access and use the application. The sample application presented in this blog is a trimmed down version of bit.ly or other solutions you may have used or encountered.\nIt’s structured as follows:\nI will start off with a quick introduction and dive into how to deploy try the solution. After that, I will focus on the code itself. This will cover: The part which is used to write the infrastructure (using Go bindings for AWS CDK) And also the core business logic which contains the Lambda function (using Lambda Go support) as well as the DynamoDB operations (using the DynamoDB Go SDK) In this blog, you will learn:\nHow to use the DynamoDB Go SDK (v2) to execute CRUD operations such as PutItem, GetItem, UpdateItem and DeleteItem How to use AWS CDK Go bindings to deploy a Serverless application to create and manage a DynamoDB table, Lambda functions, API Gateway and other components as well. Once you deploy the application, you will be able to create short codes for URLs using the endpoint exposed by the API Gateway and also access them.\n# create short code for a URL (e.g. https://abhirockzz.github.io/) curl -i -X POST -d 'https://abhirockzz.github.io/' -H 'Content-Type: text/plain' $URL_SHORTENER_APP_URL # access URL via short code curl -i $URL_SHORTENER_APP_URL/\u003cshort-code\u003e ","lets-get-started---deploy-the-serverless-application#Lets get started - Deploy the Serverless application":"Before you proceed, make sure you have the Go programming language (v1.16 or higher) and AWS CDK installed.\nClone the project and change to the right directory:\ngit clone https://github.com/abhirockzz/serverless-url-shortener-golang cd cdk To start the deployment…\n.. all you will do is run a single command (cdk deploy), and wait for a bit. You will see a (long) list of resources that will be created and will need to provide your confirmation to proceed.\nDon’t worry, in the next section I will explain what’s happening.\ncdk deploy # output Bundling asset ServerlessURLShortenerStack1/create-url-function/Code/Stage... Bundling asset ServerlessURLShortenerStack1/access-url-function/Code/Stage... Bundling asset ServerlessURLShortenerStack1/update-url-status-function/Code/Stage... Bundling asset ServerlessURLShortenerStack1/delete-url-function/Code/Stage... ✨ Synthesis time: 10.28s This deployment will make potentially sensitive changes according to your current security approval level (--require-approval broadening). Please confirm you intend to make the following modifications: ....... Do you wish to deploy these changes (y/n)? This will start creating the AWS resources required for our application.\nIf you want to see the AWS CloudFormation template which will be used behind the scenes, run cdk synth and check the cdk.out folder\nYou can keep track of the progress in the terminal or navigate to AWS console: CloudFormation \u003e Stacks \u003e ServerlessURLShortenerStack\nOnce all the resources are created, you can try out the application. You should have:\nFour Lambda functions (and related resources) A DynamoDB table An API Gateway (as well as routes, integrations) along with a few others (like IAM roles etc.) Before you proceed, get the API Gateway endpoint that you will need to use. It’s available in the stack output (in the terminal or the Outputs tab in the AWS CloudFormation console for your Stack):","shorten-some-urls#Shorten some URLs!":"Start by generating short codes for a few URLs\n# export the API Gateway endpoint export URL_SHORTENER_APP_URL=\u003creplace with apigw endpoint above\u003e # for example: export URL_SHORTENER_APP_URL=https://b3it0tltzk.execute-api.us-east-1.amazonaws.com/ # invoke the endpoint to create short codes curl -i -X POST -d 'https://abhirockzz.github.io/' -H 'Content-Type: text/plain' $URL_SHORTENER_APP_URL curl -i -X POST -d 'https://dzone.com/users/456870/abhirockzz.html' -H 'Content-Type: text/plain' $URL_SHORTENER_APP_URL curl -i -X POST -d 'https://abhishek1987.medium.com/' -H 'Content-Type: text/plain' $URL_SHORTENER_APP_URL To generate a short code, you need to pass the original URL in the payload body as part of a HTTP POST request (for e.g. https://abhishek1987.medium.com/)\n‘Content-Type: text/plain’ is important, otherwise API Gateway will do base64 encoding of your payload\nIf all goes well, you should get a HTTP 201 along with the short code in the HTTP response (as a JSON payload).\nHTTP/2 201 date: Fri, 15 Jul 2022 13:03:20 GMT content-type: text/plain; charset=utf-8 content-length: 25 apigw-requestid: VTzPsgmSoAMESdA= {\"short_code\":\"1ee3ad1b\"} To confirm, check the DynamoDB table.\nNotice an active attribute there? More on this soon\nAccess the URL using the short code\nWith services like bit.ly etc. you typically create short links for your URLs and share them with the world. We will do something similar. Now that you have the short code generated, you can share the link (it’s not really a short link like bit.ly but that’s ok for now!) with others and once they access it, they would see the original URL.\nThe access link will have the following format - \u003cURL_SHORTENER_APP_URL\u003e/\u003cgenerated short code\u003e for e.g. https://b3it0tltzk.execute-api.us-east-1.amazonaws.com/1ee3ad1b\nIf you navigate to the link using a browser, you will be automatically redirected to the original URL that you had specified. To see what’s going on, try the same with curl:\ncurl -i $URL_SHORTENER_APP_URL/\u003cshort code\u003e # example curl -i https://b3it0tltzk.execute-api.us-east-1.amazonaws.com/0e1785b1 This is simply an HTTP GET request. If all goes well, you should get an HTTP 302 response (StatusFound) and the URL re-direction happens due to the the Location HTTP header which contains the original URL.\nHTTP/2 302 date: Fri, 15 Jul 2022 13:08:54 GMT content-length: 0 location: https://abhirockzz.github.io/ apigw-requestid: VT0D1hNLIAMES8w= How about using a short code that does not exist?\nSet the status\nYou can enable and disable the short codes. The original URL will only be accessible if the association is in active state.\nTo disable a short code:\ncurl -i -X PUT -d '{\"active\": false}' -H 'Content-Type: application/json' $URL_SHORTENER_APP_URL/\u003cshort code\u003e # example curl -i -X PUT -d '{\"active\": false}' -H 'Content-Type: application/json' https://b3it0tltzk.execute-api.us-east-1.amazonaws.com/1ee3ad1b This is an HTTP PUT request with a JSON payload that specifies the status (false in this case refers to disable) along with the short code which is a path parameter to the API Gateway endpoint. If all works well, you should see a HTTP 204 (No Content) response:\nHTTP/2 204 date: Fri, 15 Jul 2022 13:15:41 GMT apigw-requestid: VT1Digy8IAMEVHw= Check the DynamoDB record - the active attribute must have switched to false.\nAs an exercise, try the following:\naccess the URL via the same short code now and check the response. access an invalid short code i.e. that does not exist enable a disabled URL (use {\"active\": true}) Ok, so far we have covered all operations, except delete. Lets try that and wrap up the CRUD!\nDelete\ncurl -i -X DELETE $URL_SHORTENER_APP_URL/\u003cshort code\u003e # example curl -i -X DELETE https://b3it0tltzk.execute-api.us-east-1.amazonaws.com/1ee3ad1b Nothing too surprising. We use a HTTP DELETE along with the short code. Just like in case of update, you should get a HTTP 204 response:\nHTTP/2 204 date: Fri, 15 Jul 2022 13:23:36 GMT apigw-requestid: VT2NzgjnIAMEVKA= But this time of course, the DynamoDB record should have been deleted - confirm the same.\nWhat happens when you try to delete a short code that does not exist?\nDon’t forget to clean up! Once you’re done, to delete all the services, simply use:\ncdk destroy Alright, now that you’ve actually seen “what” the application does, let’s move on to the “how”. We will start with the AWS CDK code and explore how it does all the heavy lifting behind to setup the infrastructure for our Serverless URL shortener service.","url-shortener-lambda-function-and-dynamodb-logic#URL shortener Lambda function and DynamoDB logic":"There are four different functions, all of which are in their respective folders and all of them have a few things in common in the way they operate:\nThey do dome initial processing - process the payload, or extract the path parameter from the URL etc. Invoke a common database layer - to execute the CRUD functionality (more on this soon) Handle errors as appropriate and return response With that knowledge, it should be easy to follow along the code.\nAs before, some parts of the code have been omitted for brevity\nCreate function\nfunc handler(ctx context.Context, req events.APIGatewayV2HTTPRequest) (events.APIGatewayV2HTTPResponse, error) { url := req.Body shortCode, err := db.SaveURL(url) if err != nil {//..handle error} response := Response{ShortCode: shortCode} respBytes, err := json.Marshal(response) if err != nil {//..handle error} return events.APIGatewayV2HTTPResponse{StatusCode: http.StatusCreated, Body: string(respBytes)}, nil } This function starts by reading the payload of the HTTP request body - this is a string which has the URL for which the short code is being created. It invokes the database layer to try and save this record to DynamoDB and handles errors. Finally, it returns a JSON response with the short code.\nHere is the function that actually interacts with DynamoDB to get the job done.\nfunc SaveURL(longurl string) (string, error) { shortCode := uuid.New().String()[:8] item := make(map[string]types.AttributeValue) item[longURLDynamoDBAttributeName] = \u0026types.AttributeValueMemberS{Value: longurl} item[shortCodeDynamoDBAttributeName] = \u0026types.AttributeValueMemberS{Value: shortCode} item[activeDynamoDBAttributeName] = \u0026types.AttributeValueMemberBOOL{Value: true} _, err := client.PutItem(context.Background(), \u0026dynamodb.PutItemInput{ TableName: aws.String(table), Item: item}) if err != nil {//..handle error} return shortCode, nil } For the purposes of this sample app, the short code is created by generating a UUID and trimming out the last 8 digits. It’s easy to replace this with another technique - all that matters is that you generate a unique string that can work as a short code. Then, it all about calling the PutItem API with the required data.\nAccess the URL\nfunc handler(ctx context.Context, req events.APIGatewayV2HTTPRequest) (events.APIGatewayV2HTTPResponse, error) { shortCode := req.PathParameters[pathParameterName] longurl, err := db.GetLongURL(shortCode) if err != nil {//..handle error} return events.APIGatewayV2HTTPResponse{StatusCode: http.StatusFound, Headers: map[string]string{locationHeader: longurl}}, nil } When someone accesses the short link (as demonstrated in the earlier section), the short code is passed in as a path parameter e.g. http://\u003capi gw url\u003e/\u003cshort code\u003e. the database layer is invoked to get the corresponding URL from DynamoDB table (errors are handled as needed). Finally, the response is returned to the user wherein the status code is 302 and the URL is passed in the Location header. This is what re-directs you to the original URL when you enter the short code (in the browser)\nHere is the DynamoDB call:\nfunc GetLongURL(shortCode string) (string, error) { op, err := client.GetItem(context.Background(), \u0026dynamodb.GetItemInput{ TableName: aws.String(table), Key: map[string]types.AttributeValue{ shortCodeDynamoDBAttributeName: \u0026types.AttributeValueMemberS{Value: shortCode}}}) if err != nil {//..handle error} if op.Item == nil { return \"\", ErrUrlNotFound } activeAV := op.Item[activeDynamoDBAttributeName] active := activeAV.(*types.AttributeValueMemberBOOL).Value if !active { return \"\", ErrUrlNotActive } longurlAV := op.Item[longURLDynamoDBAttributeName] longurl := longurlAV.(*types.AttributeValueMemberS).Value return longurl, nil } The first step is to use GetItem API to get the DynamoDB record containing URL and status corresponding to the short code. If the item object in the response is nil, we can be sure that a record with that short code does not exist - we return a custom error which can be helpful for our function which can then return an appropriate response to the caller of the API (e.g. a HTTP 404). We also check the status (active or not) and return an error if active is set to false. If all is well, the URL is returned to the caller.\nUpdate status\nfunc handler(ctx context.Context, req events.APIGatewayV2HTTPRequest) (events.APIGatewayV2HTTPResponse, error) { var payload Payload reqBody := req.Body err := json.Unmarshal([]byte(reqBody), \u0026payload) if err != nil {//..handle error} shortCode := req.PathParameters[pathParameterName] err = db.Update(shortCode, payload.Active) if err != nil {//..handle error} return events.APIGatewayV2HTTPResponse{StatusCode: http.StatusNoContent}, nil } The first step is to marshal the HTTP request payload which is a JSON e.g. {\"active\": false} and then get the short code from the path parameter. The database layer is invoked to update the status and handle errors.\nfunc Update(shortCode string, status bool) error { update := expression.Set(expression.Name(activeDynamoDBAttributeName), expression.Value(status)) updateExpression, _ := expression.NewBuilder().WithUpdate(update).Build() condition := expression.AttributeExists(expression.Name(shortCodeDynamoDBAttributeName)) conditionExpression, _ := expression.NewBuilder().WithCondition(condition).Build() _, err := client.UpdateItem(context.Background(), \u0026dynamodb.UpdateItemInput{ TableName: aws.String(table), Key: map[string]types.AttributeValue{ shortCodeDynamoDBAttributeName: \u0026types.AttributeValueMemberS{Value: shortCode}}, UpdateExpression: updateExpression.Update(), ExpressionAttributeNames: updateExpression.Names(), ExpressionAttributeValues: updateExpression.Values(), ConditionExpression: conditionExpression.Condition(), }) if err != nil \u0026\u0026 strings.Contains(err.Error(), \"ConditionalCheckFailedException\") { return ErrUrlNotFound } return err } The UpdateItem API call takes care of changing the status. It’s fairly simple except for the all these expressions that you need - especially if you’re new to the concept. The first one (mandatory) is the update expression where you specify the attribute you need to set (active in this case) and its value. The second one makes sure that you are updating the status for a short code that actually exists in the table. This is important since, otherwise the UpdateItem API call will insert a new item (we don’t want that!). Instead of rolling out the expressions by hand, we use the expressions package.\nDelete short code\nfunc handler(ctx context.Context, req events.APIGatewayV2HTTPRequest) (events.APIGatewayV2HTTPResponse, error) { shortCode := req.PathParameters[pathParameterName] err := db.Delete(shortCode) if err != nil {//..handle error} return events.APIGatewayV2HTTPResponse{StatusCode: http.StatusNoContent}, nil } The delete handler is no different. After the short code to be deleted is extracted from the path parameter, the database layer is invoked to remove it from the DynamoDB table. The result returned to the user is either an HTTP 204 (on success) or the error.\nfunc Delete(shortCode string) error { condition := expression.AttributeExists(expression.Name(shortCodeDynamoDBAttributeName)) conditionExpression, _ := expression.NewBuilder().WithCondition(condition).Build() _, err := client.DeleteItem(context.Background(), \u0026dynamodb.DeleteItemInput{ TableName: aws.String(table), Key: map[string]types.AttributeValue{ shortCodeDynamoDBAttributeName: \u0026types.AttributeValueMemberS{Value: shortCode}}, ConditionExpression: conditionExpression.Condition(), ExpressionAttributeNames: conditionExpression.Names(), ExpressionAttributeValues: conditionExpression.Values()}) if err != nil \u0026\u0026 strings.Contains(err.Error(), \"ConditionalCheckFailedException\") { return ErrUrlNotFound } return err } Just like UpdateItem API, the DeleteItem API also takes in a condition expression. If there is no record in the DynamoDB table with the given short code, an error is returned. Otherwise, the record is deleted.\nThat completes the code walk through!","with-aws-cdk-infrastructure-is-code#With AWS CDK, Infrastructure-IS-code!":"You can check out the code in this GitHub repo. I will walk you through the keys parts of the NewServerlessURLShortenerStack function which defines the workhorse of our CDK application.\nI have omitted some of the code for brevity\nWe start by creating a DynamoDB table. A primary key is all that’s required in order to do that - in this case shortcode (we don’t have range/sort key in this example)\ndynamoDBTable := awsdynamodb.NewTable(stack, jsii.String(\"url-shortener-dynamodb-table\"), \u0026awsdynamodb.TableProps{ PartitionKey: \u0026awsdynamodb.Attribute{ Name: jsii.String(shortCodeDynamoDBAttributeName), Type: awsdynamodb.AttributeType_STRING}}) Then, we create an API Gateway (HTTP API) with just one line of code!\nurlShortenerAPI := awscdkapigatewayv2alpha.NewHttpApi(stack, jsii.String(\"url-shortner-http-api\"), nil) We move on to the first Lambda function that creates short codes for URLs. Notice that we use an experimental module awscdklambdagoalpha (here is the stable version at the time of writing). If your Go project is structured in a specific way (details here) and you specify its path using Entry, it will automatically take care of building, packaging and deploying your Lambda function! Not bad at all!\nIn addition to Local bundling (as used in this example), Docker based builds are also supported.\ncreateURLFunction := awscdklambdagoalpha.NewGoFunction(stack, jsii.String(\"create-url-function\"), \u0026awscdklambdagoalpha.GoFunctionProps{ Runtime: awslambda.Runtime_GO_1_X(), Environment: funcEnvVar, Entry: jsii.String(createShortURLFunctionDirectory)}) dynamoDBTable.GrantWriteData(createURLFunction) Finally, we add the last bit of plumbing by creating a Lambda-HTTP API integration (notice how the Lambda function variable createURLFunction is referenced) and adding a route to the HTTP API we had created - this in turn refers to the Lambda integration.\ncreateFunctionIntg := awscdkapigatewayv2integrationsalpha.NewHttpLambdaIntegration(jsii.String(\"create-function-integration\"), createURLFunction, nil) urlShortenerAPI.AddRoutes(\u0026awscdkapigatewayv2alpha.AddRoutesOptions{ Path: jsii.String(\"/\"), Methods: \u0026[]awscdkapigatewayv2alpha.HttpMethod{awscdkapigatewayv2alpha.HttpMethod_POST}, Integration: createFunctionIntg}) This was just for one function - we have three more remaining. The good part is that the template for all these are similar i.e.\ncreate the function grant permission for DynamoDB wire it up with API Gateway (with the correct HTTP method i.e. POST, PUT, DELETE) So I will not repeat it over here. Feel free to grok through the rest of the code.\nNow that you understand the magic behind the “one-click” infrastructure setup, let’s move on to the core logic of the application.","wrap-up#Wrap up":"In this blog post you learnt how to use DynamoDB Go SDK using a URL Shortener sample application. You also integrated it with AWS Lambda and API Gateway to build a Serverless solution whose infrastructure was also defined using actual code (as opposed to yaml, JSON etc.), thanks to the Go support in AWS CDK."},"title":"Build a Serverless URL shortener with Go"},"/blog/use-azure-cosmos-db-as-a-docker-container-in-cicd-pipelines/":{"data":{"":"\nThere are lot of benefits to using Docker containers in CI/CD pipelines, especially for stateful systems like databases. For example, when you run integration tests, each CI job can start the database in an isolated container with a clean state, preventing conflicts between tests. This results in a testing environment that is reliable, consistent, and cost effective. This approach also reduces latency and improves the overall performance of the CI/CD pipeline because the database is locally accessible.\nThe Linux-based Azure Cosmos DB emulator is available as a Docker container and can run on a variety of platforms, including ARM64 architectures like Apple Silicon. It allows local development and testing of applications without needing an Azure subscription or incurring service costs. You can easily run it as a Docker container, and use it for local development and testing:\ndocker pull mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview ","about-azure-cosmos-db#About Azure Cosmos DB":"Azure Cosmos DB is a fully managed and serverless NoSQL and vector database for modern app development, including AI applications. With its SLA-backed speed and availability as well as instant dynamic scalability, it is ideal for real-time NoSQL and MongoDB applications that require high performance and distributed computing over massive volumes of NoSQL and vector data.\nTry Azure Cosmos DB for free here. To stay in the loop on Azure Cosmos DB updates, follow us on X, YouTube, and LinkedIn.","azure-cosmos-db-with-github-actions#Azure Cosmos DB with GitHub Actions":"Let’s walk through an example to better understand how to use Azure Cosmos DB emulator with GitHub Actions, which is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline using workflows. A workflow is a configurable automated process that can run one or more jobs. It is defined by a YAML file checked in to your repository and runs when triggered by an event in your repository, or they can be triggered manually, or at a defined schedule.\nExample: CI workflow for a .NET application This GitHub Actions workflow configures Azure Cosmos DB Linux-based emulator as a GitHub actions service container as part of a job. GitHub takes care of starting the Docker container and destroys it when the job completes – no manual intervention required (such as executing the docker run command).\nname: .NET App CI on: push: branches: [main] paths: - 'dotnet-app/**' pull_request: branches: [main] paths: - 'dotnet-app/**' jobs: build-and-test: runs-on: ${{ matrix.os }} strategy: matrix: os: [ubuntu-latest, ubuntu-24.04-arm] services: cosmosdb: image: mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview ports: - 8081:8081 env: PROTOCOL: https env: COSMOSDB_CONNECTION_STRING: ${{ secrets.COSMOSDB_CONNECTION_STRING }} COSMOSDB_DATABASE_NAME: ${{ vars.COSMOSDB_DATABASE_NAME }} COSMOSDB_CONTAINER_NAME: ${{ vars.COSMOSDB_CONTAINER_NAME }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Set up .NET uses: actions/setup-dotnet@v3 with: dotnet-version: '8.0.x' - name: Export Cosmos DB Emulator Certificate run: | sudo apt update \u0026\u0026 sudo apt install -y openssl openssl s_client -connect localhost:8081 \u003c/dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' \u003e cosmos_emulator.cert sudo cp cosmos_emulator.cert /usr/local/share/ca-certificates/ sudo update-ca-certificates - name: Install dependencies run: cd dotnet-app \u0026\u0026 dotnet restore - name: Build run: cd dotnet-app \u0026\u0026 dotnet build --no-restore - name: Run tests run: cd dotnet-app \u0026\u0026 dotnet test --no-build --verbosity normal This job is configured to run on an Ubuntu runner and uses the mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview docker image as a service container. The connection string, database name, and container name are configured as environment variables. Since the job runs directly on a GitHub Actions hosted runner, the Run tests step can access the emulator using localhost:8081.\nThe Export Cosmos DB Emulator Certificate step is specific to .NET (as well as Java) applications because at the time of writing, the .NET and Java SDKs do not support HTTP mode in emulator. The PROTOCOL environment variable is set to https in the services section and this step exports the emulator certificate and adds them to the operating system trusted certificate store.","github-actions-runner-considerations#GitHub Actions runner considerations":"The sample repository demonstrated ubuntu based runners (for x64 and ARM64 architectures). This should work for Windows ARM-based runners as well. If you are considering Windows x64 runners, note that at the time of writing, GitHub Actions service containers are not supported in non-Linux runners. But you can work around this by adding steps to install Docker, and manage its lifecycle including starting and stopping the container.","leave-a-review#Leave a review":"Tell us about your Azure Cosmos DB experience! Leave a review on PeerSpot and we’ll gift you $50. Get started here.","try-it-out#Try it out!":"This GitHub repository provides examples of how to configure the Linux emulator as part of a GitHub Actions CI workflow for .NET, Python, Java and Go applications on both x64 and ARM64 architectures (demonstrated for Linux runner using ubuntu).\nFork the repository Navigate to the GitHub repository, and click the Fork button at the top-right corner of the repository page to create a copy of the repository under your own GitHub account.\nIn your GitHub account, open the repository and make sure to enable workflows in the repository settings.\nAdd the Cosmos DB emulator connection string (COSMOSDB_CONNECTION_STRING) as Repository secret to the repository. Use the following value:\nAccountEndpoint=http://localhost:8081/;AccountKey=C2y6yDjf5/R+ob0N8A7Cgv30VRDJIWEHLM+4QDU5DE2nQ9nDuVTqobD4b8mGGyPMbIZnqyMsEcaGQy67XIw/Jw==\nAdd database name (COSMOSDB_DATABASE_NAME) and container name (COSMOSDB_CONTAINER_NAME) as Repository variables:\nClone the forked repository to your local machine (make sure to use your GitHub username):\ngit clone https://github.com/\u003cyour-username\u003e/cosmosdb-linux-emulator-github-actions.git cd cosmosdb-linux-emulator-github-actions Trigger the workflow To trigger the workflow, make a small change to any/all of the code (.NET, Java, Python, or Go), add and commit your changes. For easier understanding, separate workflows are used for each language.\nPush your changes to your forked repository on GitHub:\ngit add . git commit -m \"Your commit message\" git push origin main After pushing the changes, GitHub Actions will automatically run the workflow. Go to the Actions tab in your repository to see the status and results of the workflows. Review any logs or output to ensure the workflows are running correctly.","wrap-up#Wrap up":"As I mentioned earlier, there are a lot of benefits of using Docker container in CI pipelines. GitHub Actions was used as the CI/CD platform in this case, but these concepts apply to other solutions as well. Try it out and let us know what you think!"},"title":"Use Azure Cosmos DB as a Docker container in CI/CD pipelines"},"/blog/using-redis-on-cloud-ten-things/":{"data":{"":"","#":"Its hard to operate stateful distributed systems at scale and Redis is no exception. Managed databases make life easier by taking on much of the heavy lifting. But you still need a sound architecture and apply best practices both on the server (Redis) as well as client (application).\nThis blog covers a range of Redis related best practices, tips and tricks including cluster scalability, client side configuration, integration, metrics etc. Although I will be citing Amazon MemoryDB and ElastiCache for Redis from time to time, most (if not all) will be applicable to Redis clusters in general.\nThis is not meant to be an exhaustive list by any means. I simply chose ten since its a nice, wholesome number!\nLets dive right in and start off with what options you have in terms of scaling your Redis cluster.\n1. Scalability options You can either scale up or down:\nScaling Up (Vertical) - You can increase the capacity of individual nodes/instances for e.g. upgrade from Amazon EC2 db.r6g.xlarge type to db.r6g.2xlarge Scaling Out (Horizontal) - You can add more nodes to the cluster The requirement to scale out might be be driven by few reasons.\nIf you need to tackle a read heavy workload, you can choose to add more replica nodes. This applies both for a Redis clustered setup (like MemoryDB) or a non-clustered primary-replica mode as in the case of ElastiCache with cluster mode disabled.\nIf you want to increase write capacity, you will find yourself limited by the primary-replica mode and should opt for a Redis Cluster based setup. You can increase the number of shards in your cluster - this is because only primary nodes can accept writes and each shard can only have one primary.\nThis has the added benefit of increasing the overall high availability as well.\n2. After scaling your cluster, you better use those replicas! The default behavior in most Redis Cluster clients (including redis-cli) is to redirect all reads to the primary node. If you’ve have added read replicas to scale read traffic, they are going to sit idle!\nYou need to switch to READONLY mode to ensure that the replicas handle all the read requests are not just passive participants. Make sure to configure your Redis client appropriately - this will vary with the client and programming language.\nFor example, in the Go Redis client, you can set ReadOnly to true:\nclient := redis.NewClusterClient( \u0026redis.ClusterOptions{ Addrs: []string{clusterEndpoint}, ReadOnly: true, //..other options }) To optimize further, you can also use RouteByLatency or RouteRandomly, both of which automatically turn on ReadOnly mode.\nYou can refer to how this works for Java clients such as Lettuce\n3. Be mindful of consistency characteristics when using read replicas There is a chance that your application might read stale data from replicas - this is Eventual Consistency in action. Since the primary to replica node replication is asynchronous, there is a chance that the write you sent to a primary node has not yet reflected in the read replica. This is likely when you have a high number of read replicas specially across multiple availability zones. If this is unacceptable for your use-case, you will have to resort to using primary nodes for reads as well.\nThe ReplicationLag metric in MemoryDB or ElastiCache for Redis can be used to check how far behind (in seconds) the replica is in applying changes from the primary node.\nWhat about Strong Consistency?\nIn case of MemoryDB, the reads from primary nodes are strongly consistent. This is because the client application receives a successful write acknowledgement only after a write (to the primary node) is written to a durable Multi-AZ Transaction Log.\n4. Remember, you can influence how your keys are distributed across a Redis cluster Instead of using consistent hashing (like lot of other distributed databases), Redis uses the concept of hash slots. There are 16384 slots in total, a range of hash slots is assigned to each primary node in the cluster and each key belongs to a specific hash slot (thereby assigned to a particular node). Multi-key operations executed on a Redis cluster cannot work if keys belong to different hash slots.\nBut, you are not completely at the mercy of the cluster! It’s possible to influence the key placement by using hash tags. Thus, you can ensure that specific keys have the same hash slot. For example, if you are storing orders for customer ID 42 in a HASH named customer:42:orders and the customer profile info in customer:42:profile, you can use curly braces {} to define the specific substring which will be hashed. In this case, our keys are {customer:42}:orders and {customer:42}:profile - {customer:42} now drives the hash slot placement. Now we can be confident that both these keys will in the same hash slot (hence same node).\n5. Did you think about scaling (back) in? Your application was successful, it has a lot of users and traffic. You scaled out the cluster and things are still going great. Awesome!\nBut what if you need to scale back in?\nYou need to be careful about a few things before you do that:\nIs there enough free memory on each of the nodes? Can this be done during non-peak hours? How will it affect your client applications? Which metrics can you monitor during this phase? (e.g. CPUUtilization, CurrConnections etc.) Refer to some of the best practices in the MemoryDb for Redis documentation to better plan for scaling in.\n6. When things go wrong…. Lets face it, failures are enviable. Whats important is whether you are prepared for them? In case of your Redis cluster, here are some things to think about:\nHave you tested how your application/service behavior in face of failures? If not, please do! With MemoryDB and ElastiCache for Redis, you can leverage the Failover API to simulate a primary node failure and trigger a failover. Do you have replica nodes? If all you have is one shard with a single primary node, you are certainly going to have downtime if that node fails. Do you have multiple shards? If all you have is one shard (with primary and replica), in case of primary node failure of that shard, the cluster cannot accept any writes. Do your shards span multiple availability zones? If you have shards across multiple AZs, you will be better prepared to tackle AZ failure. In all cases, MemoryDB ensures that no data is lost during node replacements or failover\n7. Unable to connect to Redis, help! Tl;DR: It’s probably the networking/security configuration\nThis is something which trips up folks all the time! With MemoryDB and ElastiCache, your Redis nodes are in a VPC. If you have a client application deployed to a compute service such as AWS Lambda, EKS, ECS, App Runner etc., you need to ensure you have the right configuration - specifically in terms of VPC and Security Group(s).\nThis might vary depending on the compute platform you are using. For example, how you configure a Lambda function to access resources in a VPC is slightly different compared to how App Runner does it (via a VPC Connector), or even EKS (although conceptually, they are the same).\n8. Redis 6 comes with Access Control Lists - use them! There is no excuse to not apply authentication (username/password) and authorization (ACL based permission) to your Redis cluster. MemoryDB is Redis 6 compliant and supports ACL. However, to comply with older Redis versions, it configures a default user per account (with username default) and an immutable ACL called open-access. If you create a MemoryDB cluster and associate it with this ACL:\nClients can connect without authentication Clients can execute any command on any key (no permission or authorization either) As a best practice:\nDefine an explicit ACL Add users (along with passwords), and Configure access strings as per your security requirements. You should monitor authentication failures. For example, the AuthenticationFailures metric in MemoryDB gives you the total number of failed authenticate attempts - set an alarm on this to detect unauthorized access attempts.\nDon’t forget perimeter security\nIf you’ve configured TLS on the server, don’t forget to use that in your client as well! For example, using Go Redis:\nclient := redis.NewClusterClient( \u0026redis.ClusterOptions{ Addrs: []string{clusterEndpoint}, TLSConfig: \u0026tls.Config{MaxVersion: tls.VersionTLS12}, //..other options }) Not using it can give your errors that’s not obvious enough (e.g. a generic i/o timeout) and make things hard to debug - this is something you need to be careful about.\n9. There are things you cannot do As a managed database service, MemoryDB or ElastiCache restrict access to some of the Redis commands. For example, you cannot use a subset of the CLUSTER related commands since the cluster management (scale, sharding etc.) is taken of by the service itself.\nBut, in some cases, you might be able to find alternatives. Think of monitoring slow running queries as an example. Although you cannot configure latency-monitor-threshold using CONFIG SET, you can set the slowlog-log-slower-than setting in the parameter group and then use slowlog get to compare against it.\n10. Use connection pooling Your Redis server nodes (even powerful ones) have finite resources. One of them is ability to support a certain number of concurrent connections. Most Redis clients offer connection pooling as a way to efficiently manage connections to the redis server. Re-using connections not only benefits your Redis server, but client side performance is improved due to less overhead - this is critical in high volume scenarios.\nElastiCache provides a few metrics you can track:\nCurrConnections: the number of client connections (excluding ones from read replicas) NewConnections: the total number of connections that have been accepted by the server during a specific period. 11. (bonus) Use the appropriate connection mode This one is kind of obvious, but I am going to call it out anyway since this is one of the most common “getting started” mistake that I witness folks make.\nThe connection mode that you use in your client application will depend on whether you’re using a standalone Redis setup, a Redis Cluster (most likely). Most Redis clients draw a clear distinction between them. For example, if you are using the Go Redis client with MemoryDB or Elasticache cluster mode enabled), you need to use NewClusterClient (not NewClient):\nredis.NewClusterClient(\u0026redis.ClusterOptions{//....}) Interestingly enough, there is UniversalClient option which is a bit more flexible (at the time of writing, this is in Go Redis v9)\nIf you don’t use the right mode of connection, you will get an error. But sometimes, the root cause will be hidden behind a generic error message - so you need to be watchful.","conclusion#Conclusion":"The architectural choices you make will ultimately be driven by your specific requirements. I would encourage you to explore the following blog posts for a deeper dive into performance characteristics of MemoryDB and ElastiCache for Redis and how they might impact the way design your solutions:\nOptimize Redis Client Performance for Amazon ElastiCache and MemoryDB Best practices: Redis clients and Amazon ElastiCache for Redis Measuring database performance of Amazon MemoryDB for Redis Feel free to share your Redis tips, tricks and suggestions. Until then, Happy Building!"},"title":"Using Redis on Cloud? Here are ten things you should know"},"/blog/vector-embeddings-made-easy-with-go-azure-cosmos-db-and-openai/":{"data":{"":"","clean-up#Clean up\u0026hellip;":"\nWhen working on applications that need vector/semantic/similarity search, it’s often useful to have a quick and easy way to create vector embeddings of data and save them in a vector database for further querying. This blog will walk you through a simple web application using which you can quickly generate vector embeddings for various document types and store them directly into Azure Cosmos DB. Once stored, this data can be leveraged by other applications for tasks like vector search, part of a Retrieval-Augmented Generation (RAG) workflow, and more.\nThe application is built using Go using the SDKs for Azure Cosmos DB (azcosmos) and Azure OpenAI (azopenai). It also utilizes the langchaingo library for document loading and text splitting. The frontend is a simple HTML, CSS, and JavaScript embedded directly in the Go application.\nSupports any text content, including file types such as .txt, .pdf, .md, .html, and .csv. You can directly reference a file using URLs (example) or use multiple local files at once. Easily configure Azure Cosmos DB (endpoint, database, container) and Azure OpenAI (endpoint and embedding model) details. No need to use service keys since the application supports service principals via DefaultAzureCredential. Also stores source file name in the metadata attribute Although the application itself can be useful for quick prototyping, it can be built using any language (Python, JS/TS, etc.). Go is heavily used for web applications, especially backend/API components. For those interested in Go, I think this will also serve as a useful learning excercise on how to use Go for building applications using Azure Cosmos DB and Azure OpenAI.\nThe code is available in this GitHub repo\nPrerequisites Complete these steps before running the application.\nSetup Azure Cosmos DB Create an Azure Cosmos DB for NoSQL account. Enable the vector indexing and search feature - this is a one-time operation.\nCreate a database and collection (use partition key /id for this application). Also, configure a vector policy for the container with the right path, distance function, etc.\nAzure OpenAI Create an Azure OpenAI Service resource. Azure OpenAI Service provides access to OpenAI’s models including the GPT-4o, GPT-4o mini (and more), as well as embedding models. Deploy an embedding model of your choice using the Azure AI Foundry portal (for example, I used the text-embedding-3-small model).\nRBAC setup Using RBAC is a good practice as it allows us to use eliminate hardcoding API keys and secrets in the code.\nI will show you how to run the app locally. The service principal that you use for the app needs to have the right permissions for Azure Cosmos DB and Azure OpenAI.\nCreate service principal Execute the following command to create a service principal:\naz ad sp create-for-rbac You should see an output similar to this. Note down the system-assigned password as you can’t retrieve it again\n{ \"appId\": \"\u003cthe app ID\u003e\", \"displayName\": \"\u003cthe name\u003e\", \"password\": \"\u003cthe client secret\u003e\", \"tenant\": \"\u003ctenant ID\u003e\" } Get the principal ID using the following command:\nexport PRINCIPAL_ID=$(az ad sp show --id \u003center the appID from the command output\u003e --query \"id\" -o tsv) Assign Azure OpenAI role export AZURE_OPENAI_RESOURCE_NAME=\u003cyour-openai-resource-name\u003e export RG_NAME=\u003cyour-resource-group-name\u003e Get the resource ID of the Azure OpenAI resource and assign the Cognitive Services OpenAI Contributor role to the service principal:\nexport AZURE_OPENAI_ID=$(az cognitiveservices account show --name $AZURE_OPENAI_RESOURCE_NAME --resource-group $RG_NAME --query \"id\" -o tsv) az role assignment create --assignee $PRINCIPAL_ID --role \"Cognitive Services OpenAI Contributor\" --scope $AZURE_OPENAI_ID Assign Azure Cosmos DB role export COSMOSDB_ACCOUNT=\u003cyour-cosmosdb-account-name\u003e export COSMOSDB_RG_NAME=\u003cyour-resource-group-name\u003e Get the resource ID of the Azure Cosmos DB account and assign the Cosmos DB Built-in Data Contributor role to the service principal:\nexport COSMOSDB_ACC_ID=$(az cosmosdb show --name $COSMOSDB_ACCOUNT --resource-group $COSMOSDB_RG_NAME --query \"id\" -o tsv) az cosmosdb sql role assignment create -n \"Cosmos DB Built-in Data Contributor\" -g $COSMOSDB_RG_NAME -a $COSMOSDB_ACCOUNT -p $PRINCIPAL_ID --scope $COSMOSDB_ACC_ID Run the web application Make sure you have Go installed on your machine. You can download it from here.\nClone the GitHub repository:\ngit clone https://github.com/abhirockzz/cosmosdb_openai_vector_embedding_webapp_golang cd cosmosdb_openai_vector_embedding_webapp_golang go mod tidy Set the service principal credentials as environment variables, and run the application:\nexport AZURE_TENANT_ID=\"\u003ctenant value from the command output\u003e\" export AZURE_CLIENT_ID=\"\u003cvalue of appID from the command output\u003e\" export AZURE_CLIENT_SECRET=\"\u003cvalue of password from the command output\u003e\" go run main.go You will be asked to configure the app - enter values for Azure Cosmos DB endpoint, Azure OpenAI endpoint, and more.\nOnce that’s done, go ahead and enter a URL for a file, or choose any text file(s) from your local machine:\nAs part of the processing, the vector embeddings will be generated and stored in Azure Cosmos DB. The application will show you the progress as well.\nOnce the processing finishes, verify the same in Azure Cosmsos DB. For example, if you chose to process the following markdown file URL, run this query to see the results:\nSELECT c.id FROM c WHERE CONTAINS(c.metadata.source, \"vector-search.md\") Now you can execute vector queries using the Azure Cosmos DB SDKs. For example, refer to the Vector/Similarity search section in this blog.\nTroubleshooting If you see an error similar to Error: HTTP error! status: 500, message: Failed to process, the issue could be related to:\nRBAC - make sure the service principal has the right permissions for Azure Cosmos DB and Azure OpenAI You entered an incorrect database, container, or Azure OpenAI model name Either of Cosmos DB or Azure OpenAI endpoints are incorrect Clean up… Once you’re done, if you do not need the resources going forward, delete them - including Cosmos DB, Open AI, service principal, etc.\nI hope you find this useful!","prerequisites#Prerequisites":"","run-the-web-application#Run the web application":"","troubleshooting#Troubleshooting":""},"title":"Vector embeddings made easy with Go, Azure Cosmos DB, and OpenAI"}}